,title,claps,date,link,year,text,popular
1620,How does the Bay Area Commute?,517,2018-12-21,https://towardsdatascience.com/how-does-the-bay-area-commute-22f45e00419e?source=collection_archive---------4-----------------------,2018,"Semantic search at scale is made possible with the advent of tools like BERT, bert-as-service, and of course support for dense vector manipulations in Elasticsearch. While the degree may vary depending on the use case, the search results can certainly benefit from augmenting the keyword based results with the semantic ones…Keyword based search across text repositories is a known art. The Lucene library and tools like Elasticsearch excel at lightning fast retrieval of matching documents for a given query. The search results are driven by terms/tokens and tf-idf metrics around them. Generally speaking, documents that do not share any common terms with the query will not be a part of the result set. This is the feature of the keyword based search. This can clearly exclude a lot of otherwise relevant documents but those that do not share any keywords with the query. Careful use of synonyms and stemming can help increase the recall. But since when has a synonym meant exact equality? We may think sunny is a synonym to bright. There is a bright moon but no sunny moon — ever. At least not on our planet! Plus, a rare synonym applied to expand the query can push an otherwise poor result to the top. And stemming? Yes, let us not even talk about it. A runner is not the same as the run or run! Clearly, all these machinations around keywords cannot get around to addressing semantics in text.Approaches such as Latent Semantic Analysis (LSA) have been used in the past to include semantically related documents in the search results. But the application of Singular Value Decomposition (SVD) on the term-document matrix built from millions of documents distributed on a cluster of nodes is non-trivial. Semantic search based on SVD at the scale and throughput Elasticsearch deals with is impractical. So where does that leave us if we want to enable semantics with Elasticsearch?The recently popular fixed size numerical vectors for words can help. Word embeddings with say a bag-of-words approach can turn a sentence or a document into a short dense numerical vector. We have gone over this at length in previous articles. Embeddings like the ones obtained from language models like BERT are context sensitive as well unlike the one-hot word vectors or the fastText embeddings. That is, we get different sentence vectors with BERT for “eat to live” vs “live to eat”, allowing us to distinguish between them. The key for enabling semantic search at scale is then in integrating these vectors with Elasticsearch.Fortunately, the current versions (7.3+) of Elasticsearch support a dense_vector field with a variety of relevancy metrics such as cosine-similarity, euclidean distance and such that can be computed via a script_score. Exactly what we need as we can rank documents in the index as per their score for these metrics with the dense vector representation of the query. The lightning fast speed of Elasticsearch applied to millions of dense vectors distributed across a cluster of nodes. That is basically the gist of this post. Let us get with it.The architecture could not be simpler. The pieces are all there in open source and all we have to do is to put them together. We use bert-as-service to get dense vector representations of the documents and the queries. The indexing and search requests are brokered through the BERT server that generates the dense vector for the supplied document or query text.Here is a simple configuration that defines an index with a sentence (a short quote in our case) and its numerical vector as the only fields. The vector is defined as 768 long as per the uncased base BERT (uncased_L-12_H-768_A-12).The quotes are read from a file, the dense vector is computed by calling bert-as-service, and indexed into Elasticsearch in bulk.Elasticsearch employs Lucene’s practical scoring function for traditional keyword based search. It is not applicable to us here as we work with numerical vectors. We can override the default with any custom scoring function around the dense vectors. But it is better to use Elasticsearch predefined functions such as cosine-similarity, L1, or L2 norms for efficiency reasons. The relevancy order of search results will certainly vary some based on which metric is used. Not sure if one of them is always better than the others, but cosine-similarity seemed to do fine for my tests. Let us look at a quick example. Consider the following three sentences in a file sentences.txt.Clearly the first two sentences are similar. And they are both dissimilar to the third. We can readily compute the vectors for each sentence and compute different metrics. Running bert_sentence_similarity.py with:we get:All the metrics did the right thing here by yielding the highest score for the 1–2 pair. For the remainder of the post we will stick with cosine similarity of the BERT query & sentence dense vectors as the relevancy score to use with Elasticsearch. The order of the top hits varies some if we choose L1 or L2 but our task here is to compare BERT powered search against the traditional keyword based search.To test how well this scheme is going to work for us, we prepare a rather large index made up of quotes from various people. The index is similar to the index in Section 1. The quotes are no more than 50 words in length. The index has several thousands of quotes — with some near duplicates for sure. Here is a sample:Once the game is over, the king and the pawn go back in the same boxHe uses statistics as a drunken man uses lamp posts for support rather than for illuminationYou should always go to other people’s funerals; otherwise they won’t go to yours.Intelligent life on other planets? I’m not even sure there is on earth!We know that many different quotes convey similar meanings. We query the index with a quote (or a paraphrased version of it) and examine the quality of the top results. We want to see the top hits to be the most similar to the query quote — as we understand them. We can do this directly with the More Like This (MLT) query that Elasticsearch offers, and of course with cosine similarity on our BERT derived vectors as shown in Figure 1. The task is to evaluate if BERT vectors have meaningfully enhanced the quality of the results.Querying ElasticsearchFor the MLT query we override some defaults so as to not to exclude any terms in the query or documents. For the script_score query we use for semantic search, we get the dense query_vector from bert-as-service. We start it up with:where BERT_BASE_DIR points to the directory where uncased_L-12_H-768_A-12 is on the disk. Here is snippet of code to query the same index in these two different ways.With the apparatus ready, all that is left to do is run some sample queries/quotes through and see if BERT powered Elasticsearch is able to return more meaningful results than those just based on keyword abundance. We pick a few quotes and compare the top 10 BERT & MLT results side-by-side. Each result is scored — “red 1, blue 0.5, default 0” based on the quality of the match — subjective for sure. Let us start with the first one.Holding on to anger is like grasping a hot coal with the intent of throwing it at someone else — you are the one who gets burnedBuddhaThe top hit for MLT is totally irrelevant as it has been hijacked by the term “someone”. Its 8th hit was misled by “coal” a potentially rare term in a repository of quotes. But MLT does get its 6th hit missed by BERT. Overall we see that BERT is able to pull out quotes that mean the same as the query but using different words than the query (see 4, 6 and 7). Let us look at an another one.Most folks are about as happy as they make up their minds to beabraham lincolnThe problem unfortunately for MLT in this case is the choice of words used in the query. The top hit for BERT nails it, but MLT missed it because it is looking for the terms “minds” or “make” but saw “mind” and “makes”. Its own top hit got totally taken in by the phrase “make up their minds” — a complete match with the query. Applying a stemmer may have helped MLT to catch BERT’s top hit. For its second hit, MLT may have gotten side-tracked by a possibly rare word like “folks” and the phrase “make up” of all things as this quote is certainly not about dressing up!But based on the above two examples, it would be premature to think that BERT has dealt a death blow to traditional one-hot word vectors. Take this final example for instance.fortune sides with him who daresvirgilThe word dares is likely a rare word in the repo and MLT does very well sticking to it and perhaps the overall phrase who dares. BERT on the other hand thinks (logically, we might add) dares is related to passion, disaster etc… and finds very different matches. That is generally the problem with overthinking someone might say — an unkind cut against BERT.We have shown that we can obtain semantic search results at scale with Elasticsearch. This is made possible with the advent of tools like BERT, bert-as-service, and of course support for dense vector manipulations in Elasticsearch. The quality of the semantic search results will depend on the nature of documents in the index and whether semantics are important in those documents. Semantics is important in most free flowing text and speech, unless perhaps if you are talking equations! That takes us to this final quote before we close.One reason math texts are so abstruse and technical is because of all the specifications and conditions that have to be put on theorems to keep them out of crevasses. In this sense, they’re like legal documents, and often about as much fun to read.david edgar wallaceSo there may be some dry text out there with no semantics or a play on words, but few would ever want read it much less search for it. So there we go — any search worthy text will have semantically related documents.To summarize:So are we ready to replace the traditional keyword based results with these semantic ones? No. But there is perhaps no harm in augmenting them and letting the end users vote!….Originally published at http://xplordat.com on Oct 29, 2019.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
943,The Machine Learning Workflow,515,2018-08-22,https://towardsdatascience.com/the-machine-learning-workflow-8e83c3b008be?source=collection_archive---------15-----------------------,2018,"Every data analysis starts with an idea, hypothesis, problem, etc. The next step usually involves the most important element: data. Today, data is everywhere. For those of us who love diving into data, there are lots of resources to attain this part of the process. Whether it’s through Kaggle or UCI Machine Learning Repository, data is easily available. However, sometimes not all data is available to us. Sometimes, in order to continue a certain data analysis/project, we must do a bit more to get the correct, updated data we need.This brings us to our topic: web scraping to create a data set. A while back, I worked on a basketball analytics project. The project was an analysis on individual stats of NBA players, and using some of those stats to predict win shares for the 2018 NBA season. As I began the project, I realized that the NBA data sets available on Kaggle did not have all the stats I needed to continue my analysis. Therefore, I decided to do a bit more research.First of all, my go-to site for all NBA stats is Basketball Reference. This site is essentially an encyclopedia for all things NBA stats. Then came my next question: Why not get the data directly from the Basketball Reference? After further research, I discovered a great Python library that solved this portion of my project:BeautifulSoup . This library is a web scraper that allows us to search through the HTML of a webpage and extract the information we need. From there, we will store the data we scraped onto a DataFrameusing pandas.First, let’s get all the Python libraries we will be using for this project:Now, we determine the HTML page we will be scraping. For this part, I used the individual per game stats of players for the current 2018–2019 NBA season. The page on Basketball Reference looks like this:As you can see, the data is already organized into a table. This makes web scraping a lot easier! Now we will use urlopen that we imported from the urllib.request library, then create a BeautifulSoup object by passing through html to BeautifulSoup().Here, the BeautifulSoup function passed through the entire web page in order to convert it into an object. Now we will trim the object to only include the table we need.The next step is to organize the column headers. We want to extract the text content of each column header and store them into a list. We do this by inspecting the HTML (right-clicking the page and selecting “Inspect Element”) we see that the 2nd table row is the one that contains the column headers we want. By looking at the table, we can see the specific HTML tags that we will be using to extract the data:Now, we go back to BeautifulSoup. By using findAll(), we can get the first 2 rows (limit = 2) and pass the element we want as the first argument, in this case ‘tr’, which is the HTML tag for table row. After using findAll(), we use the getText() to extract the table header, or ‘th’, text we need and organize it into a list:Next step, we will extract the data from the cells of the table in order to add it to our DataFrame. Although it is similar to extracting data from column header, the data within the cell, in this case player stats, is in a 2-dimensional format. Therefore, we must set up a 2-dimensional list:Now comes something a bit more familiar: pandas. With the data and columns headers we have extracted from Basketball Reference, we create a simple DataFrame:Voilà! We have our own data set! In a matter of minutes, we analyzed an HTML document, extracted the data from the table, and organized it onto a DataFrame. For free! And without any hassle of searching for a .csv file that might not be up-to-date. We have data that comes directly from a source that is updated everyday. Of course, not every HTML document is created equally. There are pages that might require a bit more time in analyzing the HTML tags in order to extract the proper data.To conclude this quick walk through, web scraping can be a simple task, but for certain HTML documents, it can be difficult. There are a bunch of helpful resources out there that will help you understand HTML tags, and get the data you need. There might be a problem you want to dive into, so don’t let limited data be an issue!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
749,How to collect your deep learning dataset,467,2018-07-06,https://towardsdatascience.com/how-to-collect-your-deep-learning-dataset-2e0eefc0ba24?source=collection_archive---------5-----------------------,2018,"Pink singlet, dyed red hair, plated grey beard, no shoes, John Lennon glasses. What a character. Imagine the stories he’d have. He parked his moped and walked into the cafe.This cafe is a local favourite. But the chairs aren’t very comfortable. So I’ll keep this short (spoiler: by short, I mean short compared to the amount of time you’ll actually spend doing EDA).When I first started as a Machine Learning Engineer at Max Kelsen, I’d never heard of EDA. There are a bunch of acronyms I’ve never heard of.I later learned EDA stands for exploratory data analysis.It’s what you do when you first encounter a data set. But it’s not a once off process.The past few weeks I’ve been working on a machine learning project. Everything was going well. I had a model trained on a small amount of the data. The results were pretty good.It was time to step it up and add more data. So I did. Then it broke.I filled up the memory on the cloud computer I was working on. I tried again. Same issue.There was a memory leak somewhere. I missed something. What changed?More data.Maybe the next sample of data I pulled in had something different to the first. It did. There was an outlier. One sample which had 68 times the amount of purchases as the mean (100).Back to my code. It wasn’t robust to outliers. It took the outliers value and applied to the rest of the samples and padded them with zeros.Instead of having 10 million samples with a length of 100, they all had a length of 6800. And most of that data was zeroes.I changed the code. Reran the model and training began. The memory leak was patched.Pause.The guy with the pink singlet came over. He tells me his name is Johnny.He continues.‘The girls got up me for not saying hello.’‘You can’t win,’ I said.‘Too right,’ he said.We laughed. The girls here are really nice. The regulars get teased. Johnny is a regular. He told me he has his own farm at home. And his toenails were painted pink and yellow, alternating, pink, yellow, pink, yellow.Johnny left.Back to it.What happened? Why the break in the EDA story?Apart from introducing you to the legend of Johnny, I wanted to give an example of how you can think the road ahead is clear but really, there’s a detour.EDA is one big detour. There’s no real structured way to do it. It’s an iterative process.When I started learning machine learning and data science, much of it (all of it) was through online courses. I used them to create my own AI Masters Degree. All of them provided excellent curriculum along with excellent datasets.The datasets were excellent because they were ready to be used with machine learning algorithms right out of the box.You’d download the data, choose your algorithm, call the .fit() function, pass it the data and all of a sudden the loss value would start going down and you’d be left with an accuracy metric. Magic.This was how the majority of my learning went. Then I got a job as a machine learning engineer. I thought, finally, I can apply what I’ve been learning to real-world problems.Roadblock.The client sent us the data. I looked at it. WTF was this?Words, time stamps, more words, rows with missing data, columns, lots of columns. Where were the numbers?‘How do I deal with this data?’ I asked Athon.‘You’ll have to do some feature engineering and encode the categorical variables,’ he said, ‘I’ll Slack you a link.’I went to my digital mentor. Google. ‘What is feature engineering?’Google again. ‘What are categorical variables?’Athon sent the link. I opened it.There it was. The next bridge I had to cross. EDA.You do exploratory data analysis to learn more about the more before you ever run a machine learning model.You create your own mental model of the data so when you run a machine learning model to make predictions, you’ll be able to recognise whether they’re BS or not.Rather than answer all your questions about EDA, I designed this post to spark your curiosity. To get you to think about questions you can ask of a dataset.How do you explore a mountain range?Do you walk straight to the top?How about along the base and try and find the best path?It depends on what you’re trying to achieve. If you want to get to the top, it’s probably good to start climbing sometime soon. But it’s also probably good to spend some time looking for the best route.Exploring data is the same. What questions are you trying to solve? Or better, what assumptions are you trying to prove wrong?You could spend all day debating these. But best to start with something simple, prove it wrong and add complexity as required.Example time.You’ve been learning data science and machine learning online. You’ve heard of Kaggle. You’ve read the articles saying how valuable it is to practice your skills on their problems.Roadblock.Despite all the good things you’ve heard about Kaggle. You haven’t made a submission yet.You decide it’s time to enter a competition of your own.You’re on the Kaggle website. You go to the ‘Start Here’ section. There’s a dataset containing information about passengers on the Titanic. You download it and load up a Jupyter Notebook.What do you do?What question are you trying to solve?‘Can I predict survival rates of passengers on the Titanic, based on data from other passengers?’This seems like a good guiding light.If a checklist is good enough for pilots to use every flight, it’s good enough for data scientists to use with every dataset.1. What question(s) are you trying to solve (or prove wrong)?2. What kind of data do you have and how do you treat different types?3. What’s missing from the data and how do you deal with it?4. Where are the outliers and why should you care about them?5. How can you add, change or remove features to get more out of your data?We’ll go through each of these.I put an (s) in the subtitle. Ignore it. Start with one. Don’t worry, more will come along as you go.For our Titanic dataset example it’s:Can we predict survivors on the Titanic based on data from other passengers?Too many questions will clutter your thought space. Humans aren’t good at computing multiple things at once. We’ll leave that to the machines.You’ve imported the Titanic training dataset.Let’s check it out.Column by column, there’s: numbers, numbers, numbers, words, words, numbers, numbers, numbers, letters and numbers, numbers, letters and numbers and NaNs, letters. Similar to Johnny’s toenails.Let’s separate the features (columns) out into three boxes, numerical, categorical and not sure.In the numerical bucket we have, PassengerId, Survived, Pclass, Age, SibSp, Parch and Fare.The categorical bucket contains Sex and Embarked.And in not sure we have Name, Ticket and Cabin.Now we’ve broken the columns down into separate buckets, let’s examine each one.Remember our question?‘Can we predict survivors on the Titanic based on data from other passengers?’From this, can you figure out which column we’re trying to predict?The Survivedcolumn. And because it’s the column we’re trying to predict, we’ll take it out of the numerical bucket and leave it for the time being.What’s left?PassengerId,Pclass, Age, SibSp, Parch and Fare.Think for a second. If you were trying to predict whether someone survived on the Titanic, do you think their unique PassengerIdwould really help with your cause?Probably not. So we’ll leave this column to the side for now too. EDA doesn’t always have to be done with code, you can use your model of the world to begin with and use code to see if it’s right later.How about Pclass, SibSp and Parch?These are numbers but there’s something different about them. Can you pick it up?What does Pclass, SibSp and Parch even mean? Maybe we should’ve read the docs more before trying to build a model so quickly.Google. ‘Kaggle Titanic Dataset’.Found it.Pclassis the ticket class, 1 = 1st class, 2 = 2nd class and 3 = 3rd class. SibSp is the number of siblings a passenger has on board. And Parch is the number of parents someone had on board.This information was pretty easy to find. But what if you had a dataset you’d never seen before. What if a real estate agent wanted help predicting house prices in their city. You check out their data and find a bunch of columns which you don’t understand.You email the client.‘What does Tnummean?’They respond. ‘Tnum is the number of toilets in a property.’Good to know.When you’re dealing with a new dataset, you won’t always have information available about it as Kaggle provides. This is where you’ll want to seek the knowledge of an SME.Another acronym. Great.SME stands for subject matter expert. If you’re working on a project dealing with real estate data, part of your EDA might involve talking with and asking questions of a real estate agent. Not only could this save you time, but it could also influence future questions you ask of the data.Since no one from the Titanic is alive anymore (RIP (rest in peace) Millvina Dean, the last survivor), we’ll have to become our own SMEs.There’s something else unique about Pclass, SibSp and Parch. Even though they’re all numbers, they’re also categories.How so?Think about it like this. If you can group data together in your head fairly easily, there’s a chance it’s part of a category.The Pclasscolumn could be labelled, First, Second and Third and it would maintain the same meaning as 1, 2 and 3.Remember how machine learning algorithms love numbers? Since Pclass, SibSp and Parch are already all in numerical form, we’ll leave them how they are. The same goes for Age.Phew. That wasn’t too hard.In our categorical bucket, we have Sex and Embarked.These are categorical variables because you can separate passengers who were female from those who were male. Or those who embarked on C from those who embarked from S.To train a machine learning model, we’ll need a way of converting these to numbers.How would you do it?Remember Pclass? 1st = 1, 2nd = 2, 3rd = 3.How would you do this for Sex and Embarked?Perhaps you could do something similar for Sex. Female = 1 and male = 2.As for Embarked, S = 1 and C = 2.We can change these using the .LabelEncoder() function from the sklearn library.training.embarked.apply(LabelEncoder().fit_transform)We’ve made some good progress towards turning our categorical data into all numbers but what about the rest of the columns?Name, Ticket and Cabin are left.If you were on The Titanic, do you think your name would’ve influenced your chance of survival?It’s unlikely. But what other information could you extract from someone's name?What if you gave each person a number depending on whether their title was Mr., Mrs. or Miss.?You could create another column called Title. In this column, those with Mr. = 1, Mrs. = 2 and Miss. = 3.What you’ve done is created a new feature out of an existing feature. This is called feature engineering.Converting titles to numbers is a relatively simple feature to create. And depending on the data you have, feature engineering can get as extravagant as you like.How does this new feature affect the model down the line? This will be something you’ll have to investigate.For now, we won’t worry about the Name column to make a prediction.What about Ticket?The first few examples don’t look very consistent at all. What else is there?training.Ticket.head(15)These aren’t very consistent either. But think again. Do you think the ticket number would provide much insight as to whether someone survived?Maybe if the ticket number related to what class the person was riding in, it would have an effect but we already have that information in Pclass.To save time, we’ll forget the Ticket column for now.Your first pass of EDA on a dataset should have the goal of not only raising more questions about the data but to get a model built using the least amount of information possible so you’ve got have a baseline to work from.Now, what do we do with Cabin?You know, since I’ve already seen the data, my spidey-senses are telling me it’s a perfect example for the next section.The Cabin column looks like Johnny’s shoes. Not there. There are a fair few missing values in Age too.How do you predict something when there’s no data?I don’t know either.So what are our options when dealing with missing data?The quickest and easiest way would be to remove every row with missing values. Or remove the Cabin and Age column entirely.But there’s a problem here. Machine learning models like more data. Removing large amounts of data will likely decrease the ability of our model to predict whether a passenger survived or not.What’s next?Imputing values. In other words, filling up the missing data with values calculated from other data.How would you do this for the Age column?Could you fill missing values with average age?There are drawbacks to this kind of value filling. Imagine you had 1000 total rows, 500 of which are missing values. You decide to fill the 500 missing rows with the average age of 36.What happens?Your data becomes heavily stacked with the age of 36. How would that influence predictions on people 36-years-old? Or any other age?Maybe for every person with a missing age value, you could find other similar people in the dataset and use their age. But this is time-consuming and also has drawbacks.There are far more advanced methods for filling missing data out of scope for this post. It should be noted, there is no perfect way to fill missing values.If the missing values in the Age column is a leaky drain pipe the Cabin column is a cracked dam. Beyond saving. For your first model, Cabin is a feature you’d leave out.‘Did you check the distribution?’ Athon asked.‘I did with the first set of data but not the second set…’It hit me.There it was. The rest of the data was being shaped to match the outlier.If you look at the number of occurrences of unique values within a dataset, one of the most common patterns you’ll find is Zipf’s law. It looks like this.Remembering Zipf’s law can help to think about outliers (values towards the end of the tail which don’t occur often are potential outliers).The definition of an outlier will be different for every dataset. As a general rule of thumb, you may consider anything more than 3 standard deviations away from the mean might be considered an outlier.Or from another perspective.How do you find outliers?Distribution. Distribution. Distribution. Distribution. Four times is enough (I’m trying to remind myself here).During your first pass of EDA, you should be checking what the distribution of each of your features is.A distribution plot will help represent the spread of different values of data you have across. And more importantly, help to identify potential outliers.train.Age.plot.hist()Why should you care about outliers?Keeping outliers in your dataset may turn out in your model overfitting (being too accurate). Removing all the outliers may result in your model being too generalised (it doesn’t do well on anything out of the ordinary). As always, best to experiment iteratively to find the best way to deal with outliers.The Titanic dataset only has 10 features. But what if your dataset has hundreds? Or thousands? Or more? This isn’t uncommon.During your exploratory data analysis process, once you’ve started to form an understanding AND you’ve got an idea of the distributions AND you’ve found some outliers AND you’ve dealt with them, the next biggest chunk of your time will be spent on feature engineering.Feature engineering can be broken down into three categories: adding, removing and changing.The Titanic dataset started out in pretty good shape. So far, we’ve only had to change a few features to be numerical in nature.However, data in the wild is different.Say you’re working on a problem trying to predict the changes in banana stock requirements of a large supermarket chain across the year.Your dataset contains a historical record of stock levels and previous purchase orders. You're able to model these well but you find there are a few times throughout the year where stock levels change irrationally. Through your research, you find during a yearly country-wide celebration, banana week, the stock levels of bananas plummet. This makes sense. To keep up with the festivities, people buy more bananas.To compensate for banana week and help the model learn when it occurs, you might add a column to your data set with banana week or not banana week.Adding a feature like this might not be so simple. You could find adding the feature does nothing at all since the information you’ve added is already hidden within the data. As in, the purchase orders for the past few years during banana week are already higher than other weeks.What about removing features?We’ve done this as well with the Titanic dataset. We dropped the Cabin column because it was missing so many values before we even ran a model.But what about if you’ve already run a model using the features left over?This is where feature contribution comes in. Feature contribution is a way of figuring out how much each feature influences the model.Why is this information helpful?Knowing how much a feature contributes to a model can give you direction as to where to go next with your feature engineering.In our Titanic example, we can see the contribution of Sex and Pclass were the highest. Why do think this is?What if you had more than 10 features? How about 100? You could do the same thing. Make a graph showing the feature contributions of 100 different features.‘Oh, I’ve seen this before!’Zipf’s law back at it again. The top features have far more to contribute than the bottom features.Seeing this, you might decide to cut the lesser contributing features and improve the ones contributing more.Why would you do this?Removing features reduces the dimensionality of your data. It means your model has fewer connections to make to figure out the best way of fitting the data.You might find removing features means your model can get the same (or better) results on fewer data and in less time.Like Johnny is a regular at the cafe I’m at, feature engineering is a regular part of every data science project.Finally. We’ve been through a bunch of steps to get our data ready to run some models.If you’re like me, when you started learning data science, this is the part you learned first. All the stuff above had already been done by someone else. All you had to was fit a model on it.Our Titanic dataset is small. So we can afford to run a multitude of models on it to figure out which is the best to use.Notice how I put an (s) in the subtitle, you can pay attention to this one.Running multiple models is fine on our small Titanic dataset. But might not be the best for larger datasets.Once you’ve had some practice with different datasets, you’ll start to figure out what kind of model usually works best. For example, most recent Kaggle competitions have been won with ensembles (combinations) of different gradient boosted tree algorithms.Once you’ve built a few models and figured out which is best, you can start to optimise the best one through hyperparameter tuning. Think of hyperparameter tuning as adjusting the dials on your oven when cooking your favourite dish. Out of the box, the preset setting on the oven works pretty well but out of experience you’ve found lowering the temperature and increasing the fan speed brings tastier results.It’s the same with machine learning algorithms. Many of them work great out of the box. But with a little tweaking of their parameters, they work even better.But no matter what, even the best machine learning algorithm won’t result in a great model without adequate data preparation.EDA and model building is a repeating circle.I left the cafe. My ass was sore.At the start of this article, I said I’d keep it short. You know how that turned out. It will be the same as your EDA iterations. When you think you’re done. There’s more.We covered a non-exhaustive EDA checklist with the Titanic Kaggle dataset as an example.Start with the simplest hypothesis possible. Add complexity as needed.Is your data numerical, categorical or something else? How do you deal with each kind?Why is the data missing? Missing data can be a sign in itself. You’ll never be able to replace it with anything as good as the original but you can try.Distribution. Distribution. Distribution. Three times is enough for the summary. Where are the outliers in your data? Do you need them or are they damaging your model?The default rule of thumb is more data = good. And following this works well quite often. But is there anything you can remove get the same results? Start simple. Less but better.There are examples of everything we’ve discussed here (and more) in the notebook on GitHub and a video of me going through the notebook step by step on YouTube (the coding starts at 5:05).If you’ve got something on your mind you think this article is missing, leave a response below or send me a note.Otherwise, you can find me on Twitter, LinkedIn and YouTube.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
71,Uber Driver Schedule Optimization,800,2018-01-21,https://towardsdatascience.com/uber-driver-schedule-optimization-62879ea41658?source=collection_archive---------5-----------------------,2018,"It has been long time since I wrote the first machine learning for everyone article. From now on, I will try to publish articles more frequently.Quick Note: Unfortunately, Medium does not support mathematical type setting (Latex etc.), so I put mathematical formulas as images to articles and I have no idea, if equations look elegant in different devices.Today’s topic is Generalized Linear Models, a bunch of general machine learning models for supervised learning problems(both for regression and classification).Let’s start with linear regression models. I think, everyone has encountered linear regression models during his/her university years, in one way or another. The goal of linear regression models is to find a linear mapping between observed features and observed real outputs so that when we see a new instance, we can predict the output. In this article, we accepted that there are N observations with output y and M features x, for training.We define an M dimensional vector w to represent weights which map inputs to outputs. We also define N by M dimensional matrix X to represent all the inputs. y is defined as N dimensional output vector.Our aim is to find best w that minimizes the Eucledian distance between real output vector y and approximation Xw. For this purpose, we generally use Least Squares error and matrix calculus to minimize it. Here we use L to represent loss(error) function.This is a linear algebraic approximation to problem, but in order to understand the problem better, and extend it to different problem settings, we will handle it in a more probabilistic manner.In the begginning, we said that outputs are real. Actually, we assumed that outputs are sampled from Normal distribution. Let’s define it explicitly, by setting mean of distribution to Xw and variance to I(unit variance).Now, our aim is to find w that maximizes the likelihood of y which is p(y|X,w). We defined p(y|X,w) as Normal distribution above, so we know its expanded form which is pdf of Normal distribution.It is hard to work directly with likelihood function, instead, we will work with loglikelihood which has the same maxima and minima with likelihood. We can either maximize loglikelihood or minimize negative loglikelihood. We choose the second one and call it loss function.This loss function is exactly same with the least squares error function. So we statistically explained linear regression and this will be very helpful in upcoming models.The solution above is called maximum likelihood method because that is what we exactly did, maximizing likelihood. We can put prior probabilities on weights and maximize posterior distribution of w instead of likelihood of y.In above equations, we defined zero mean, unit variance prior on weight w, and derived loss function by using negative log posterior distribution. Prior distribution of w try to keep weight values around its mean which is 0 in this case. This process called L2 regularization(Ridge regression) which penalizes marginal w values as it can be seen in loss function.Prior distribution reflects our beliefs on w values and it does not have to be Normal distribution. If we put Laplace distribution as prior, regularization term will be 1-norm of w(L1 regularization-Lasso).In order to illustrate the regularization effect better, I will give you an example. Let’s assume we have a data point with features [2,1] and output 3. There are infinitely many ways to set up weights for this problem, but among them, L2 regularization will prefer [1,1] and L1 regularization will prefer [1.5,0] because 2-norm of [1,1] and 1-norm of [1.5,0] are the smallest ones among all possible solutions. Therefore, we see that L2 regularization try to keep all the weight values close to 0 as much as possible. On the other hand, L1 regularization prefers sparse solutions.We used linear regression for real valued outputs. More specifically, if the output values are counts, then we can change the likelihood distribution and use the same setup for this new problem. Poisson distribution is an appropriate distribution to model count data and we will utilize it.Hyperparameter of Poisson distribution can not take negative values. So we change the definition of generative model a little bit and use linear model to not generate the hyperparameter directly as in the case of Normal distribution, but to generate the logarithm(natural logarithm ln actually) of it. Logarithm is the link function of Poisson distribution for Generalized Linear Models, and we work with negative loglikelihood again to find maximum likelihood solution.We take the derivative of loss function with respect to w and equalize it to 0. As far as I see, it does not have a closed form solution, opposite to linear regression. But we can use unconstrained optimization methods to find iterative solutions and below I supplied Gradient descent method for it. In Gradient descent method, we gradually update weights and take small steps through the negative direction of gradient of loss(because this is the direction where the loss decreases with respect to w).Above I presented models for regression problems, but generalized linear models can also be used for classification problems. In 2-class classification problem, likelihood is defined with Bernoulli distribution, i.e. output is etiher 1 or 0.This time we use sigmoid function to map the linear model’s output to a range of (0,1), because mean of Bernoulli should be in this range.The loss function defined above is known as Cross-Entropy loss function and widely used in classification problems, and we showed the reason why we use it, statistically. No closed form exits for the solution, and we will use Stochastic Gradient Descent which takes the instances one by one and offer an online iterative update mechanism.If the number of classes is greater than 2, Bernoulli distribution is not sufficient to describe the data. In this case, we prefer multinomial distribution. For K-class classification problem, we express the output with one-hot encoding. In one-hot encoding, every output is expressed with K-dimensional vectors with all 0’s except the index that takes the value 1 and shows the class of the instance.This time, we define K different probabilities that y can belong to. Each class has its own probability and weights. Therefore, unlike the above models, we define K different weight vectors.Each class’ probability needs to be greater than 0, so we can take the exponential of linear mapping as it is done in Poisson regression. But this time there are K different probabilities, and their summation should be equal to 1. Therefore, it needs to be normalized and for this purpose we use softmax function.Again, we use negative loglikelihood to define loss function and it is called Cross-Entropy Loss function. Similar to Poisson Regression and Logistic Regression, Gradient Descent Optimizer can be used to solve the problem.In this article I tried to be clear as much as possible. Equations that we derived are important, because they forms the basics of more complex machine learning models, like neural networks. I hope you enjoyed it.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1778,My Road to Data,32,2019-01-17,https://towardsdatascience.com/a-bit-about-me-7b8b46bbf022?source=collection_archive---------19-----------------------,2019,"Hello!This article is a part of my work done in Julia Season of Contributions (JSoC) 2019. It describes the Julia implementation of ARIMA models using the Probabilistic Programming Language (PPL) Turing.jl, which provides great ease in defining probabilistic models. This aspect of Turing will become more obvious when we look at model definitions later on in the article. Furthermore, Turing supports the use of custom distributions for specifying models.Okay, so let’s get straight onto it!We will use a dataset containing S&P 500 Adjusted Closing Values 1995–2015 with a monthly frequency. The dataset can be downloaded from here (MIT License).The following plot is obtained after running the code above. This is essentially how our data looks like, plotting the value at each time index:We split the data into training and test sets, taking 95% of the data as training set:We can see from the plots that the mean of the series rises towards the end. So, the series is not stationary. This is reinforced by the Augmented Dickey-Fuller (ADF)Test for stationarity:Thus, we difference the time series in an attempt to remove the stationarity:This series seems to have a roughly constant mean, though this does not necessarily mean that the series is stationary. We use the ADF test once again:Success! We can safely assume this series to be stationary since the p-value is so low. We can now move on to selecting the AR and MA terms for our differenced time series with the help of ACF and PACF plots.The ACF and PACF plots obtained for our training data are as shown below:These plots can be construed in the following ways:Having both AR and MA terms in an ARIMA model is not very common. So, we will not consider this case. From the two points above, it seems that the model is more likely to have a Moving Average term. Nevertheless, we will consider two plausible cases for our ARIMA model:The notation for an ARIMA(p, d, q) model is defined as follows:We implement both of these cases and compare the models using the Akaike information criterion (AIC). This webpage¹ is used as a reference for defining the ARIMA(1, 1, 0) and ARIMA(0, 1, 1) models below.The ARIMA(1,1,0) model is defined as follows:Here, x is the original time series as we have accounted for differencing in the model definition itself. Note that we will have one autoregressive term since p = 1.The ARIMA(1,1,0) model is defined as follows:As in the previous model definition, x is the original time series. Note that we will have one Moving Average term since q = 1.A point to be observed here is how the code written in Turing is essentially the same as it would be written on a piece of paper. This is evident from the model definitions above where one can understand these model definitions simply by looking at the code.The chain is sampled using the NUTS sampler. You can check out the docs to know more about NUTS and several other samplers that Turing supports. The code for sampling is as follows:To get the visualisations and the summary statistics for the parameters, you can have a look at them in the code here (Jupyter notebook) or here (.jl file).The Akaike information criterion (AIC) measures the relative “goodness” of different statistical models. Thus, it can be used for the purpose of model comparison. The lower the value of AIC, the better the model is. Also, one must remember that the absolute value of AIC does not have much meaning, the relative values are what matter. Mathematically, AIC is given by:Using this formula, we can calculate the AIC values for our two models. This PDF² has been used as a reference for calculating the AIC values of the two models.Using this function, we get the value of AIC for ARIMA(1,1,0) as approximately -299655.26Using this function, we get the value of AIC for ARIMA(1,1,0) as approximately 6.22 x 10⁷.Clearly, we can see that the ARIMA(0,1,1) model is better.This brings to an end my article on ARIMA Models with Turing.jl. I hope you found it interesting. If you have any questions or doubts regarding this article, feel free to contact me at sshah@iitk.ac.in or you can tag me on the Julia slack with @Saumya Shah.[1] Introduction to ARIMA models. (2019). Retrieved 26 August 2019, from https://people.duke.edu/~rnau/411arim.htm[2] Thomas, S. (2009). Estimating AR/MA models [Ebook]. Retrieved from http://www.igidr.ac.in/faculty/susant/TEACHING/TSA/print06.pdfWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1100,Putting “Uber Movement” Data into Action — Machine Learning Approach,467,2018-09-25,https://towardsdatascience.com/putting-uber-movement-data-into-action-machine-learning-approach-71a4b9ed0acd?source=collection_archive---------8-----------------------,2018,"One of the most familiar settings for a machine learning engineer is having access to a lot of data, but modest resources to annotate it. Everyone in that predicament eventually goes through the logical steps of asking themselves what to do when they have limited supervised data, but lots of unlabeled data, and the literature appears to have a ready answer: semi-supervised learning.And that’s usually when things go wrong.Historically, semi-supervised learning has been one of those rabbit holes that every engineer goes through as a rite of passage only to discover a newfound appreciation for plain old data labeling. The details are unique to every problem, but in broad strokes, they can often be depicted as follows:In low data regimes, semi-supervised training does indeed tend to improve performance. But in a practical setting, you often go from ‘terrible and unusable’ levels of performance to ‘less terrible but still completely unusable.’ Essentially, when you are in a data regime where semi-supervised learning actually helps, it means you’re also in a regime where your classifier is just plain bad and of no practical use.In addition, semi-supervision generally doesn’t come for free, and a method which uses semi-supervised learning very often doesn’t provide you with the same asymptotic properties that supervised learning does in high-data regimes — unlabeled data may introduce bias, for instance. See e.g. Section 4. A very popular method of semi-supervised learning in the early days of deep learning was to first learn an auto-encoder on unlabeled data, followed by fine-tuning on labeled data. Hardly anyone does this any more because representations learned via auto-encoding tend to empirically limit the asymptotic performance of fine-tuning. Interestingly, even vastly improved modern generative methods haven’t improved that picture much, probably because what makes a good generative model isn’t necessarily what makes a good classifier. As a result, when you see engineers fine-tuning models today, it’s generally starting from representations that were learned on supervised data — and yes, I consider text to be self-supervised data for the purpose of language modeling. Wherever practical, transfer learning from other pre-trained models is a much stronger starting point, which semi-supervised approaches have difficulty outperforming.So a typical machine learning engineer’s journey through the swamps of semi-supervised learning goes like this:1: Everything is terrible, let’s try semi-supervised learning! (After all, that’s engineering work, much more interesting than labeling data …)2: Look, numbers go up! Still terrible, though. Looks like we’ll have to label data after all …3: More data is better, yay, but have you tried what happens if you discard your semi-supervised machinery?4: Hey, what do you know, it’s actually simpler and better. We could have saved time and a whole lot of technical debt by skipping 2 and 3 altogether.If you’re very lucky, your problem may also admittedly have a performance characteristic shaped like this instead:In that case, there is a narrow data regime where semi-supervised is non-terrible and also improves data efficiency. In my experience, it’s very rare to hit that sweet spot. Factoring in the cost of the extra complexity, the fact that the gap in the amount of labelled data is typically not orders of magnitude better, and the diminishing returns, it’s rarely worth the trouble, unless you’re competing on an academic benchmark.But wait, isn’t this piece titled ‘The Quiet Semi-Supervised Revolution’?One fascinating trend is that the landscape of semi-supervised learning may be changing to something that looks more like this:And that would change everything. First, these curves match one’s mental model of what semi-supervised approaches should do: more data should always be better. The gap between semi-supervised and supervised should be strictly positive even for data regimes where supervised learning does well. And increasingly this is happening at no cost and remarkably little additional complexity. The ‘magic zone’ starts lower, and equally importantly, it isn’t bounded in high data regimes.What’s new? Lots of things: many clever ways to self-label the data and express losses in such a way that they are compatible with the noise and potential biases of self-labeling. Two recent works exemplify recent progress and point to the relevant literature: MixMatch: A Holistic Approach to Semi-Supervised Learning and Unsupervised Data Augmentation.Another fundamental shift in the world of semi-supervised learning is the realization that it may have a very important role to play in machine learning privacy. For example, the PATE approach (Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data, Scalable Private Learning with PATE,) whereby the supervised data is presumed private, and a student model with strong privacy guarantees is trained using only unlabeled (presumed public) data. Privacy-sensitive methods for distilling knowledge are becoming one of the key enablers of Federated Learning, which offers the promise of efficient distributed learning that doesn’t rely on the model having access to user data, with strong mathematical privacy guarantees.It’s an exciting time to be revisiting the value of semi-supervised learning in practical settings. Seeing one’s long-held assumptions challenged is a great indicator of the amazing progress happening in the field. This trend is all very recent, and we’ll have to see if these methods stand the test of time, but the potential for a fundamental shift in the architecture of machine learning tools that could result from these advances is very intriguing.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2006,Statistics is the Grammar of Data Science — Part 3/5,475,2019-02-02,https://towardsdatascience.com/statistics-is-the-grammar-of-data-science-part-3-5-173fdd2e18c3?source=collection_archive---------6-----------------------,2019,"ste·ga·no·graph·y\ ˌste-gə-ˈnä-grə-fē \nounthe art or practice of concealing a message, image, or file within another message, image, or fileI’m Greg Rafferty, a data scientist in the Bay Area. You can check out the code for this project on my github. Feel free to contact me with any questions!During World War II, the Allies intercepted German messages looking like this:They asked their best cryptographers to work on decoding the hidden message, because obviously there’s more to this than meets the eye. But if they had intercepted some Germans sending each other 1942 Instagrams, they probably wouldn’t have bothered too much reading into it.“Steganography” comes from the Greek word steganos, meaning “covered” or “reticent.” Whereas cryptography concerns itself with hiding the contents of a message, steganography concerns itself with hiding the fact that’s there’s even a message at all, which discourages potential code-breakers from looking deeper.This post is about a certain kind of steganography, termed Least Significant Bits (or LSB steganography). If I offered you $00000000 and said you could change just one digit of that number before I handed the cash over, only a fool would change the rightmost digit and request $00000001. It’s pretty obvious that changing the leftmost digit results in $10,000,000, or 10 million dollars, as compared to just a single dollar when changing the rightmost digit. Even though in both cases, we’re changing just a single digit, the rightmost digit is the least significant because it has the smallest effect on the total. This is the same concept as LSB steganography.One byte of data consists of 8 bits — eight digits of either 0 or 1, such as 00011010. If this byte represents the color of a pixel in an image, we could change the rightmost digit and only very imperceptibly change the color of that pixel. We can take our secret message and encode it in these least significant bits, so the overall image doesn’t change much to the human eye and draws no attention to itself, and then extract these bits later to reveal the secret.Here’s a visual example. Colors on a computer screen can be represented by various balances of the colors red, green, and blue. This is called the RGB color model and each pixel can have a value between 0 and 255. In the palate above, the red square on the left is created from red, green, and blue pixel values of R=240, G=64, B=64. Converted to binary representation, these numbers are R=11110000, G=0100000, B=0100000. The next square to the right has the least significant bits of each color channel changed from 0 to 1, resulting in values of R=11110001, G=0100001, B=0100001. The square appears as exactly the same color as the first, and we’ve encoded 3 bits of information into that single pixel. The next square over has the 2nd least significant bit changed as well, resulting in values of R=11110011, G=0100011, B=0100011. Each square over has the next least significant bit changed, and you can see that it isn’t clear that the color is changing until we get to the middle of the sequence (and then, it’s only because we’re able to compare it with the original value). By the time we get to the last square, and each bit in each byte of the red, green, and blue color channels has been switched, the color has completely changed. We’re now at pixel values of R=0000111, G=10111111, B=10111111, or R=15, G=191, B=191, when converting back to integer values.In order to encode the maximum amount of information into a pixel without changing it too much, I’ve chosen to discard the 4 least significant bits. So, to hide one image inside another, I take the 4 most significant bits of the cover image and the 4 most significant bits of the hidden image. I shift the 4 bits from the hidden image to the position of the least significant bits, and merge them with the 4 bits from the cover image. And, voilà!, the cover image hasn’t appeared to have changed at all but it now contains the information necessary to reconstruct the hidden image.Here’s an animation showing the process:We first loop through each pixel of the cover image (the red box, in the video above) and the hidden image (green) and for each pixel,Remember that image above, of the forest? It’s got an image hidden inside it; I’ve already replaced all of the least significant bits with the most significant bits of a hidden image. So if I take that image, throw away the four most significant bits from each pixel and shift the least significant bits over, what appears? Rick Astley!Without seeing the original pic of that forest (and even if you do!), it’d be very difficult to know that an image was hidden inside. I intentionally chose a “busy” image with lots of random lines and contrast because changes are more difficult to notice. The decoded pic of Astley though does show some artifacts from the loss of the least signifiant bits. Look at the images below; the smooth (so smooth!) skin of Astley’s forehead and cheeks shows some posterization in the decoded image where colors abruptly shift without a smooth transition. You can also see in the bricks, particularly at the beginning of the arch above his right (your left) shoulder, where a lot of detail in the bricks is lost and they appear as one continuous color.It’s important with LSB steganography not to use an image from the public domain, because one could easily compare the original to the coded one. Here, I’ve taken the original image of the forest and subtracted from it the coded image. This subtracts each pixel value and effectively shows the difference between the two photos.The forest image mostly drops out because all of its most significant bits have been subtracted from each other. We’re left with only the least significant bits, which is where Astley is hiding. Also, the Rick Astley photo has a different aspect ratio, so the bottom 1/8 of the photo is devoid of any hidden image. The image above has also had its brightness increased dramatically. The changes to the image were so imperceptible that the un-enhanced difference shows up as pure black.LSB steganography is a tool not just limited to spies. This technique is also used often by movie studios, for instance. When they need to send a movie to reviewers before releasing it to the public, they’ll hide a watermark in some of the frames of the film. This watermark is specific to each recipient of an advance copy so if the film later shows up on The Pirate Bay, the movie studio simply needs to decode the hidden watermark from that copy to discover who had leaked it. Crucially, these hidden images can still be decoded if the image has been compressed — because the hidden image is just a “dimmer” version of itself (not quite correct, but just go with it), it’ll still be there even if the image is resized, compressed, or cropped.It’s also possible to hide pure text inside another image with LSB steganography. Text requires a slightly different procedure though; the text is first converted to an array of bytes and each bit is sequentially overwritten into the least significant bits of an image. Because the order matters and each pixel contains a portion of the long string of bits, the image must be saved as a bitmap file and cannot be compressed (unless saved in a lossless format, such as in a .png), cropped, resized, enhanced, or any other procedure which would change the pixels. In contrast to hiding another image, where any change to the cover image will change but not corrupt the hidden image, when hiding text any change to the underlying pixel values will completely destroy the text. This obviously can be rather inconvenient with large amounts of text because the resulting bitmap file will be very large (I encoded the entire contents of War and Peace into a single image of 243 megabytes). Here is another version of the forest image which contains a much shorter string of text than War and Peace:And what’s hidden in this image?We’re no strangers to loveYou know the rules and so do IA full commitment’s what I’m thinking ofYou wouldn’t get this from any other guyI just wanna tell you how I’m feelingGotta make you understandNever gonna give you upNever gonna let you downNever gonna run around and desert youNever gonna make you cryNever gonna say goodbyeNever gonna tell a lie and hurt youWe’ve known each other for so longYour heart’s been aching but you’re too shy to say itInside we both know what’s been going onWe know the game and we’re gonna play itAnd if you ask me how I’m feelingDon’t tell me you’re too blind to seeNever gonna give you upNever gonna let you downNever gonna run around and desert youNever gonna make you cryNever gonna say goodbyeNever gonna tell a lie and hurt youNever gonna give you upNever gonna let you downNever gonna run around and desert youNever gonna make you cryNever gonna say goodbyeNever gonna tell a lie and hurt youNever gonna give, never gonna give(Give you up)(Ooh) Never gonna give, never gonna give(Give you up)We’ve known each other for so longYour heart’s been aching but you’re too shy to say itInside we both know what’s been going onWe know the game and we’re gonna play itI just wanna tell you how I’m feelingGotta make you understandNever gonna give you upNever gonna let you downNever gonna run around and desert youNever gonna make you cryNever gonna say goodbyeNever gonna tell a lie and hurt youNever gonna give you upNever gonna let you downNever gonna run around and desert youNever gonna make you cryNever gonna say goodbyeNever gonna tell a lie and hurt youNever gonna give you upNever gonna let you downNever gonna run around and desert youNever gonna make you cryWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1844,Simple Yet Practical Analysis of ClinicalTrials.gov,644,2019-01-17,https://towardsdatascience.com/simple-yet-practical-analysis-of-clinicaltrials-gov-f84b367e1e68?source=collection_archive---------16-----------------------,2019,"CONTEXT | From Harvard Business Review (March 2018): Despite the many machine learning discoveries being made by academics, … , companies are struggling to deploy machine learning to solve real business problems. In short, the gap for most companies isn’t that machine learning doesn’t work, but that they struggle to actually use it.Here, I will present efforts being made to make Deep Learning (part of Machine Leaning) more user-friendly, so that it becomes easier to use by companies. Hopefully these efforts will help reduce the “struggle” faced by companies when they dip in the depths of Deep Learning.So, is it possible to make Deep Learning (MLP, CNN, RNN, LSTM, GAN, DRL, …) more user-friendly ? More user-friendly like going from MS-DOS to Windows 2.0 (remember MS-DOS ? the CP/M clone). Or like going from an Integrated Development Environment (IDE) to a Graphical Programming Environment (GPE).Well, everything is possible … if you put enough effort into it.A lot of efforts have already been made to make it easier to use Deep Learning:Thanks to the pervasive Open Source culture in Deep Learning, all these are readily and freely available. And now that the Open culture has moved to DL models, very high performance NN have been made available to all through Deep Learning model Zoos. These make transfer learning a “breeze”.They’re also are other efforts to make it very easy to use Deep Learning potential without any development effort, any need for a DS/DL department or dedicated engineers:- Object recognition Web Services (YOLO, …)- Google AutoML serviceBut these provide Black Box services … well actually a Black Box inside another Black Box. Some companies might not care, and find the service very useful/economic, but some will care and will want to master the Deep Learning process.Still, right now, building a Deep Learning model involves quite a bit of programming with one of the current Deep Learning framework (Theano, Tensorflow, PyTorch, CNTK, Caffe2, …) or Meta-API ( Keras) and programing language (Python, Java, R, …). It is usually done by fairly advanced users that have been specifically trained.So Deep Learning user-friendliness could take the form of:Other motivations to go the User-Friendly way are that it could yield:That means user-friendliness could also come from:User-friendliness comes with restricted flexibility in building custom DL models. It can work if the cursor between the two opposite aims, customizability & ease of use, is properly positioned. But the position of the cursor depend on the type of the user.Who’s likely to need user-friendliness:→ need to build new DL model faster an modify them faster→ need to make a lot experiments: different architectures, optimize Hyper-parameter, tune datasets, …Who is less likely to need it:So far, a few attempts have been made to make Deep Learning more user-friendly. Here, I will present three that have gone quite a way to get there:All of these tools have a graphical editor to edit the Deep Learning models. They all allow to build a Deep Learning architecture from DL layers like 2D-Convolution, 2D-MaxPooling, …Two other candidates to user-friendliness will not be reviewed since, as of now, they don’t provide a graphical editor to edit DL models:Back in the old days (90s), when Neural Networks were still in the SHALLOW ERA, there were Neural Network simulators with GUI like:These already tried to make Neural Network modeling more user-friendly. I actually used SNNS for my first steps into the SHALLOW NN field. It was a great Open Source tool to play around with:After the SHALLOW era, the neural networks jumped in at the DEEP LEARNING end and got into the DEEP era.Deep Leaning Studio is a very interesting platform that has two operation modes: cloud and desktop.DLS is available here: http://deepcognition.ai/DLS has the essential user-friendly ingredient → a Deep Learning model editor that looks like:The basic ingredients of the DL model editor are the layer (see left panel) that make up a DL model. To build the convolution part of a CNN, layers like Convolution2D, MaxPooling2D, BatchNormalization layer are available. All the layer defined in the KERAS API are available plus a few other. The model is build by drag’n dropping these layer on the editor workspace and defining the connection graph between these layer. The parameters of each layer can be set by selecting the layer and then setting the values in a side panel on the right of the editor screen.An exemple of building a DL model with DLS can be seen in this video: https://player.vimeo.com/video/198088116""Each time a layer is added or a parameter of a layer is changed a background process check that the network is “coherent”. That way, one is warned early on if is side-tracking into building an “impossible” model.So Deep Learning Studio has got what it take to make Deep Learning more user-friendly.Basic characteristicsAdvanced characteristicsBut it is got a more features that go beyond providing a simple editor and provide a tight integration with the rest of the environment. One of these is:The graphical editor does not come alone. There are 4 more part (accessible as tabs) in the DLS environment:the Graphical DL model editor is in the MODEL tabAll these functionalities deal with building DL modelsFurther to that there a other sections of the environment that provide that extra + that takes the tool in another dimension:plus a few more practicality orientated sectionsSo the DLS solution does provide quite a bit more than just a DL model editor. Here is not the place to present the full DLS environment. So I will just point a few functionalities that are quite well thought-out and pretty useful:It automatically generate an easily trainable DL architecture for a specific Dataset. It’s not same approach as Google AutoML (https://techcrunch.com/2018/01/17/googles-automl-lets-you-train-custom-machine-learning-models-without-having-to-code/). I was able to test it on a leaf shape recognition dataset and it works very well.Once the model is built DLS allows deployment of the model as REST API. In addition to deployed REST API, a simple form based web application is also generated and deployed for rapid testing and sharing. Deployed models can be managed from the deployment menu.Jupiter NotebookDSL provides the possibility to program inside a Jupyter Notebook or run already existing Notebook in the environments provided (Desktop or Cloud).Pre-configured EnvironmentsDeep Cognition has introduced pre-configured environment for deep learning programmers. This feature frees AI developers from headache of setting up development environments. This is especially important as many deep learning frameworks and libraries require different version of packages. Conflict in version of these packages often lead to wasted time in debugging.Currently latest version of Tensorflow, Keras, Caffe 2, Chainer, PyTorch, MxNet, and Caffe are available. These enable developers to use various different github AI projects very fast. These environments are isolated and supports both CPU and GPU computing.Ultimately these free up developers time from devops work and help them focus on real AI model building and optimization work.Pre-configured environments in Deep Learning Studio not only gives access to terminal but also to a full-fledged web-based IDE that is based on open source components from VS Code.These two features (Jupiter Notebook and Pre-configured Environments) are a real asset. They make it possible to use the Deep Cognition Cloud and GPUs for any Deep Learning, Machine Leaning or Data Science task → Doesn’t lock people in an Editor only solution.SONY’s Neural Network Console (NNC) seems to have initially be an internal tool that has been made into a product with the associated in-house DL framework Neural Network Libraries (https://nnabla.org/) released in Open Source here https://github.com/sony/nnabla . So NNC should benefit from that internal experience.NNC is available here: https://dl.sony.com/The model editor in the NNC works pretty much the same way as the editor in DLS. The layers that can be added to the DL model are specific to SONY’s DL framework Neural Network Libraries (NNL).Here are a few specificities and some differences with DLS:Similarly to DLS, NNC provide an environment that goes beyond the DL model editor.The graphical editor does not come alone. There are 4 more panels ( in the NNC environment:For a Video on NNC see : https://www.youtube.com/watch?v=-lXjnaUSEtMNeural Network Modeler has already had a few names: DARVIZ and IBM Deep Learning IDE (https://darviz.mybluemix.net/#/) . Bravo IBM marketing or shall we call it confusing. It is now part of the Watson Studio suite.The aim of NNM is different than for the previous 2 tools. It is meant to produce code at the end (Theano, Tensorflow/Keras, Caffe 1). Now the code is to be used by other tools in the Watson Studio suite AFAIU.NNM is available here: https://darviz.mybluemix.net/#/dashboardor in the Watson Studio https://dataplatform.ibm.com/docs/content/analyze-data/ml_dlaas.html?audience=dr&context=refineryDo to that NNC provide a nice DL model Graphical Editor:Here are a few specificities and some differences with DLS:For more info on the NNM see this Medium article: https://medium.com/ibm-watson/accelerate-your-deep-learning-experiments-with-ibms-neural-network-modeler-dd0c92fba814For a video on DARVIZ (old name of NNM) see: https://www.youtube.com/watch?v=mmRw_MuMPC4 Worth seing for the the “kid” (Yes!) presenting it. Great communication and commercial skills!Deep Learning user-friendliness is on its way. But is could in the near future get much further.SONY’s NNC is a good solution to build DL models quickly and provides quite a complete environment.IBM’s NNM is a more restricted solutions with the main aim to produce code to use elsewhere. That elsewhere being the Watson Studio suite so it’s efficiency will depend on that of the WS suite and on its integration inside the WS suite.DEEP COGNITION’s DLS is a very well thought out solution, that provides a very complete environment and it is not limited to the graphical editing of the Deep Learning models. It’s orientation towards “de facto” standards makes it more interesting than SONY’s NNC that is based on a “confidential” framework.I have initiated a feature comparison table on the socialcompare.com site. The aim is to provide an easily readable table to summarize which features are present in which product. It is open to other people to contribute.Here a screen shot as of end of April 2018:Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1269,"SQL at Scale with Apache Spark SQL and DataFrames — Concepts, Architecture and Examples",2200,2018-10-25,https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f?source=collection_archive---------0-----------------------,2018,"Do you only use metrics like ‘mean’ or ‘ratio’ to make data-driven business decisions? If so, you are probably doing business analytics wrong.I’ll use two data analytics cases to show you why only focusing on those metrics can be dangerous, and what you should do instead.We use average a lot when analyzing product and business performance, but using average alone create blind spots. Because there are always variations due to different segments of the market or pure randomness, and the average value doesn’t tell you the variation of stories.A company is trying to understand the average amount of items purchased by a customer. For New York and LA, they found that the average of the items purchased per customer is the same (45 items).Now, based on the plot below, should we apply the same marketing strategy for the customers in New York and LA?NO.In LA (green line), 85% customers purchased 40–50 items, means the average amount(45) can represent most customer’s behavior. You might only need one big campaign to target the majority.However, in New York, the average value can only represent 50% of the customers’ behavior. The majority of customers, say 85%, lie between the buckets of purchasing 10 items to 80 items, which we can observe from the large ‘spread’ of data as shown by the orange ‘dumbbell’ shaped line.This means, the customers in New York has more variance than those in LA, and you probably need multiple campaign strategies for New York when the customers’ behaviors are more diversified.Find out the range around the mean by calculating variance.Typically, data scientists report a Confidence Interval (CI) to estimate where the average lies in with a probability. (This link can help you construct a Confidence Interval, and you can create it within Excel)An example for reporting is: the mean of item purchased per customer in New York is 45, and the 85% Confidence Interval is between 10 to 80.Ratio metric consists of at least two metrics; for example, Click-Through Rate is Clicks divided by Views. With each metric’s variation, the ratio metric’s variation is more complicated, and it doesn’t follow any common distribution.Let’s look at the table below first. You are measuring Click-Through Rate, from this table, it looks like Click-Through Rate increased from Jan to Feb. Sound great?Well, actually both Clicks and Views decreased, it’s just because the Views decreased more. So this increase is probably not what you want.Now, let’s look at the 4 more scenarios to see how Click-Through Rate changes when we control one variable and change the other. Can we trust the ratio with the same level of certainty in each scenario?The Left Table shows that, if the denominator (View) is stable, the ratio metric moves proportionally as the numerator (Click) moves, and the uncertainty of the data is easy to estimate, and the scale of uncertainty doesn’t change much.In the Right Table, when the denominator (View) is large enough as shown on the first few rows, ratio (CTR) is very stable with only 1–2% uncertainty. However, if you look at the bottom rows, the ratio can be very sensitive to changes and unstable when the denominator is small! When this is the case, it’s better to monitor the Views and Clicks and expect a wide range of scenarios when you make decisions.Data analytics is not just calculation, it’s also the measurement of uncertaintyWhile summary statistics like mean or some ratio metrics help us ‘Zoom Out’ and see a big picture of data and our business, we also need to ‘Zoom In’ for the range and shape of data, to make sure we understand the uncertainty associated with the metrics.Want to get more free tutorials on data analytics for your startup? Click the link or image below to get my newsletters!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1511,Clustering Ethereum Addresses,1300,2018-12-06,https://towardsdatascience.com/clustering-ethereum-addresses-18aeca61919d?source=collection_archive---------11-----------------------,2018,"Python programming language is not young at all. It was first released in 1991 by a Dutch programmer, Guido van Rossum.One of the most interesting things about python is that it is actually a single person’s work. Most of the other popular programming languages are developed and published by large companies employing lots of professionals. Python is an exception!Of course, he did not develop and evolve all the components of python by himself. It is an Open source project and thousands of people have contributed to the development of the Python. Although python has evolved a lot over the years, it is still widely used for the same purposes it was back then.Start Your Journey with The Perfect Python GuideThe main intention of developing python was to help programmers write clear, logical codes for small and large scale projects. This is the reason it became so popular among developers. Python is so versatile, it is used in Web development, Gaming, building scripts and GUI for desktop applications, configuring servers, performing scientific calculations and data analysis. Python can be used for literally everything!Among all the major programming languages, Python is rapidly growing in the past recent years. Stack Overflow survey on the growth of programming languages shows how python growth is climbing to the top among its competitors.Do You Know? Python is Chilling with NetflixSo, What actually happened in these years? How is python growing so fast?Let me tell you about some of the technologies that have contributed to rocketing the use of python language.The advancement in Artificial intelligence(AI) and Machine Learning(ML) technology has gone beyond science fiction. As Chris Duffey has spoken in his book, Superhuman Innovation,“The only limit to AI is human imagination.”Today, with the expansion in volume and intensity of data, Artificial intelligence and Machine Learning is helping in handling tasks that seemed impossible to do back in the time. All the Tech Giants (Facebook, Microsoft, Google, Amazon) are working extensively and contributing to promoting these fields.According to studies, AI and ML practitioners prefer using python because of the ease of coding and readability python provides so that we don’t get caught up in the constructs of the language.Up Your Game in Python with these Top Python ProjectsThe world is overwhelming in data. There’s a virtual tsunami of data around the globe, and every day it’s getting just bigger and bigger. Everything that we do is now being generated as data. From taking pictures to posting comments on social media, from searching for things on the internet to doing online purchases, from stock market prices to weather reports, everything is being recorded.By 2020, it is expected to create 44 zettabytes of data daily. This number means 40 times more stars in the observable universe. But data is of no use if we couldn’t collect, collate, analyze and apply it to the benefits of society. That is what data science is used for.Python is heavily used in the data science life cycle. The python community has developed excellent libraries like Numpy, Pandas, sci-kit-learn, etc for working with data. Collecting data, cleaning the dataset, extracting important features, building a machine learning model and visualizing results with graph, python provides a rich set of features to perform all these tasks.“The job of a data scientist has only grown sexier,” said Andrew Flowers, an economist at Indeed, and author of the Indeed report. “More employers than ever are looking to hire data scientists.”Every year Github conducts an annual survey, The State of the Octoverse 2018 reports shows us how the use of python has increased in recent years.Big companies like Spotify, Netflix, Quora, Facebook, and Google have Python deep into the development. Google has supported Python from the very beginning. It is now an official server-side language at google. They re-coded many scripts written in Bash or Perl into Python.Peter Norvig, the director of research at google said -“Python has been an important part of Google since the beginning and remains so as the system grows and evolves. Today dozens of Google engineers use Python, and we’re looking for more people with skills in this language.”Spotify and Netflix are heavily dependent on python for analyzing the huge data they possess on the server-side. Analyzing the information from millions of subscribers allows them to suggest better recommendations to each user and this is how they are making billions.Python isn’t a young language, it has evolved over the years and will maintain its position in trend in the upcoming years. This is the world of Python and the IT industry is just living in it.Time spent learning Python programming language is the best investment you can do.So, what are you waiting for?Spent some Quality Time with PythonWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1628,"A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn",1100,2018-12-23,https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf?source=collection_archive---------2-----------------------,2018,"You might have come across this sentiment before. There is a good chance, if you are reading this article with any purpose and intent, that you have come across this sentiment quite often. And, in fact, I am guessing that there is a better than average chance it is possible that you have personally experienced this sentiment.I certainly have. That is the genesis and the foundation of this article. For quite some time, I was genuinely frustrated to the point of distraught, as I truly and honestly believed that someone with my educational background and experience was likely incapable of understanding or working with Machine Learning techniques.The short answer is yes. We can argue over what “average” means in this context, but my own personal experience (which is admittedly quite limited) indicates to me that a background in those disciplines that are assumed to be required (e.g. computer science, calculus and other forms of higher-level math, statistics, programming, etc.), while certainly helpful, is either (i) not required at all or, (ii) if actually required, typically at a significantly more surface level than is initially perceived.I can’t tell you how to become a Machine Learning expert for no other reason than I myself am just beginning down this path. But, I do feel comfortable in saying that the path that I have started on to “Machine Learning Beginner” is accessible and should be repeatable across a broad spectrum of individuals with a wide variety of backgrounds.As I mentioned above, I have been interested in Machine Learning, Artificial Intelligence, Natural Language Processing and other areas of cutting-edge computing, but honestly and truly felt that my lack of a “technical” education background was holding me back.To be clear, by society’s bizarre standard of equating a diploma with actual intellectual accomplishment, I am considered very educated. I have an undergraduate degree in Russian Language and Literature from Dartmouth College, a master’s degree in the same from Harvard University, and a law degree from Suffolk University (in Boston, Massachusetts).Until 2016, I was practicing as a corporate attorney at a very large, very prominent, international law firm, where I focused very narrowly on private equity transactions. In the middle of 2016, I made a completely age-appropriate decision (at age 45) to abandon a very lucrative career as a partner-track attorney to become a computer programmer, a discipline in which I had no prior formal training and no real substantive knowledge.For the sake of transparency, it wasn’t quite that bad. I had always been interested in computers and had, prior to attending law school, worked for a few software companies, but any programming experience I had was (i) extraordinarily minimal, (ii) completely self-taught, and (iii) acquired in 2001–2002, which means that the knowledge was, as a practical matter, essentially useless.After about a year of self-teaching, I convinced a company to take a chance on me. I used that job to learn C#, improved my JavaScript, continued to learn about web application architecture, etc. Fast forward to today, I work at the Center for Clinical Data Science, but not as a Data Scientist.So, this is where I stand as I begin my journey into Data Science in the second half of 2019:If you believe you are, or might be, similar to me, I sincerely urge you to continue reading.Tutorials and help articles are great at trying to tell you what you should affirmatively be doing, but they traditionally spend significantly less time explaining what not to do. I believe that setting yourself up for success includes making sure to avoid potential traps and failures, and this journey has more than its fair share of those for the unwary.You will almost certainly fail in your journey to become a Machine Learning Beginner if:I can tell you as a programmer, if not as a Machine Learning practitioner, that learning to program is an exercise in patience. Computers are, in a very important (and yet ironic) sense, extraordinarily dumb machines — they do only what you tell them and literally nothing more.If you do not have the patience to sit in front of a computer and engage in the iterative process of coding, then debugging, and then researching, this is probably not going to be enjoyable for you.Regardless of the path you eventually choose to take to acquire your Machine Learning knowledge, it is almost certain that you will be given the advice to “type in all the code” or “work along with the exercises” or “go out and build the thing that we just built, but slightly different”. These are all just variants of the same idea, which is that there is value in just “tinkering” — experimenting, reading the documentation when something fails, looking at other peoples’ source code, etc. If you are not fundamentally driven and sustained by your own curiosity and a desire to “build”, you will almost certainly not keep up with those who are.This is, in essence, a re-statement of the initial concern, meaning that this is simply a concern as to whether someone with a non-traditional background can succeed in the world of Machine Learning. If this article accomplishes anything, I hope that it will at least dispel the myth that Machine Learning is completely unapproachable, or otherwise only for the lucky few that happen to have dual PhDs in Computer Science and Applied Mathematics/Statistics.I have posited that this is all do-able, not that it is easy. If it was easy, suffice it to say that everyone would be doing it, and if everyone was doing it, educational background could not, by force of logic, be a relevant factor, which means the operative question that this article attempts to address would be moot.The point simply is that there will be hard bumps in the road and how you deal with those will likely be more of a determining factor in the outcome of your success. Are you going to start scouring forums, mailings lists, Twitter, Medium, etc. and really pound the pavement until you get a satisfactory answer? Or are you going to be unsettled in the face of unfamiliar material? See “You lack patience and resolve” above.The amount of material that can/could be learned in the process of just becoming a Machine Learning Beginner is simply overwhelming in its volume. To further complicate matters, the body of scholarship and relevant work product is increasing at such a rate that we tend to measure relevance in terms of single months and years as opposed to decades.There is no “end” to the development of technology now or in the foreseeable future and to try to identify some point at which you will know “everything about machine learning” is to worship at the alter of a false god. Take your time, go over the relevant material a second and third time, write more code, build more things, document your learning process, and enjoy and celebrate the individual, smaller accomplishments along the way.So, yes, it is quite obviously possible to fail and it is not particularly hard to do. But then again, it is also not particularly hard to avoid most of these issues.There are tons of great resources out there that offer a comprehensive introduction to beginning Machine Learning concepts. One well known option is Andrew Ng’s course on Coursera. Another option is offered by fast.ai. There isn’t a right or wrong decision —both have excellent reputations, with each course offering a teaching style that is slightly different than the other.¹The salient point here is that you should make every effort to stick to a single course of study. Switching courses when you are presented with something you don’t understand is unlikely to produce meaningful progress in the average case over a statistically significant period of time. The solution is not “just one more book from Amazon” or “a different, less technical tutorial.” At some point, you simply have to buckle down, grit your teeth, and fight your way up and to the right of the learning curve.To be eminently clear, I am not suggesting that you don’t supplement your own study efforts with outside resources or otherwise consult third party materials when you have a problem. I am suggesting that at some point, as you get deeper and deeper, you will run into concepts that are difficult to understand simply because they are, in fact, difficult concepts. Embrace the difficulty as a sign that you are pushing yourself past your comfort zone, which is where (in my experience) all things that are worth discussing in this world happen. Changing courses or books, or video series just isn’t a viable long-term learning strategy and is indicative of a mindset that is looking for a “silver bullet.” As stated above, there are no silver bullets, no shortcuts — there is just hard work.Now is not the time to compare and contrast TensorFlow with PyTorch with MXNet, etc. As a complete beginner, you don’t have sufficient knowledge or understanding of either the problem space of the implementation of any of these frameworks for any such decision to be something other than a regurgitation of another’s thought process.Pick a mainstream framework with a large and active development community and stick to that. At this point, focus on learning over-arching concepts and avoid the rabbit hole that is the particulars of a given framework that may or may not be relevant a year from now.If you ever were required to study a foreign language, it’s quite possible that you have run into the concept of gendered nouns. To grossly over-simplify, in many languages (of which English is not one), certain nouns are either masculine or feminine. By way of example, take this excerpt from this article (my emphases added).There are some nouns that express entities with gender for which there is only one form, which is used regardless of the actual gender of the entity, for example, the word for person; personne; is always feminine, even if the person is male, and the word for teacher; professeur; is always masculine even if the teacher is female.The point is that, just as trying to understand the logic of why a person is always feminine regardless of gender is a fruitless exercise (the answer is inevitably “it just is — accept it as an article of truth and move on”), trying to understand everything about Machine Learning during the first pass is simply not a reasonable expectation. There are certain things that you will simply have to take on faith for the time being.That is not to say that you don’t note the question for further research at some later point in time, but trying to learn everything about everything that you don’t understand each time you confront something you don’t understand is merely a clever variation of the “You are intending to learn all of Machine Learning” discussed above.Despite what your better angels may be telling you, your drive to try to delve into the details is not helping you learn. In fact, quite the opposite, as these are rabbit holes that just potentially delay the time until you obtain a solid grasp and foundation on the larger picture that provides the necessary context for more meaningful exploration.I don’t have anything particularly interesting (and certainly nothing original) to add about the specifics of any one tool or workflow, other than to say it is important to have a set of tools that you know, are comfortable with, and that are reasonably reliable.There are multiple, well-documented solutions on everything from Google Cloud to AWS to Azure to Jupyter Notebooks to plugins for Vim, Emacs, VSCode, et al. Setting up an industry-grade Machine Learning pipeline for a single developer’s use is, as of the latter half of 2019, very affordable and very well-documented.²I don’t believe this gets enough attention. You will be, even in the initial stages of this journey, bombarded with information, some of which you will be able to grasp and some of which you will not. As stated above, given the state of our knowledge, there are certain things that you will have to take on faith for the time being. But you should absolutely noting those concepts that you do not understand so that you can eventually, when you have more context, do the deep dive that is truly required.I personally use Coggle, but at the risk of boring you with repetition, it is not really about the tool you pick. Text files, Markdown files, Jupyter Notebooks, OneNote, Evernote, etc. will all work. My personal preference stems from the fact that I prefer “mind map”-style diagrams. As opposed to a hierarchical format (as in an outline, for instance), which I often find do not line up with the mental model of the material in my mind. A mind-map allows me to connect nodes to other nodes in fairly arbitrary patterns, which allows me to document things in a way that is more natural to my brain.If you are transitioning to Machine Learning from something other than Software Engineering, this might seem non-intuitive. Put simply (and more than a little tongue-in-cheek), an overwhelming majority of the job could be colloquially described as “getting your code to compile”, which, by definition, means that the majority of time, your code is in a non-working, failing state.Of course, as human beings, we do everything in our power possible to exacerbate that feeling of inadequacy in others and ourselves by making sure that errors are shown in bold text, in bright red, accompanied by iconography that indicates that you are engaged in wrongdoing, etc.The reality is that there is an aspect of this work that, like Software Engineering (and I feel comfortable saying this after having observed more than a few Data Scientists at work), represents a somewhat tedious cycle of experimentation, bug-identification and fixing, followed by further experimentation. If you don’t have the tenacity, the drive to build, and a natural curiosity that is capable of powering you through those moments, this might not be the discipline for you.Becoming even a Machine Learning Beginner is far from a simple undertaking, but then, nothing worth doing ever is.I will leave you with a simple thought experiment — if (i) Data Science is the fastest growing job in the United States and (ii) only people who have a Data Science background can fill that job, how can (i) be true? How can it possibly be the fastest growing job?In other words, for the first (I believe, spurious) assertion to be true, wouldn’t there have to be scores of people just sitting around who, for some reason, just happen to have the perfect skill set required by an industry that didn’t meaningfully exist a decade ago. Does that seem likely?Or is it more likely that the more reasonable explanation — that otherwise capable individuals like yourselves and like me are cross-training and developing new skills in this area precisely because these skills are, in fact, accessible?I leave that as an exercise to the curious reader.N.B. — I am not associated and/or affiliated any of the organizations mentioned in this article other than my current employer, the Center for Clinical Data Science. I do not receive any discounts or other benefits from any of the companies mentioned (other than my employer) that are not publicly available to anyone with a valid credit card as of the time of this writing.[1] For the sake of complete disclosure, I am working through the fast.ai coursework. After briefly evaluating both the fast.ai coursework and Andrew Ng’s course offering, I decided that the fast.ai approach would personally serve me better. I like the “top-down” approach in which, very generally speaking, you start by “doing” and then move to “understanding”.[2] I personally use Google Cloud Platform and their AI Notebooks service, but I have also used, and was happy with, AWS SageMaker.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
856,Comparing Multi-Armed Bandit Algorithms on Marketing Use Cases,675,2018-08-03,https://towardsdatascience.com/comparing-multi-armed-bandit-algorithms-on-marketing-use-cases-8de62a851831?source=collection_archive---------2-----------------------,2018,"Today I would like to share my experience with open source machine learning library, based on gradient boosting on decision trees, developed by Russian search engine company — Yandex.Library is released under Apache license and offered as a free service.You might be familiar with gradient boosting libraries, such as XGBoost, H2O or LightGBM, but in this tutorial I’m going to give quick overview of the basis of gradient boosting and then gradually move to more core complex things.Before talking about gradient boosting I will start with decision trees. A tree as a data structure has many analogies in real life. It is used in many areas, as it is a good representation of a decision process. Tree consists of the root node, decision node and terminal node (nodes, that are not going to be splitted further). Trees are typically drawn upside down , in the sense that the leaves are at the bottom of the tree. Decision trees can be applied to both regression and classification problems.In classification problem as a criterion to make a binary split we use different metrics — the most popular one are Gini index and Cross-entropy. Gini index is a measure of total variance across K classes. In regression problem we use variance or mean deviation from medianGrowing a tree involves deciding on which features to choose and what conditions to use for splitting, along with knowing when to stop. Decision tree tend to be very complex and overfitted — which means, the error of training set will be low, but high on the validation set. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.Decision trees shows good result in non-linear dependencies. In the example above we can see that the dividing surface of each class is piecewise constant, and each side of the surface parallel to the coordinate axis, since each condition compares the value of one sign to the threshold.We can avoid overfitting in two ways: to add a stopping criteria, or to use tree pruning. Stopping criteria helps to decide, whether we need to continue dividing the tree or we can stop and turn this vertex into a leaf. For example, we can set number of objects in each node. If m > n then continue divide the tree, else stop. n == 1 — the worst case.Or we can fix the tree height.Another approach is tree pruning — we construct an overfitted tree, and then delete leaves based on selected criteria. Pruning can start at either root or the leaves.Remove branches from a “fully grown” tree — get a sequence of progressively pruned trees. On cross-validation, we compare overfitted tree with the split and without split. If the result is better without this node, we exclude it. There are many different techniques for tree pruning that are used to optimise the performance — for example reduced error pruning and cost complexity pruning, where a learning parameter (alpha) is used to weigh whether nodes can be removed based on the size of the sub-tree.The decision trees suffer from high variance. This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different.However, researchers found out that the combination of different decision trees may show much better results. Ensemble — it it when we have N base algorithm and the result on the final algorithm would be a function of the results from the base algorithm. We combine a series of k learned models with the aim of creating an improved model.There are various ensemble techniques, such as boosting — weighted vote with a collection of classifiers, bagging — averaging the prediction over a collection of classifiers and stacking— combining a set of heterogeneous classifiers.For building a tree ensemble we need to train over algorithms on different samples. But we can’t train them on one single set. We need to use randomization here to train classificators on different dataset. For example we can use bootstrap.The expectation of an error is a sum of variance, bias and noise. The ensemble consist of the trees with low bias and high variance. The main objective of gradient boosting algorithm is to keep constructing trees with low bias.For example, we need to approximate the green function on the right picture based on 10 dots with a noise. On the left graph we show the polinims trained on different samples. The averaged polynomial is shown on the right picture with the red line.We can see that the red graph is almost the same as the green one, while algorithms separately is significantly different from the green function. The following family of algorithms has low bias, but high variance.Decision trees are characterised by low bias but high variance even with a small changes in training sample. The ensemble variance is the variance of one base algorithm divided by number of algorithm + correlation between base algorithm.To reduce the impact of correlation between base algorithm we can use bagging algorithm and random subspace method. One of the most notable example of this approach is random forest classifier. This is algorithm based on method of random subspaces and bagging and use CART decision trees as a base algorithm.Method of random subspaces can help to reduce the correlation between trees and avoid overfitting. Let’s have a closer look: Suppose we have a dataset with D features, L objects and N base trees.Each of the base algorithm is fitted on sample from bootstrap.We choose d features from D randomly and construct tree until stopping criterion (which I mentioned earlier). Usually we build the overfitted trees with low bias.Number of features d for regression problem is D/3, and for classification sqrt(D).It should be emphasized that a random subset of size d is selected again eachThis is the time to split another vertex. This is the main difference of suchapproach from the method of random subspaces, where a random subset of features was chosen once before building the base algorithm.After all we apply bagging and average the result of each of the base algorithm.Random forest has various advantages, such as insensitivity to outliers, works well for large feature space, hard to overfit by adding more trees.However there is one drawback, storing ,models requires O(NK) memory storage, where K — number of trees. Which is quite a lot.Boosting is a weighted ensemble method. Each of the base algorithm is added sequentially, one by one. A series of N classifiers iteratively learned. Weights are updated to allow subsequent classifiers to “pay more attention” on training tuples that were misclassified by previous classifier. Weighted vote doesn’t influence the complexity of the algorithm, but smoothes the answers of the basic algorithms.How does boosting compare with bagging? Boosting focuses on misclassified tuples, it risk overfitting the resulting composite model to such data.• Greedy algorithm for construction linear models• Each of the following algorithms is constructed to correct the errors of existed ensemble.As an example take the standard regression task with MSE as a loss functionUsually as a base algorithm we take the most simple one, for instance we can take a short decision treeThe second algorithm must be fitted in a way to minimize the error of the composition b1(x) and b2(x)And as for bN(x)Gradient boosting is known to be one of the leading ensemble algorithm.Gradient boosting algorithm uses gradient descent methond to optimize the loss function. It is iterative algorithm and the steps are following:3. Then the algorithm would be4. Finally we add the algorithm bN to the ensembleThere are several gradient boosting libraries available: XGBoost, H20, LightGBM. The main difference between them in the tree structure, feature engineering and working with sparse dataCatboost can be used for solving problems, such as regression, classification, multi-class classification and ranking. Modes differ by the objective function, that we are trying to minimize during gradient descend. Moreover, Catboost have pre-build metrics to measure the accuracy of the model.On official catboost website you can find the comparison of Catboost (method) with major benchmarksPercentage is metric difference measured against tuned CatBoost results.Catboost introduces the followign algorithmic advances:Categorical feature is one with a discrete set of values called categories that are not comparable to each other.The main advantage of catboost is a smart preprocessing of categorical data. You don’t have to preprocess data on your own. Some of the most popular practices to encode categorical data are:One-hot encoding is a popular approach for the categorical features with small number of distinct features. Catboost use one_hot_max_size for all features with number of different values less than or equal to the given parameter value.In the case features with high cardinality(like, e.g., “user ID” feature), such technique leads to infeasibly large number of new features.Another popular method is to group categories by target statistics (TS) that estimate expected target value in each category.The problem of such greedy approach is target leakage: the new feature is computed using target of the previous one. This leads to a conditional shift — the distribution differes for training and test examples.The common methods for solving this problem are holdout TS and leave-one out TS. But still they doesn’t prevent model from target leakage.CatBoost uses a more effective strategy. It relies on the ordering principle and called Target-Based with prior (TBS). It is inspired by online learning algorithms which get training examples sequentially in time. The values of TS for each example rely only on the observed history. To adapt this idea to standard offline setting, we introduce an artificial “time”, i.e., a random permutation σ of the training examples.In Catboost, the data is randomly shuffled and mean is calculated for every object only on its historical data. Data can be reshuffled multiple times.Another important detail of CatBoost is using combinations of categorical features as additional categorical features which capture high-order dependencies like joint informa- tion of user ID and ad topic in the task of ad click prediction. The number of possible combinations grows exponentially with the number of categorical features in the dataset, and it is infeasible to process all of them. CatBoost constructs combinations in a greedy way. Namely, for each split of a tree, CatBoost combines (concatenates) all categorical features (and their combinations) already used for previous splits in the current tree with all categorical features in the dataset. Combinations are converted to TS on the fly.CatBoost implements an algorithm that allows to fight usual gradient boosting biases. The existed implementations face the statistical issue, prediction shift. The distribution F(x_k) | x_k for a training example shifts from the distribution of F(x) | x for a test example x. This problem is similar to the one that occurs in preprocessing of categorical variables, that was described above.The Catboost team derived ordered boosting, a modification of standard gradient boosting algorithm, that avoid target leakage. CatBoost has two boosting modes, Ordered and Plain. The latter mode is the standard GBDT algorithm with inbuilt ordered TS.CatBoost uses oblivious decision trees, where the same splitting criterion is used across an entire level of the tree. Such trees are balanced, less prone to overfitting, and allow speeding up prediction significantly at testing time.Here is the implementation of oblivious tree evaluation in Catboost:As you can see, there are no “if” operators in this code. You don’t need branches to evaluate an oblivious decision tree.An oblivious decision tree can be described as a list of conditions, one condition per layer. With oblivious trees you need just to evaluate all tree’s conditions, compose binary vector from them, convert this binary vector to the number and access leafs array by the index equals to this number.For example in LightGBM (XgBoost has similar impelementation)In the Ordered boosting mode, during the learning process, we maintain the supporting models Mr,j , where Mr,j(i) is the current prediction for the i-th example based on the first j examples in the permutation σr. At each iteration t of the algorithm, we sample a random permutation σr from{σ1, . . . , σs} and construct a tree Tt on the basis of it. First, for categorical features, all TS are computed according to this permutation. Second, the permutation affects the tree learning procedure.Based on Mr,j(i), the corresponding gradients are computed. While constructing a tree, we approximate the gradient G in terms of the cosine similarity where for each example i, we take the gradient based on the previous examples is σs. When the tree structure Tt (i.e., the sequence of splitting attributes) is built, we use it to boost all the models Mr′,jThe detailed information you can find in the original paper or in the NIPS’18 slidesCatBoost can be efficiently trained on several GPUs in one machine.CatBoost achieves good scalability. On 16 GPUs with InfiniBand, CatBoost runs approximately 3.75 faster than on 4 GPUs. The scalability should be even better with larger datasets. If there is enough data, we can train models on slow 1GbE networks, as two machines with two cards per machine are not significantly slower than 4 GPUs on one PCIe root complex. You can read more in this NVIDIA articleAmong described advantages also need to mention the following one:Catboost model can be integrated into Tensorflow. For example, it is a common case for combining Catboost and Tensorflow together. Neural network can be used for feature extraction for gradient boosting.Also, now Catboost model can be used in the production with the help of CoreML.I created an example of applying Catboost for solving regression problem. I used data from Allstate Claims Severity as a basement.Feel free to use my colab in your further research!Also you can find a plenty of other examples in official Catboost’s githubIn case if you want to make CatBoost better:Follow on twitter or wechat (zkid18) to stay updated with my posts.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1865,Using Image Data to Determine Text Structure,33,2019-02-01,https://towardsdatascience.com/using-image-data-to-determine-text-structure-5c361e76aae?source=collection_archive---------8-----------------------,2019,"The country has never been as polarized in its modern history as it is today. It is said that in the past presidential elections there were much more competitive counties where the margins between democratic and republican candidates. Following maps depict the trend in the past five presidential elections, and you can find more white counties where the margins were narrow in 2000 than 2016. Some obvious shifts over time include deepening red color in the Midwest and the opposite in coastal California.The margins of voting shares of candidates of the Republican and Democratic parties have widened in many counties. The standard deviation of margins has been constantly increasing over past years: 0.26 in 2000, 0.29 in 2008, and 0.35 in 2016. During this time, distribution tails, i.e. non-competitive counties, have substantially increased.In my previous article, I found that population and social profile data had a strong power to tell the 2016 presidential election results at county level. The question posited in this article is whether or not changes in population have contributed to the polarization in US politics. For instance, if cities had inflows of people whose values are aligned with the Democratic Party, they might become bluer. At the same time, if people with affinity for the Republican Party remain in the countryside, those counties would turn redder.I found, however, that shifts in population have not driven the polarization of the nation at least in 2010–2017 period, while characteristics of elections (e.g. candidates, issues) were more accountable for the polarization trend.In this analysis, I used the Data Profiles tables of the US Census American Community Survey (Five-year data) from 2010 to 2017, selecting 292 variables which are consistently available at county level. Missing values were imputed by XGBoost Regressor. During that period of time, there were two presidential elections. I constructed regression models with independent variables from US Census ACS and a dependent variable of vote share margin between Republican and Democratic candidates in the presidential elections.XGBoost Regressor was also selected as the base model for the best score among other machine learning regression models. Samples were weighted by county total voting counts. Nominal errors increase by applying the weight, as shown in the figure on the left.Yet the weighting samples leads to narrower errors in terms of voting counts. The model improves accuracy in larger counties and cities at the cost of that in smaller counties. Thus in this analysis I will use a model with sample (county) weight according to the total vote counts of counties.Firstly, I fitted the 2016 Census data and election result of the same year to the model and predicted with census data of different years. In that way we can think of hypothetical cases of 2016 election taking place in populations of different years. If the shift in population structure at county level contributed to the polarization, red counties in the result with 2010 Census data would be redder with Census data in later years.The result is not compelling. In the following chart, you see predicted margins in a model with 2010 Census data on the x axis and difference between predictions by models with 2010 and 2016 data. If there were an upward slope, we could say red counties in 2010 had wider margins in favor of Republican candidate in 2016 and vice versa. We see, instead, a flat picture below; that means shifts in voting share margins stemming from the change in population had no apparent correlations with the initial conditions and population has not been the clear driver of polarization in general.Having said that, it may be useful to know the shift in demography and social profile of counties and cities in swing states. Following charts describe voting shares of Democratic and Republican candidates predicted by 2016-model with population data in different years until 2017. In Florida where Mr. Trump won 29 electoral votes (EVs) with a narrow margin of 1.2%, the change in population structure between 2016 and 2017 favored the Republican party, while we see the opposite, benefiting Democrats in Pennsylvania whose 20 EVs were for Mr. Trump in 2016 with a 0.7% margin. The 2018 US Census ACS data will be released on December 19th, and it will be interesting to see the prediction with the latest data.Secondly, I compared two models constructed with 2012 and 2016 datasets and examined important features and errors of each model.I found many features in common in 2012 and 2016 models as shown below. Commuting by public transportation or by their own cars matters because cities (where public transportation is available) tend to be bluer than rural counties. Units in structure (i.e. living in a stand alone house or multiple-unit apartment) and number of bedrooms are also associated with urban-rural distinctions. Those variables mattered more in 2016 than 2012; the division between cities and countryside deepened over the past four years. It is also noted that race (including “Hispanic and Latino and Race”) had a larger power in 2016 as Republicans strengthened in white counties, while Democrats increased its support in counties and cities which were more racially diverse. Detailed analysis of features that drove the 2016 election results is found in my previous article.Unsurprisingly, the model errors in 2016 were statistically significantly smaller than those in 2012, indicated in the chart on the left. Population and social profiles at county level tell more about the result of the presidential election in 2016 than the previous one. That fact is one of the evidences of the deepening social cleavage in the country.Lastly, I compared predicted margins of 2012- and 2016-models with both 2016 population data. 2012-model with 2016 data would tell what would have happened if Mr. Obama and Mr. Romney had run the presidential race in 2016.It was clear that 2016-model result (y-axis) was much more polarized than hypothetical 2012-election result with 2016 population (x-axis). With the population unchanged, 2016-election resulted in a Republican candidate stronger in red counties and a Democratic candidate having more supports in blue cities.As discussed above, I found no evidence supporting the argument that change in population structure of counties contributed to the political polarization of the United States, while it may be useful to monitor shift in population and social profiles in swing states. The polarization was rather attributed to the changes in characteristics of presidential candidates, issues and elections themselves. The 2020 election is coming soon, and we have not yet seen any clues that the polarization is alleviated so far.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1760,The UN-definitive UN-comprehensive Guide to becoming at Data Scientist,333,2019-01-08,https://towardsdatascience.com/the-un-definitive-un-comprehensive-guide-to-becoming-at-data-scientist-39593b27b1dc?source=collection_archive---------17-----------------------,2019,"Natural Language Processing is a fascinating field. Since all predictors are extracted from the text, data cleaning, preprocessing and feature engineering have an even more significant impact on the model’s performance.Having worked for a few months on a machine learning project of my own involving NLP, I’ve learned one thing or two about Scikit-Learn’s vectorizers that I would like to share. Hopefully, by the end of this post, you will have some new ideas to use on your next project.As you know machines, as advanced as they may be, are not capable of understanding words and sentences in the same manner as humans do. In order to make documents’ corpora more palatable for computers, they must first be converted into some numerical structure. There are a few techniques used to achieve that, but in this post, I’m going to focus on Vector Space models a.k.a. Bag-of-Words (BoW) models.Bag-of-Words is a very intuitive approach to this problem, the methods comprise of:The vectorizer objects provided by Scikit-Learn are quite reliable right out of the box, they allow us to perform all the above steps at once efficiently, and even apply preprocessing and rules regarding the number and frequency of tokens. To top it all off, they come in three different flavors (there are other versions like DictVectorizers, but they are not that common):Here is an example of a CountVectorizer in action.Out:For a more in-depth look at each step, check this piece of code that I’ve written. It implements a simplified version of Sklearn’s CountVectorizer broken down into small functions, making it more interpretable.Vectorizers are very capable on their own, performing some surprisingly complex operations with ease. That said, by modifying these structures, one can incorporate transformations not natively supported by Scikit-Learn into the Bag-of-Words model.There are primarily two approaches used when creating custom vectorizers: instantiating the vectorizer using a modified version of Sklearn’s default analyzer and creating a class which inherits from CountVectorizer.Under the hood, Sklearn’s vectorizers call a series of functions to convert a set of documents into a document-term matrix. Out of which, three methods stand out:In a nutshell, these are the methods responsible for creating the default analyzer, preprocessor and tokenizer, which are then used to transform the documents. By instantiating the vectorizer with the correct arguments, we can easily bypass these methods and create a customized vectorizer. Let’s see an example of how to achieve that.Let’s simulate a scenario where we scrapped some text from the web. Using the default settings:Out:Now let’s try to clean up the documents set a bit by creating a vectorizer that removes HTML entities as a preprocessing step and lemmatizes the words as the document is tokenized:Out:See how “x00021” and “x0002e” were replaced by the human readable “!” and “.” while “jumps” was reduced to it’s lemma? Also, if you never used spaCy before, I insist that you take a look at its documentation. SpaCy is a highly optimized module capable of performing multiple complex NLP tasks at once, it can be a life saver.If for some reason you cannot afford to split the transformation into preprocessing and tokenization phases (maybe it’s not efficient this way), one option it to instead create a custom analyzer to perform both steps at once.Even though user defined analyzers might come handy, they will prevent the vectorizer from performing some operations such as extracting n-grams and removing stop words. Quoting Scikit-Learn’s documentation:“The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps.”For example, when using the preprocessor + tokenizer combo:The vectorizer creates unigrams, bigrams and remove stop words like “the”:On the other hand, when trying to perform the same using a custom analyzer:The output presents no bigrams and contains stop words:This happens because when a user defined analyzer is used, the build_analyzer method does not call _word_ngrams, which is responsible for removing stop words and extracting n-grams.One way to circumvent this is by creating custom vectorizer classes. The concept is pretty simple, just create a new class inheriting from the base vectorizer and overwrite the build_preprocessor, build_tokenizer and/or build_analyzer methods as desired.So, let’s try once again, but this time creating a custom vectorizer class:Out:Works like a charm! Albeit time-consuming, this method makes the analyzer aware of any argument used during the vectorizer’s instantiation, allowing for seamless integration of custom functions with rest of vectorizer.Given the different approaches to create custom vectorizers, my suggestion is to always try to use the simplest method possible first. This way you mitigate the odds of mistakes occurring and avoid a lot of unnecessary headaches.Also, it’s worth pointing out that the structures discussed in this post are very versatile in their own right, but their real potential only becomes apparent once used in conjunction with other transformers in pipelines. You can learn more about this here.I hope this post gave you a set of new tools to use on your next project!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
629,Explaining supervised learning to a kid (or your boss),1700,2018-06-05,https://towardsdatascience.com/explaining-supervised-learning-to-a-kid-c2236f423e0f?source=collection_archive---------5-----------------------,2018,"Few-shot learning is an exciting field of machine learning right now. The ability of deep neural networks to extract complex statistics and learn high level features from vast datasets is proven. Yet current deep learning approaches suffer from poor sample efficiency in stark contrast to human perception — even a child could recognise a giraffe after seeing a single picture. Fine-tuning a pre-trained model is a popular strategy to achieve high sample efficiency but it is a post-hoc hack. Can machine learning do better?Few-shot learning aims to solve these issues. In this article I will explore some recent advances in few-shot learning through a deep dive into three cutting-edge papers:I will start with a brief explanation of n-shot, k-way classification tasks which are the de-facto benchmark for few-shot learning.I’ve reproduced the main results of these papers in a single Github repository. You can check out this post to read about my experience implementing this research.The ability of a algorithm to perform few-shot learning is typically measured by its performance on n-shot, k-way tasks. These are run as follows:While there is much previous research on few-shot approaches for deep learning, Matching Networks was the first to both train and test on n-shot, k-way tasks. The appeal of this is straightforward — training and evaluating on the same tasks lets us optimise for the target task in an end-to-end fashion. Earlier approaches such as siamese networks use a pairwise verification loss to perform metric learning and then in a separate phase use the learnt metric space to perform nearest-neighbours classification. This is not optimal as the initial embedding function is trained to maximise performance on a different task! However, Matching Networks combine both embedding and classification to form an end-to-end differentiable nearest neighbours classifier.Matching Networks first embed a high dimensional sample into a low dimensional space and then perform a generalised form of nearest-neighbours classification described by the equation below.The meaning of this is that the prediction of the model, y^, is the weighted sum of the labels, y_i, of the support set, where the weights are a pairwise similarity function, a(x^, x_i), between the query example, x^, and a support set samples, x_i. The labels y_i in this equation are one-hot encoded label vectors.Notice that if we choose a(x^, x_i) to be 1/k for the closest k samples to the query sample and 0 otherwise we recover the k-nearest-neighbours algorithm. The key thing to note is that Matching Networks are end-to-end differentiable provided the attention function a(x^, x_i) is differentiable.The authors choose a straightforward softmax over cosine similarities in the embedding space as their attention function a(x, x_i). The embedding function they use for their few-shot image classification problems is a CNN which is, of course, differentiable hence making the attention and Matching Networks fully differentiable! This means its straightforward to fit the whole model end-to-end with typical methods such as stochastic gradient descent.In the above equation c represents the cosine similarity and the the functions f and g are the embedding functions for the query and support set samples respectively. Another interpretation of this equation is that the support set is a form of memory and upon seeing a new samples the network generates a prediction by retrieving the labels of samples with similar content from this memory.Interestingly the possibility for the support set and query set embedding functions, f and g, to be different is left open in order to grant more flexibility to the model. In fact Vinyals et al. do exactly this and introduce the concept of full context embeddings or FCE for short.They consider the myopic nature of the embedding functions a weakness in the sense that each element of the support set x_i gets embedded by g(x_i) in a fashion that is independent of the rest of the support set and the query sample. They propose that the embedding functions f(x^) and g(x_i) should take on the more general form f(x^, S) and g(x_i, S) where S is the support set. The reasoning behind this is that if two of the support set items are very close, e.g. we are performing fine-grained classification between dog breeds, we should change the way the samples are embedded to increase the distinguishability of these samples.In practice the authors use an LSTM to calculate the FCE of the support and then use another LSTM with attention to modify the embedding of the query sample. This results in an appreciable performance boost at the cost of introducing a bunch more computation and a slightly unappealing arbitrary ordering of the support set.All in all this is a very novel paper that develops the idea of a fully differentiable neural neighbours algorithm.In Prototypical Networks Snell et al. apply a compelling inductive bias in the form of class prototypes to achieve impressive few-shot performance — exceeding Matching Networks without the complication of FCE. The key assumption is made is that there exists an embedding in which samples from each class cluster around a single prototypical representation which is simply the mean of the individual samples. This idea streamlines n-shot classification in the case of n > 1 as classification is simply performed by taking the label of the closest class prototype.Another contribution of this paper is a persuasive theoretical argument to use euclidean distance over cosine distance in metric learning that also justifies the use of class means as prototypical representations. The key is to recognise that squared euclidean distance (but not cosine distance) is a member of a particular class of distance functions known as Bregman divergences.Consider the clustering problem of finding the centroid of a cluster of points such that the total distance between the centroid and all other points is minimised. It has been proven that if your distance function is a Bregman divergence (such as squared euclidean distance) then the centroid that satisfies this condition is simply the mean of the cluster — this is not the case for cosine distance however! This centroid is the point that minimises the loss of information when representing a set of points as just a single point.This intuition is backed up by experiments as the authors find that both ProtoNets and their own implementation of Matching Networks are improved across the board by swapping from cosine to euclidean distance.Prototypical Networks are also amenable to zero-shot learning, one can simply learn class prototypes directly from a high level description of a class such as labelled attributes or a natural language description. Once you’ve done this it’s possible to classify new images as a particular class without having seen an image of that class. In their experiments they perform zero-shot species classification of images of birds based only on attributes such as colour, shape and feather patterns.I am quite fond of this paper as it achieves the highest performance on typical benchmarks of all of the approaches in this article while also being elegant and the easiest for me to reproduce. Well done Snell et al!Finn et al. take a very different approach to few-shot learning by learning a network initialisation that can quickly adapt to new tasks — this is a form of meta-learning or learning-to-learn. The end result of this meta-learning is a model that can reach high performance on a new task with as little as a single step of regular gradient descent. The brilliance of this approach is that it can not only work for supervised regression and classification problems but also for reinforcement learning using any differentiable model!MAML does not learn on batches of samples like most deep learning algorithms but batches of tasks AKA meta-batches. For each task in a meta-batch we first initialise a new “fast model” using the weights of the base meta-learner. We then compute the gradient and hence a parameter update from samples drawn from that task and update the weights of the fast model i.e. perform typical mini-batch stochastic gradient descent on the weights of the fast model.After the parameter update we sample some more, unseen, samples from the same task and calculate the loss on the task of the updated weights (AKA fast model) of the meta-learner. The final step is to update the weights of the meta-learner by taking the gradient of the sum of losses from the post-update weights . This is in fact taking the gradient of a gradient and hence is a second-order update — the MAML algorithm differentiates through the unrolled training process.This is the key step as it means we are optimising for the performance of the base model after a gradient step i.e. we are optimising for quick and easy gradient descent. The result of this is that the meta-learner can be trained by gradient descent on datasets as small as a single example per class without overfitting.A follow up paper from OpenAI provides some valuable intuition on why this works using a Taylor expansion of the gradient update. The conclusion they come to is that MAML is not only minimising the expected loss over a distribution of tasks but also maximising the expected inner product between gradient updates from the same task. Hence it is optimising for generalisation between batches.The set of above equations shows the expectation of the gradient of MAML, a first order simplification of MAML (FOMAML) and Reptile, a first-order meta-learning algorithm introduced in the same paper. The AvgGrad term represents the loss over tasks and the AvgGradInner term represents the generalisation term. Note that to leading order in the learning rate, alpha, all of the algorithms are performing a very similar update, with second order MAML putting the highest weight on the generalisation term.Perhaps the only downside of MAML is the second order update as calculating the second derivative of the loss is very memory and compute intensive. However first order simplifications such as FOMAML and Reptile produce very similar performance which hints that the second order update can be approximated with the gradients on the updated weights.However high computational requirements have no bearing on the fact that Model-Agnostic Meta-Learning is a brilliant paper that has opened up exciting new paths for machine learning.The field of few-shot learning is making fast progress and although there is much still to be learnt I’m confident that researchers in this field will keep closing the gap between machine and human performance on the challenging task of few-shot learning. I hope you enjoyed reading this post.I’ve reproduced the main results of these papers in a single Github repository. If you’ve had enough concepts and want some juicy technical details and code you can check out this post to read about my experience implementing this research.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2252,Keeping Data Science Simple,42,2019-04-07,https://towardsdatascience.com/keeping-data-science-simple-7c6f4dc51a85?source=collection_archive---------13-----------------------,2019,"If you’re like me…You might have a somewhat unhealthy obsession with machine-learning. I definitely do… I’ve been using myself as a personal lab rat for data science activities. Its been rough, collecting data on myself and all, but the exciting part is that now I get to tell you.Over the course of the past couple weeks, I’ve been logging my web-browsing searches, along with a rating corresponding with my experience while browsing. I did this with the intention ofpowering my life with AI.Well, maybe not… Maybe I just did it for fun. Similarly to normal data science projects, a very large part of this torturous activity is actually collecting the data. I have had a lot of great ideas that simply weren’t feasible because of the data collection step. A couple of failed arduino projects (tried the brain implant, didn’t work) are sitting somewhere lost in the folders of one of my computers. Most of these brilliant ideas were related to my personal health or automating something I do everyday.For example, I was working for a solid month on software that would help me sleep, because I’m an insomniac. So the first part was to come up with a way to monitor when I woke up, and when I went to sleep… And you’re going to laugh, but I trained a neural network with a webcam that could tell if my eyes were open or closed.Take that, Google!Turns out, detecting open or closed eyes was actually really difficult, but I had a model that worked decently and I was ready to start using it. So what went wrong? Life is so full of variables that you can’t really control, and some that you can control, I made myself a test set, and the predictions weren’t even close because I’m brutally inconsistent.The idea of a completely software-based, high-level application to collect some sort of data definitely stuck out to me when I started my web-browsing project. I was going to build a web browser in Python that logs my search queries (typed into a search box in-browser), that asked me to rate my experience on a scale of one to ten every time I closed it,And yes, that got annoying.At the very inception of the project, I ran into my first big hiccup. I could not for the life of me get any form of web-kit working in GTK3+. I nearly quit entirely, rather than switching to a different GUI Framework. In the end, I settled on Qt for two main reasons:After building my minimalist browser (glad to be back in Chrome,) I made the close button request a number and dump, and setup a little script to log my web-browsing habits.After about two weeks, I was pretty confident with the amount of data I had, which was about 2,500 browser sessions, so I loaded my data into Python and decided on my model. I decided to use the tfidf transformer along with my good ole buddy,The Random Forest ClassifierAfter getting my model up and running, I was able to input a query and get about how my day was going at that time. The return was unusually negative, most results returning below 5, the most I remember being 7.But… Why?Because I’m a cynical individual, and I tend to keep my browser open for hours on end, I rated my experience on a pretty personally bias scale…Plus the browser delivered a terrible experience.After training this model, I can say confidently I was completely disappointed in myself. I did get enjoyment out of predicting the other way around, guessing what phrases I Googled that made me so upset, but it wasn’t enough to redeem the time I spent on the project in the name of science. Similarly to the other projects, this one was definitely a flop. Regardless of how terribly I think of the thing I created, I did really enjoy making it. Hopefully in the future I’ll be able to revisit this, or a similar idea and make something cooler. Maybe I’ll pay someone to use the browser for me so I don’t have to.It’s a painful experience, losing a project you loved so much because one day you woke up and said:This project is terrible.But failing is part of the fun. Data science is a fun thing because of experimentation. Testing things out is always welcome, and never up for debate. Hopefully I can come to terms with this defeat and counteract it later down the line with more data, a better browser likely written in Kivy, and less bias. Maybe one day I’ll revisit that old sleeping project, either way I’m excited to do whatever it is that I do.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1990,Unmaking Graphs,987,2019-01-31,https://towardsdatascience.com/unmaking-graphs-f584625c5bfd?source=collection_archive---------16-----------------------,2019,"The best way to learn data science is by doing it, and there’s no other alternative . From this post, I am going to reflect my learning on how I developed a machine learning model, which can classify movies reviews as positive or negative , and how I embed this model to a Python Flask web application. The ultimate goal is to sail through an end to end project. I firmly believe at the end of this post, you’ll be equipped with all the necessary skill that need to embed an ML model to a web application. I came across this end to project on the book, “Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow, 2nd Edition”[1] by Sebastian Raschka and Vahid Mirjalili. I found this book as a great investment to my data science journey and I encourage you to tryout this book.Kindly note that I am not going in discussing about the theory and concepts in details. To keep this post simple and easy to understand, I’ll provide explanations in nutshell and I’ll share links, so that you can read more on the concepts and theories when time permits. My recommendation is to follow this post from beginning to end and re-visit the new concepts in the second phase of your reading.Buckle up now, it’s going to be an amazing journey :DThis post has two main partsI set up my developer environment in Paperspace which is a cloud infrastructure provider (may be there are other uses, but I only use as an PaaS), who provides GPU based computation power to develop machine learning and deep learning models. I created a separate project folder, “Sentiment-Analysis” in a location I selected. This is the root directory of the project.First let’s go ahead and download the required movie data. I created a separate file ‘download_data.ipynb’. This will download and extract movie reviews to the folder ‘data’. When you navigate inside ‘data’ folder you’ll be able to see a folder named ‘acllmdb’. Inside ‘acllmdb’ folder, there are ‘train’ and ‘test’ folders. Both ‘train’ and ‘test’ folder contains two sub-folders named ‘pos’, which contains positive reviews and ‘neg’, which contains negative reviews(Image[1]).You’ll note that these reviews in the ‘pos’ and ‘neg’ folders are in the form of text files. For us to do data processing and manipulation easily, our next step is to create a Python Pandas data frame.Create Pandas data frame from text filesAbove script creates a Pandas data-frame ‘df’ which contains the movie reviews from the text files from both ‘pos’ and ‘neg’ sub-directories in both ‘train’ and ‘test’ folder (this step will take around 10–20 minutes depending on the performance of your PC). If the movie review is positive, we flagged the sentiment as ‘1’ and if it is negative, we flagged the sentiment as ‘0’. In our data-frame ‘df’ we’ve two columns, ‘review’ column, which contains the review as text strings and, ‘sentiment’ column, which contains the sentiment of the review as ‘1’ or ‘0’ depending on the positiveness and the negativeness of the sentiment. Image[2] contains the first five rows of the data frame ‘df’.In Image[2], we can only see the movie reviews which are positive. The reason is that, when we were creating the data-frame, the function arranged reviews in a sorted manner. For us to do better and useful data manipulations, we need to randomize the order of the movie reviews (i.e. we should see ‘0’s and ‘1’s in an uneven unsorted manner). To achieve this we can make use of in built functions ‘permutation’ and ‘random’ of NumPy library.Randomize data-frame and save as a CSV fileAs you’ll see in Image[3], now we’ve a randomized data frame and we’ve saved the data to a csv file which named as ‘movie_data.csv’.Cleaning text dataThe ‘review’ in our data frame has text. It is extremely important to look at these text very carefully. Let’s go ahead and visualize the last 999 characters from the data frame that we prepared in the above step.It is apparently clear that ‘review’ column contains HTML mark-up. These mark-up does not add any useful insight to reviews. So, as a part of text cleaning process, we have to make sure that we remove these unwanted mark up before we use these reviews to develop the model.While HTML mark-up does not contain much useful semantics, punctuation marks can represent useful, additional information in NLP context. For simplicity we are going to remove punctuation marks except for emoticons such as :) since these emoticons are certainly useful for semantic analysis. We will use Python’s regular expressions (regex) to carryout this task.These reviews are huge chunk of words. For us to analyze reviews, we need to split these reviews in to individual elements. This process is know as “tokenization”[2] in the NLP context. There are various techniques to tokenize a given text string. The simplest way is to use split() in built function in Python. Given below is a simple illustration of using split() function to tokenize set of strings to their individual elements as shown in Image[5].In Image[5] you can see that we’ve successfully tokenize the text to it’s individual elements. In the resulted output, we can see that the words ‘running’, and ‘run’. In NLP, there’s a technique to generate words into their root form. This technique is called “word stemming”[3]. “Porter Stemmer”[4] is quite popular among researchers in the NLP domain. In the below code segment you can see how we can use NLTK package’s PorterStemmer to obtain the root form of words (Image[6).In this project we are not going to look at the root form of the words. The reason is, it’s been proved that it’s not going to add a significant improvement to the model that we are going to build. For the purpose of completeness of this post, I shared this information with you.Another vital concept in the data cleaning and pre-processing step is the concept known as “stop word removal”. “stop words” are the words that are commonly occur in all forms of texts and probably bear no useful information. Few ‘stop words’ are, is, and, has, are, have, like… Stop word removal makes our text processing mechanism efficient as it reduces the number of words we need to analyze. Python NLTK provides an efficient mechanism to remove ‘stop words’ from a given text corpus. You can refer the below code snippet and Image[7] to get an understanding on the mechanism of stop words removal.Note: In the above code snippet we used the ‘tokenizer_porter’ function which was defined in the previous code snippet.As shown in the Image[7], the package ‘stopwords’ have removed the mostly occurring words such as ‘a’, ‘and’. This will reduce the size of our ‘bag of words’ (which will illustrate later in this post), hence make the computation much efficient.By now, you are aware of number of important steps in cleaning text data.Making movie reviews classifierWith this background knowledge now we can go ahead and develop the sentiment classifier. We are going to apply the above steps (except for stemming) to our created movie_data.csv file.There are two approaches to develop the classifier. One is to make use of the entire data set at once or in other words read the whole movie_data.csv file at once, create training and test set and fit the model. The drawback of this approach is that, we need to have a high performing PC/computational power. Even though I used Paperspace while trying out this tutorial, it took me almost two hours to construct my classifier with this approach. It is a very cumbersome experience. So, in this post I’m going ahead with the second approach.When working with bigger volumes of data, machine learning practitioners use online learning algorithms. Similarly, in our case we are also going to use one of the online learning algorithm known as “out-of-core learning”[5]. In very simple terms, this is where we use a portion of data set at a given time and create the model from this portion of data. We are going to update the model with the each new data portion what we feed. By following this approach we can easily construct our model with a reasonable time frame. Also, we are going to define series of functions to perform followings :and finally create the classifier with the use of above two functions. You may refer to Image[8] and the code snippet provided below to get more understanding about the classifier construction process.This is all what you need to create the movie classifier. Perhaps this might seem bit complicated now, so let me walk you through the code. You may refer Image[8] as necessary.According to Image[8], our first step is to read the csv file we created at the very beginning. In the above code snippet line #16 reads the csv file. It reads one row (document) at a time and this document then passed down to (line 53) get_minibatch() function to create a mini document. We create a mini document until the mini document’s size reach 1000 (line #53). Once this mini batch created inside the get_minibatch() function, it returns the mini batch for further processing (from line #36 to line #53). We use this mini batch and create training set variables X_train and y_train.This X_train variable is then passed down to create the bag of words (line #56). Since we are using out-of-core learning approach we’ve used scikit-learn’s HashingVectorizer. HashingVectorizer is responsible in creating the bag of words. While creating the bag of words, it will do pre-processing over the X_train, which contains the movie reviews, and will remove unnecessary HTML mark-up while removing stop words (or frequently occuring words which do not add any value to our text corpus such as ‘a’, ‘like’ ‘is’ etc.) (line #49 and line #38–44).We initialized HashingVectorizer with tokenizer funciton and set the number of features to 2**21. Furthermore, we reinitialized a logistic regression classifier by setting the loss parameter of the SGDClassifier to ‘log’. The reason to choose a large number of features in HashingVectorizer is to reduce the chance of causing hash collisions while increasing the number of coefficients in the logistic regression model.Using the for loop (line #52) we iterated over 45 mini-batches of documents where each mini batch consists of 1000 documents. Having completed the incremental learning process, we’ll use 5000 documents to evaluate the performance of our model. Below I have given the code snippet with respect to the test set generation and accuracy calculation. It is pretty much self explanatory :) (you may refer the above detailed explanation to deepen your understanding).We can see that our model has produced an accuracy of 86.7%, which is fairly okay. By now, we have completed the most important step, next step is to save this model for a later reference. Otherwise we need to perform all these steps again to come up to this point. Please do keep your current Python session open.PS: Our csv file contains 50K records, we use 45K as the training set and 5K as the test set. You can use Pandas “info” function to see the number of records in our csv.It’s going to be a pretty hectic task for us to train our model every we shut down our Python session. So we are going to save the classifier we trained and built. For this purpose we use Python’s in-built pickle module which allows us to serialize and deserialize Python objects to compact byte code. We can straight away reload these objects when we want to classify new samples.In the above code segment, we created a folder named ‘movieclassifier’ and ‘pkl_objects’ sub-directory to save serialized Python objects. What ‘dump’ method does is, it serialize the trained logistic regression model as well as ‘stop word’ set from NLTK library. I encourage you to read Python pickle documentation[6] to understand more about the package (if I am to explain here, it’s going to be another blog post :D)Our next step is to create a new Python script which we can use to import the vectorizer into the existing Python session. Let’s go ahead and create a new Python script, vectorizer.py in the movieclassifier directory that we created in the previous step.Now, at this point, we should be able to use the Python serialized objects that we created, even from a new Python session (the hassle of training is no longer going to be there ! YaY!!!). Let’s go ahead and test. Navigate to the ‘movieclassifier’ folder. Stop your current Python session. Let’s start a new Python session and see if we can load our model from the hard disk. Fingers crossed !!!Above code segment is responsible for loading the vectorizer we created and to unpicle the classifier. Now we are in a position to use these objects and pre-process document samples and make predictions about their sentiment. For example, let’s try to check if “I love this movie”, classifies as positive or negative.Awesome! It seems that our model is working correctly. We are in a position to integrate this machine learning model with the web application that we are going to develop. And that is the end of the first part of this two part series.This is going to be the “Part-2” of this post. I encourage you to get familiar with Python Flask web application development. A decent amount of Python Flask is fairly enough to follow the second part of this post. If you are a newbie, you can check Flask Web App Development series by Traversy Media.Thanks a great deal for your interest in reading this article. Please do give this article a round of applauds, leave your comments and feedback, I always appreciate and welcome your views. If you found this article useful, go ahead and share with your friends :DReferences:[1] https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939[2]https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html[3]https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html[4]http://snowball.tartarus.org/algorithms/porter/stemmer.html[5]https://simplyml.com/hunt-for-the-higgs-boson-using-out-of-core-machine-learning/[6]https://docs.python.org/3/library/pickle.htmlWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3639,Over-the-Wall Data Science and How to Avoid Its Pitfalls,4,2019-08-28,https://towardsdatascience.com/over-the-wall-data-science-and-how-to-avoid-its-pitfalls-5af6fa2eef2b?source=collection_archive---------23-----------------------,2019,"As a first time founder, I often found myself asking “what problem are we trying to solve” and seldom “what is the ultimate goal of technology and how does what we’re doing fit into it.” While you can certainly find a product-market fit by only asking the first question, I can’t really answer the personal why am I doing this without asking the second question. I also believe a company that can answer the second question has a higher ceiling to its growth — nearly all of the biggest tech companies follow this pattern.To start let’s consider the first part of this second question, what is the ultimate goal of AI technology; then we can considering how an individual company fits into it. One way I like to visualize this “some day” eventuality is through Science Fiction. Reading or writing or watching stories about our future, and noticing what it is we imagine in a utopia, or even dystopia, can provide a detailed starting place; then we can work backwards to understand an AI company’s piece of it.It seems silly to write, but one of the stories which first influenced my vision for AI in the future is Iron Man. Tony Stark, a superhero for engineers, became the benevolent genius inventor archetype for our generation and his virtual assistant a less dated version of Hal.In Iron Man our protagonist builds an AI that provides everything he wants, so let’s break down each feature that he builds and correlate it with what AI products exist today, and what barriers still exist:Each of these AI features corresponds to a piece of the market that one or more companies are currently competing for. By understanding the entire combined problem that AI companies are trying to solve, we can identify shared barriers and corresponding opportunities for collaboration such as machine learning products that will aid the creation of other machine learning products. Only by taking this macro view, I can begin to convince myself which AI products on the market today will actually make the world a better place.While this analysis might provide an interesting context for what an individual can get out of some of the AI technology being developed today, it lacks details of what AI can offer society. In my next post, I’ll provide further examples which allow us to explore the relationship between AI and existing social structures.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1986,Interactively exploring Reddit posts using basic Scala in your browser,73,2019-02-25,https://towardsdatascience.com/interactively-exploring-reddit-posts-using-basic-scala-in-your-browsers-f394843069de?source=collection_archive---------19-----------------------,2019,"Welcome to my new article. Data analysis is always a necessary skill if you want to stand out among other candidates during your job hunting. Maybe you know how to use Python or R or even SAS/Matlab to perform. But knowing SQL is often a necessary yet important skill that you should have. Many companies still use SQL a lot for data extraction and analysis. Simple but able to handing a sufficiently large amount of data help SQL still important after many years (I have coded SQL for over seven years).Therefore, I am here to demonstrate a data analysis using SQL. The platform I used for SQL is MySQL Workbench. You can download for free from the below link:And the dataset for this analysis is “House Property Sales Time Series” from Kaggle.About how to use MySQL Workbench is out of this article’s scope. You can have more information from the documentation. One thing to mention is that I code SQL with MySQL. There may be some differences if you code with Oracle or others. No need to panic if you encounter errors. Just change back to what it should be in your language.The schema is called “dataset ”and the data is stored as a table called “raw_sales”The first step is always printing some records from the dataset.So there are five columns in the dataset, datesold, postcode, price, propertyType, and bedrooms.The dataset covers from 7th Feb 2007 to 27 Jul 2019 with 29580 records.So which date has the most frequent sales?So the maximum number of sales for one day is 50 with a total price of 42M.Which postcode has the highest average price per sales?There is a total of 9 sales in postcode 2618 with an average price of 1.08M.And which year has the lowest number of sales?Is it possible to know which top five postcodes by price in each year? Of course you can, but you need to know what window function is.First I didn’t know what a window function was. But then I was asked again and again and again during interviews. So I looked it up and realized I should have known this earlier (at least helped me answer questions in an interview).Basically, window function can perform an aggregation based on the partition and return the result back to a row. If you want to know more about the syntax and usage of a window function, I recommend you read the doc from Oracle as below:So I first aggregate price by year and postcodeThen I use the window function to get the ranking of the total price by each year.Finally, select all records with ranking smaller than or equal to 5Next, I will move on to propertyType. There are two types, house, and unit.So how many sales of houses and units are there for each year?The easy way is to do a group by and then count how many records. But here I present another method by using case when. Case when function is another useful function to do aggregation with criteria. Here when I calculate the numbers of records, instead of using count function, I use sum function with a case when function inside.So for house_sales_count, if the propertyType equals house then return 1. Otherwise, it returns 0. And then sum all the records. This will show how many sales with propertyType equal to house.Another advantage is that the result looks like the Excel pivot table. You can do a comparison directly.As you can see clearly, there are way more house sales than unit sales in each year. Then the follow-up question must be about the average price difference between house and unit. Sure we can use the case when function also. But one thing to be cautious is that the else part.Instead of returning 0, it is necessary to return a null value so that that particular record will not be included when calculating the average.Again, the average price for the house is more expansive than the unit.How about further considering the number of bedrooms?The picture is different this time. The average price for the unit is higher than the house in terms of the number of bedrooms.Here is the end of this analysis. I would say SQL has undermined its importance in data analysis, especially many companies still use SQL solely for data mining and analysis. Knowing how to master SQL definitely helps you land an analytical position. I hope you learn something after reading my article. Feel free to give your comment or what you want me to introduce in the next time. Hope you enjoy and see you next time.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1759,ONNX.js: Universal Deep Learning Models in The Browser,472,2019-01-08,https://towardsdatascience.com/onnx-js-universal-deep-learning-models-in-the-browser-fbd268c67513?source=collection_archive---------16-----------------------,2019,"At work, we are working with Siamese Neural Net (NN) for one shot training on telecom data. Our goal is to create a NN that can easily detect failure in Telecom Operators networks. To do so, we are building this N dimension encoding to describe the actual status of the network. With this encoding we can then evaluate what is the status of network and detect faults. This encoding as the same goal as something like word encoding (Word2Vec or others). To train this encoding we use a Siamese Network [Koch et al.] to create a one shot encoding so it would work on any network. A simplified description of Siamese network is available here. For more details about our experiment you can read the blog of my colleague who is the master brain behind this idea.Original post on coffeeanddata.caThe current experiment, up to now is working great. This network can split the different traffic scenarios. As you can see on this picture, the good traffic (Green) is easily spitted away from error type 1 (Red) and error type 2 (Orange)So what is the problem, it seems to work fine, doesn’t it? After some reflection I realized that there was a big flaw in the loss function.First here is the code for our model. (Don’t have to read all the code, I will point out the issue.)My issue is with this line of the loss function.loss = K.maximum(basic_loss,0.0)There is a major issue here, every time your loss gets below 0, you lose information, a ton of information. First let’s look at this function.It basically does this:It tries to bring close the Anchor (current record) with the Positive (A record that is in theory similar with the Anchor) as far as possible from the Negative (A record that is different from the Anchor).The actual formula for this loss is:This process is detailed in the paper FaceNet: A Unified Embedding for Face Recognition and Clustering by Florian Schroff, Dmitry Kalenichenko and James Philbin.So as long as the negative value is further than the positive value + alpha there will be no gain for the algorithm to condense the positive and the anchor. Here is what I mean:Let’s pretend that:The loss function result will be 1.2–2.4+0.2 = -1. Then when we look at Max(-1,0) we end up with 0 as a loss. The Positive Distance could be anywhere above 1 and the loss would be the same. With this reality, it’s going to be very hard for the algorithm to reduce the distance between the Anchor and the Positive value.As a more visual example, here is 2 scenarios A and B. They both represent what the loss function measure for us.After the Max function both A and B now return 0 as their loss, which is a clear lost of information. By looking simply, we can say that B is better than A.In other words, you cannot trust the loss function result, as an example here is the result around Epoch 50. The loss (train and dev) is 0, but clearly the result is not perfect.Another famous loss function the contrastive loss describe by Yan LeCun and his team in their paper Dimensionality Reduction by Learning an Invariant Mapping is also maxing the negative result, which creates the same issue.With the title, you can easily guess what is my plan… To make a loss function that will capture the “lost” information below 0. After some basic geometry, I realized that if you contain the N dimension space where the loss is calculated you can more efficiently control this. So the first step was to modify the model. The last layer (Embedding layer) needed to be controlled in size. By using a Sigmoïde activation function instead of a linear we can guarantee that each dimension will be between 0 and 1.Then we could assume that the max value of a distance would be N. N being the number of dimensions. Example, if my anchor is at 0,0,0 and my negative point is at 1,1,1. The distance based on Schroff formula would be 1²+1²+1² = 3. So if we take into account the number of dimensions, we can deduce the max distance. So here is my proposed formula.Where N is the number of dimensions of the embedding vector. This look very similar, but by having a Sigmoïde and a proper setting for N, we can guarantee that the value will stay above 0.After some initial test, we ended up with this model.On the good side we can see that all the points from the same cluster get super tight, even to the point that they become the same. But on the downside it turns out that the two error cases (Orange and Red) got superimposed.The reason for this is the loss is smaller like this then when the Red and Orange split. So we needed to find a way to break the cost linearity; In other words, make it really costly as more the error grows.Instead of the linear cost we proposed a non-linear cost function:Where the curve is represented by this ln function when N = 3With this new non-linearity, our cost function now looks like:Where N is the number of dimensions (Number of output of your network; Number of features for your embedding) and β is a scaling factor. We suggest setting this to N, but other values could be used to modify the non-linearity cost.As you can see, the result speaks for itself:We now have very condensed cluster of points, way more than the standard triplet function result.As a reference here is the code for the loss function:Keep in mind, for it to work you need your NN last layer to be using a Sigmoïde activation function.Even after 1000 Epoch, the Lossless Triplet Loss does not generate a 0 loss like the standard Triplet Loss.Based on the cool animation of his model done by my colleague, I have decided to do the same but with a live comparison of the two losses function. Here is the live result were you can see the standard Triplet Loss (from Schroff paper) on the left and the Lossless Triplet Loss on the right:This loss function seems good, now I will need to test it on more data, different use cases to see if it is really a robust loss function. Let me know what you think about this loss function.Follow me on my blog: coffeeanddata.caWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
4666,A women’s perspective on data analysis,8,2019-11-19,https://towardsdatascience.com/a-womens-perspective-on-data-analysis-8df373c4f7fd?source=collection_archive---------45-----------------------,2019,"Every year it seems like software companies become more valuable and more central to equity markets. Just this week, the prototypical Software-as-a-Service (SaaS) company, Salesforce, was added to the Dow Jones Industrial average. It’s now the third largest component of the index. Since the Covid crisis, the accretion of value to these companies has even accelerated.Looking at Salesforce data, it’s clear that a large part of its growing valuation comes from increasing revenue. Salesforce and other SAAS companies just keep making more money.Still, that isn’t the only factor. Look at the company’s Price-to-Sales ratio, or stock ‘multiple’. Over the last 5 years it hovered around 8 before spiking to almost 12 this year. This change has many of us wondering, have SAAS valuations gotten ahead of themselves?To answer that question, brilliant venture capitalist Chamath Palihapitiya has an answer — low interest rates.This got me wondering, how well can we model tech company valuations using just interest rates? If we average all SAaS companies together, can we ignore things like revenue growth, margins, customer acquisition, lifetime value, retention, and other metrics that comprise the classic SaaS model?I attempted to answer that question using some basic modeling techniques in R. Code and data for the project are all available here. The exercise provided a few excellent lessons in the pitfalls of both simple models and the assumptions we make about software company valuation.First, I gathered a list of all the largest publicly traded SaaS companies in the United States. Mike Sonders has a great list, and I just had to add a few that he missed. Next, I collected historical price-to-sales data (aka ‘multiples’) for these companies. The best source for this was macrotrends, which has great financial information.Finally, I recorded a few measures of interest rates over the same period. I first got the effective federal funds rate from the St Louis Fed website. Then, I decided that a better measure might be the yields on 10-year and 30-year Treasury bonds as these would reflect expectations of future interest rates. Gathering this information was easy using marketwatch. The final result of the data collection was a large matrix.Each column represented an earnings quarter and each value represented a price-to-sales ration. At the bottom are interest rates, measured as perentages.To start, I wanted to see if there was any obvious visual relationship between interest rates and SaaS multiples. Since the data was already in Excel, it was easy to plot the average SaaS company P/S ratio (blue) against the 10yr yield, the 30yr yield, and the effective federal funds rate.There did seem to be a decent correlation, but the different scales made it hard to tell. The same plot with normalized data is below.Now the relationship isn’t so clear. Doing a quick check, it seems like SaaS multiples are pretty inversely correlated with the 30yr yield and to a lesser extent the 10yr. Multiples also seem to have some relation to the fed funds rate, but not an obvious one. Confirming this in R was easy.Most surprising is how closely multiple expansion is related to the simple advancement of time. Comparing SaaS multiples to a basic increasing sequence, I got a correlation of 0.86! I generated simple plots with these variables measured against each other to see if I was missing anything.One problem with comparing averages directly is that they hide some information. In this case, younger SaaS companies tend to trade for very high multiples - likely due to a combination of hype and higher revenue growth. These companies biased the average multiple for recent quarters.To avoid this bias, I normalized P/S data for each company in order to weigh all rows equally.Now, with clean data and some intuition about what I was looking for, it was time to build some basic models.The R language makes building linear models a breeze with the ‘lm’ function. Using ‘lm’ it was easy to construct simple formulas to see how closely they tracked real SaaS data. I decided to construct all kinds of polynomial relationships between the various interest rates measures to see which one performed best. Here are some of these models below in R.Each of these models represents some kind of polynomial combination of time, the 30yr yield, the 10yr yield, the fed funds rate, and normalized SaaS multiplesMany of the models I constructed were unecessarily complex. For instance, there is no obvious reason why a polynomial sum of the fed funds rate, the thirty yr yield, and the ten yr yield should perform well. Still, that is what I observed. Perhaps this was a classic case of overfitting, or else maybe there is some deeper relationship between short and long term rate expectations.The quality of these models could be easily calculated using the R ‘predict’ function. Then the ‘rmse’ function was used to calculate the root-mean-squared-error between predicted values and the actual SaaS averages.The complex models performed the best. Again, this is probably due to overfitting. The true test would be to use these models to predict future data.I selected a few of the better performing models from the previous section and looked at their formulasmodel_time = 0.01568*time + 0.22692model_fed_cubic = 0.2984*fed + 0.2884*fed² — 0.2989*fed³ + 0.4073model_thirtyyr_squared_fed_squared_tenyr_squared= -1.80371*thirty_yr + 0.22576*thirty_yr² + 0.02209*fed + 0.29625*fed² + 1.72330*ten_yr — 0.23045*ten_yr² + 0.40729With these formulas in mind, I plotted them against the actual SaaS data to see how they looked. The normalized Saas multiples are the red points, and the model predictions are the blue points.I think it’s fair to say that the relationship between tech stocks and interest rates is not completely clear. While it makes intuitive sense for lower rates to expands SaaS multiples, we need some really complex polynomial models to get a good fit with the data. I have heard finance gurus say things like ‘each half point rate cut leads to 2 points of multiple expansion’. While this has been true under certain circumstances, it certainly isn’t an absolute law.Now, there are many possible reasons for my failure to get at a nice, clean relationship between rates and multiples. Perhaps there were not enough companies to aggregate, so individual differences in businesses skewed things too much. Perhaps the relationship isn’t really linear. Multiples are very high now with a near-zero federal funds rate, but they were also high when the economy was booming and the rate was closer to 2%. Perhaps there are larger macro-level variables that are missing from my analysis, including investor sentiment. In fact, from this coarse data the best single predictor of SaaS multiples is time. Each quarter of the bull market expands multiples beyond what interest rates alone would predict. Tech folks are all hoping for some quantitative reason that they are getting so rich so fast. Maybe like our animal spirits, the real reason is unquantifiable.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
5684,Super-Convergence with just PyTorch,5,2020-02-14,https://towardsdatascience.com/super-convergence-with-just-pytorch-c223c0fc1e51?source=collection_archive---------33-----------------------,2020,"Most types of neural networks are built to make predictions on samples whose targets they have been trained on. A prime example is the MNIST dataset. A regular neural network like the MLP knows there are 10 digits and it is only based on them that it makes predictions even if the images are very different from those the network has been trained on.Now, imagine we could leverage such a network with a sequential analysis by providing a sequence of 9 ordered digits and let the network guess the 10th. The network would not only know how to discriminate the 10 digits but would also know that, in a sequence of 0 to 8, the next digit is most likely a 9.When analyzing sequenced data, we understand that most often than not, the elements within that sequence are somehow related, meaning they depend on one another. Because of this, we need to take into account each element to understand the sequence’s idea.The Cambridge University Press defines a sequence as either “the order in which things or events follow each other” or, and most importantly, “a series of related things or events”. To adjust this definition into the scope of Deep Learning, sequence is a set of data that contains trainable context, and removing some elements may render it useless.But what does a sequence contain? What pieces of grouped data could have context? And how can we extract context to leverage the power of neural networks? Before getting into the neural network itself, let me show you two types of problems frequently solved with Recurrent Neural Networks (RNNs).This first example is a Timeseries Forecasting problem, where we train a neural net with a sequence of existing numerical values (blue) in order to predict future time steps (red).If we make a sequence out of the monthly energetic expenditure of a household throughout the years, we can see that there is a rising sinusoidal trend with a sudden fall.The sinusoidal part’s context may be the different energetic requirements throughout the year from summer to winter and back to summer again. The rising energetic expenditure could come from using more electrical appliances and devices or switching to more powerful ones that may require more energy. The sudden fall’s context could mean one person grew old enough to leave home and the energy required by that person is no longer there.The better you understand the context, the more information you can feed the network with to help it understand the data, generally through concatenation of the input vector. In this case, for every month, we could concatenate three more values to the energy that would be the number of electric appliances and devices, their energetic efficiency and the number of people the household contains.Natural Language ProcessingMary rides the bycicle, the bycicle is ____.A second example is a Natural Language Processing problem. This is also a good example because the neural net must take into consideration the context provided by the existing sentence to complete it.Let’s say our network is trained to complete sentences with possessive pronouns. A well trained network would understand the sentence is built in the third person singular and that Mary is most likely a feminine name. So, the predicted pronoun should be “hers” instead of the masculine “his” or the plural “theirs”.Now that we have seen two examples of sequenced data, let’s explore the network going about its forward and backward propagation.RNN configurationsAs we have seen, RNNs extract information out of sequences to improve their predictive power.A simple RNN diagram is represented above. The green node lets in some input x^t and outputs some value h^t that is also fed to the node again containing information gathered from the input. Whatever pattern there is in what is being fed to the node, it learns it and keeps that information for the next input. The superscript t stands for a time step.There are some variations to the neural network’s configuration based on the shape of the input or output, we’ll understand what happens inside the nodes later.Many-to-one configuration is when we feed several inputs in different time steps to get one output, which could be the sentiment analysis captured in various frames of a movie scene.One-to-many uses one input to obtain several outputs. For instance, we could encode a poem conveying a certain emotion using a many-to-one configuration and use the one-to-many configuration to create new lines of the poem with that same emotion.Many-to-many uses several inputs to obtain several outputs like using a sequence of values like in the energetic usage and forecast twelve months in the future instead of only one.The stacked configuration is just a network with more than one hidden layer of nodes.RNN forward passTo understand what’s happening inside the nodes of the neural network, we’ll use a simple dataset as a Timeseries Forecasting example. Bellow is the full sequence of values and its restructuring as a training and a testing dataset.I took this example from this website which is a great resource for Deep Learning in general. Now, let’s separate the datasets into batches.I don’t show it here, but don’t forget the dataset should be normalized. It is important since neural networks are sensitive to the magnitude of the dataset values.The idea is to predict one value into the future. So, say we pick the first row of the batch: [10 20 30], after training our network we should get the value 40. To test the neural net, one could feed the vector [70 80 90] and expect to obtain a value close 100 if the network is well trained.We’ll use the many-to-one configuration feeding the three time steps of each sequence separately. When using recurrent networks, the input is not the only value that goes into the network, there is also a hidden array that is the structure that will transport the context of the sequence from node to node. We initialise it as an array of zeros and it is concatenated to the input. Its dimensionality (1 x 2) is a personal choice, just to use a different dimension from the step input of 1 x 1.Taking a closer look, we can see that the weight matrix is separated into two parts. The first deals with the input creating two outputs and the second deals with the hidden array creating also two outputs. These two sets of outputs are then added together and a new hidden array is obtained, containing information from the first input (10), and is fed to the next time step input (20). It should be noted that the weight and bias matrices are the same from time step to time step.The global input vector X^t, the weight matrix W and the bias matrix B and the calculation of the hidden arrays are represented above. Only one step missing to finish the forward pass and that is using the last hidden array to predict the next time step into the future with a linear layer to calculate the final result. The full network has the following form:Can you see the many-to-one configuration? We feed three inputs from a sequence, their context is captured by the weight and bias matrices and stored in a hidden array that is updated with new information at every time step. Finally, that context stored in the final hidden array goes through another set of weights and biases and one value is outputed after all time steps of the sequence were inputed into the network.We can see the linear form of the final hidden state and the weight and bias matrices of the linear layer as well as the prediction value calculation (y_hat).Now, this was the forward propagation of the RNN but we still haven’t seen the backward pass.The backward propagation is a very important step of the training of every neural network. Here, the error between the predicted output and the real value is propagated towards the neural net with the goal of improving the weights and biases in order to obtain better predictions with every iteration.Most times, this step gets overlooked because of its complexity. I’ll present you an explanation as simple as I can make it while mentioning the important bits.The backward pass is a series of derivations using the chain rule of calculus from the loss until all the weight and bias parameters. This means that we ultimately want the following values (or arrays if they are multi dimensional as is the case):I recommend reading about Gradient Descent if you are not familiar with the mathematical meaning of first derivatives but in essence, when the first derivative is a zero, generally it means we found a minimum value in our system and ideally we wouldn’t be able to improve it any further.A little side note here: the zero could also be maximum values, which are unstable and an optimization shouldn’t go there, or a saddle point which in itself isn’t very stable position as well. And the minimum may be global (lowest minimum of the function) or local. This doesn’t really matter to my explanation, but if you want to know more about it you can look it up!What you see in the picture are two balls rolling down the valley. Visually, the first derivative gives us a magnitude of the inclination of the hills. If we travel the direction in which the W axis increases (left to right), the inclination will be negative (going down) for the green ball and positive (going up) for the red ball.Read the next paragraph carefully and go back to the figure as needed.If we want the loss to be minimal, we want the balls to go to the lowest point in the valley. W represents the values of the weights and biases so if we are in the position of the green ball we will subtract some portion of the negative derivative (making it positive) to the W position of the green ball moving it right and subtract some portion of the positive derivative (making it negative) to the W position of the red ball moving it left in order to approach both balls to the minimum.Mathematically what we have is the following:The η adjusts the proportion of the derivative we use to update the weights and biases.Now, to get on with the backward pass issue. I’ll present the chained derivatives from the loss to all the parameters and we’ll see what each derivative represents. It is important to have the equations from the layers presented above along with their parameter’s matrices in mind.One thing to bear in mind is that the shapes of the four first derivative arrays we are looking for need to be the same as the parameters we will update. For example, the shape of the array dL/dW_h must be the same as the weight array W_h. The superscripted T means the matrix is transposed.We have found the derivatives of the final linear layer’s parameters. Knowing that dL/dy_hat has shape 1 x 1 and h³ 2 x 1 and that for matrix multiplication to happen the inner dimensions must be equal, h³ needs to be transposed. We obtain a matrix that is 1 x 2 and since W_h is 2 x 1, we need a second transposition.Now, the recurrent backward pass is slightly different. Despite the parameters are the same in all the three time steps, we still back propagate through all of them and for each hidden node, we calculate a new dL/dW_x and dL/dB_x and add to them together.The numbered superscript in the derivatives’ denominator relates to the time step. We calculated the parameters for the third time step and now we need to go back to the second time step.We have gone back to dL/dh², this means we back propagated an entire recurrent hidden node and for that we needed to deconcatenate the input and the hidden state from the global input derivative. The next two hidden states’ calculations are the same so I won’t go over them.Now we add the derivatives together and apply the Gradient Descent equation we saw earlier to update the parameters and the model is ready for another iteration. Let’s see how to build a simple RNN with PyTorch.RNN with PyTorchUsing PyTorch makes it very simple since we don’t really need to worry about the backward pass. However, I still believe it is important to know how it works even if we don’t use it directly.Moving on, if we refer to PyTorch’s documentation, we can see they already have an RNN object ready to be used. When defining it, there are two essential parameters:input_size — The number of expected features in the input xhidden_size — The number of features in the hidden state hThe input_size is 1 since we are using one time step of each sequence (eg. 10 from the sequence 10, 20, 30) at a time and the hidden_size is 2 since we obtain a hidden state containing two values.Defining the n_layers parameter to 2 would mean we have a stacked RNN with two hidden layers.Also we are going to define the parameter batch_first to True. This means the batch dimension in input and output comes first (don’t mistake with Input and Output)Inputs: input, h_0input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence.h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch.The inputs of the RNN should be an input array of shape 1 x 3 x 1. The sequence contains three time steps that are 10, 20 and 30 for the first batch of the dataset. From each batch, an input of size one will be fed three times to the network being the three time steps of the sequence.The hidden state h_0 is our first hidden array that we feed the network along with the first time step input of shape 1 x 1 x 2.Outputs: output, h_noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.The output contains all the hidden states calculated by the neural network at each time step of shape 1 x 3 x 2 and h_n is the hidden state from the last time step. This is useful to keep because, if we choose to use a stacked recurrent network, this will be the hidden state that will be fed at the first time step and has shape 1 x 1 x 2.All these arrays are represented in the example above and can be seen in the RNN diagram. One more thing to note is, using recurrent networks and the specific example of Timeseries Forecasting, setting the num_directions to 2 would mean predicting into both the future and the past. That type of configuration will not be considered here.I’ll leave a piece of code where I implement the RNN and how to train it. I’ll also leave it to you to use it as you need with the dataset you want. Don’t forget to normalise your data and to create a dataset and a dataloader before using the network.To end this story with a brief summary of what was discussed here, we started by seeing two types of problems commonly solved with recurrent networks being Timeseries Forecasting and Natural Language Processing.Later we saw examples of some typical configurations and an actual example whose objective was to predict one time step into the future using the many-to-one configuration.In the forward pass we understood how the inputs and the hidden states interact with the weights and biases of the recurrent layers and how to use the information contained in the last hidden state to predict the next time step value.The backward pass is just the application of the chain rule from the loss gradient with respect to the predictions until it becomes with respect to the parameters we want to optimize.Finally, we went through part of the documentaion of PyTorch on RNNs and discussed the most important bits in order to build a basic recurrent network.Thank you for reading! Maybe you got something out of this lengthy story. I write them to help me understand new concepts and hopefully help others too.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1286,What is a present day scraper?,5,2018-11-09,https://towardsdatascience.com/what-is-a-present-day-scraper-e2c3d3b79648?source=collection_archive---------16-----------------------,2018,"This is the follow-up of the series Basic data analysis technique, every data analyst should know. For those who haven't read part one yet, find it here.In part I we introduced the first set of techniques a starting data analyst should master, we went over: basic filtering, filtering with multiple conditions, aggregation and joins.In this part we will build forth on the first part and introduce a new set of techniques:Again we will make use of two Python libraries for our analysis, which are: pandas & numpy. If you haven’t installed these two packages yet, use the following code to install them:Anaconda:PIP:Each column (Series) in pandas has a type assigned to it, this refers to the type of data which is stored in this column.The most basic dtypes in Pandas are the following:Why is it crucial to have the right types in each column? Because it will define which actions you can perform on your data.For example, whenever a column is defined as string you cannot perform calculations on them. For example, we can create a pandas DataFrame with a values column which contains numbers but as object dtype.If we check the dtypes of this dataframe with the following code:print(df.types)We get the following output:As we can see, our Value column is in object dtype which are strings. So if we try to do a calculation on these values, we will get an error or wrong output:For example taking the square of these values:Will give a TypeError:And multiplying by 2 will not raise an error, but give us the wrong output:The reason it does not raise an error, is because in Python we can concatenate strings by multiplying:So to prevent these errors or wrong outputs, we have to convert the type of the columns to the correct ones. And in Pandas we have the Series.astype method for this.This method takes the dtype which you want to convert to as an argument:Will convert the column to int32 :Important note: this is not “inplace”. So we didn’t overwrite the original column in our DataFrame. For that we need the following code:If we check the dtypes of our DataFrame again, we can see that the conversion went correctly:The conversion of datetime is different from the other dtypes in pandas, since you have many different formats the date can be stored in:These are just a couple of the many possibilities and there are formats which can get quite complex. We won’t cover those in these series, but we will in one of my another series called Handling timeseries in Python, which will be published very soon.In pandas there are several ways to convert object type to datetime, for now we look at the two most common and used ones:Before we dive into some examples, we have to extend our DataFrame with a datetime column first, we can do this with a list of strings that represent dates:Which gives us the following DataFrame:If we look at the output of print(df.dtypes). We see that our Date column is object type (string).The to_datetime function takes a column as input, so in our example it’s df['Date']. The total code looks like the following:In our previous examples we used astype to convert string to integer. This time we want datetime64[ns] which is the datetime type from the numpy module as can be found here. So we can pass that to our astype method and convert our strings to datetime:Sometimes there’s the need to create certain overviews and to summarize your data in a certain way so it is more readable, plus it’s easier to see valuable information since the data is aggregated.Pivoting is one of these techniques that is used a lot in the scenario described above. It gives you the possibility to aggregate data and at the same time transform your data so the index and columns are replaced by the categorical data in two columns from the original dataframe.Enough talk, let’s try this method in pandas. First we need a new dataframe, which, as always, we create ourself:As we see in our dataframe, each company is repeated three times for each department, which are repeated as well. We can aggregate this data by using pivot_table method in pandas.This method takes many arguments which you can find in the documentation, but for now we will look at three of these:So in our example it will look like the following:When you work with data, there can be the need to add more information to your table based on the columns already present in your table. These columns are called conditional columns since their information depends on other columns.If we look at our table about company’s and department size:We might want to make a distinction between a big department / small department and define that in a new column called size_indicator.For now we put the threshold of a big department on 100. So if we write our logic out in pseudocode, it will look like the following:In Python we can apply this logic in several ways, we will look at the two most used methods:Since we are going to make use of the numpy module, we have to import this module first:In numpy there’s the .where method which works like the following:So in our case we have to pass the method the following:So that would result in the following:List comprehension is a method which can speed up your code significantly compared to a for and if loop while creating lists.Example for and if loop:Same list creation, but with list comprehension:Back to our data, we can apply the same logic with the list comprehension to create our Size_indicator column:In part I we already took a look at aggregating data with the groupby function in pandas. We were able to apply a single function while applying groupby, for example max or mean of the group.In pandas you have to possibility to apply multiple functions while applying groupby to create more extensive overviews.This is possible by using the aggregate method. We can pass this method a dictionary with the column name(s) and the function(s) we want to apply:As we can see above, we grouped by Company and took the sum and mean of the Employees and the count of Size_indicator.So that was it for part II: basic data analysis techniques every data analyst should know, using Python.You can find the code of this article on my GitHub in the form of a Jupyter Notebook: LinkFor any questions or other discussion, feel free to comment!Expect part III soon, where we look at some new techniques!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1044,Where did the least-square come from?,3000,2018-09-13,https://towardsdatascience.com/where-did-the-least-square-come-from-3f1abc7f7caf?source=collection_archive---------3-----------------------,2018,"The original issue of NLP (Natural Language Processing) is the encoding of a word/sentence into an understandable format for computer processing. Representation of words in a vector space allows NLP models to learn word meaning. In our previous post, we saw the Skip-gram model that captures the meaning of words given their local context. Let’s remember that by context we mean a fixed window of size n of words surrounding a target word.In this post, we are going to study the GloVe model (Global Vectors) which has been created to look at both local context and global statistics of words to embed them. The main idea behind the GloVe model is to focus on the co-occurence probabilities (equation 0 below) of words within a corpus of texts to embed them in meaningful vectors. In other terms, we are going to look at how often a word j appears in the context of a word i within all our corpus of texts.To do so, let X be our word-word co-occurence matrix (co-occurence matrix example) and X_ij be the number of times word j appears in the context of word i.GloVe will look at the ratio between those co-occurence probabilities to extract the inner meaning of words. More specifically, we are going to focus on the last line of the table in Figure 1.For words related to “ice” but not “steam” like “solid”, the ratio will be high. On the opposite, for words related to “steam” but not “ice” the ratio will be low and for words that are related to both or none of them like “water” and “fashion”, the ratio will be close to 1. At first glance, it quickly appears that the co-occurence probabilities ratio gathers more information than the raw probabilities and better captures the relationship between “ice” and “steam”. Indeed, looking only at raw probabilities, the word “water” best represents the meaning of both “ice” and “steam” and we would not be able to distinguish those 2 words inner meaning.Now that we have understood that co-occurence probabilities ratios capture relevant information about words’s relationship, the GloVe model aims to build a function F that will predict those ratios given two word vectors w_i and w_j and a context word vector w_k as inputs.The reader willing to get a high level overview of GloVe might want to skip the following equations (from equation 2 to 6) which dive a bit deeper in the understanding of how the GloVe model constructs this F function to learn word vectors representation.Let’s see how this F function is built, step by step, to catch the logic behind the final formula, which looks pretty complex at first sight (equation 6).To compare vectors w_i and w_j which are linear structures, the most intuitive way is by subtracting them, so let’s do it.We have two vectors as inputs of F and a scalar on the right-hand side of the equation, mathematically speaking it add complexity to the linear structure we want to build if we keep it that way. It is easier to associate scalar values to scalar values, this way we won’t have to play with vectors dimension. Therefore GloVe model uses the dot product of the two inputs vectors.All along we have separated word vectors from context words vectors. However, this separation is only a matter of point of view. Indeed, if “water” is a context word to “steam”, then “steam” can be a context word to “water”. This symmetry of the X matrix (our co-occurence matrix) has to be taken into account when building F, we must be able to switch w_i and w_k. First, we need F to be a homomorphism (F(a+b)=F(a)F(b)).The exp function is solution to the equation 4, exp(a-b)=exp(a)/exp(b), so let’s use it.To restore the symmetry, a bias b_k is added for the vector w_k.Thanks to our F function, we are now able to define a cost/objective function using our word vectors representations (equation 7). During the training GloVe will learn the proper word vectors w_i and w_j to minimize this weighted least square problem. Indeed, a weight function f(X_ij) must be used to cap the importance of very common co-occurrences (like “this is”) and to prevent rare co-occurrences (like “snowy Sahara”) to have the same importance as usual ones.In summary, the GloVe model uses a meaningful source of knowledge for the word analogy task we asked him to perform: the co-occurence probabilities ratios. Then, it builds an objective function J that associates word vectors to text statistics. Finally, GloVe minimises this J function by learning meaningful word vectors representations.Et voilà !References and other useful ressources:- The original Glove paper- Stanford NLP ressources- Well explained article comparing Word2vec vs GloveWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3843,Two Stories About Labeling Data by Hand — It Still Works,6,2019-09-16,https://towardsdatascience.com/two-stories-about-labeling-data-by-hand-it-still-works-fec167d74ac7?source=collection_archive---------27-----------------------,2019,"There exists the idea that practicing something for over 10000 h (ten-thousand-hours) lets you acquire enough proficiency with the subject. The concept is based on the book Outliers by M. Gladwell. The mentioned 10k hours are how much time you spend practicing or studying a subject until you have a firm grasp and can be called proficient. Though this amount of hours is somewhat arbitrary, we will take a look on how those many hours can be spent to gain proficiency in the field of Data Science.Imagine this as a learning budget in your Data-apprenticeship journey. If I were to start from scratch, this is how I would spend those 10 thousand hours to become a proficient Data Scientist.Mathematics and Statistics: Basic statistics, Bayesian statistics, Intro to Data Analysis, some Probability Theory, basic Analysis and Calculus. A Data Scientists main job is to provide insights into data from problem domains. To have a firm grasp of the underlying mathematics is essential. You can find a lot of excellent content from available university courses and Coursera courses. This is a good way to get started on your journey and get a rudimentary understanding of how statistics work. Though, fundamental courses are essential, please challenge yourself with the real deal. Lets give that 2500h (312 days of 8h work).Analysis/Stats Languages and modules: R, Python (pandas), Julia, SAS.This is the bread-and-butter for you and can be considered a part of your stats-education. Here it is about learning by doing. Reading a O’Reilly book on R will only get you so far. Load up a curated data-set, some kaggle challenge and work through some problems.There is a spectrum on which statistical languages lie. So if you’re close to scientific computing you might consider Julia. The other end of the spectrum with just statistical analysis is SAS. Some people argue that R can do both. 1000h (125 days of 8h work)Multi-purpose programming languages: Python, Go, Java, Bash, Perl, C++,… . This very much depends on the systems that you are facing on a daily basis. If you just start out, pick a language and stick to it. Once you learn the concept you will pick up different languages easier.I for myself rely heavily on a combination of Python and Bash for my daily work. Other tasks require a thorough understanding of the good old Java or even Perl to get started. 2000h (250 days of 8h work).Database Technologies: {T-, My-, U-}SQL, PostgreSQL, MongoDB, … .Relational or non-relational databases are some of the systems that you will have to work with in a production environment. It is useful to know, how your data is stored, how queries run under the hood, how to reverse a transaction. For later work your data-sources might vary a lot and it is good to have a basic understanding.750h (94 days of 8h work)Operating Systems: Windows, Linux, MacOS.Whatever your work-environment is: master it! Know the ins-and outs. You might need to install some really weird library this one time to solve a specific problem (true story). Knowing where things are and why they are there goes a long way. Know what an SSH connection is and how to run analysis on a different system. Running analysis not on your local machine is going to happen at some point in the future.500h (62 days of 8h work) .ETL/ELT Systems: This is a mixture between programming languages, database technologies and operating systems. Frameworks like Spark, Hadoop, Hive offer a advanced means for data-storage and also analysis on cluster computing platforms. Some companies may rely on a different tech-stacks like Microsoft Azure Systems, Google Cloud or AWS solutions.This goes hand in hand with Database Technologies and might already require a firm understanding of higher level programming languages (like Java or Python). There are also beginner systems, like KNIME to get your feet wet .400 (50 days of 8h)Your Problem Domain: This may be Fin-Tech, Bio-Tech, Business Analytics.You should be familiar with your field of work. Is it research and development that you are doing? Are you visualizing business processes and customer behavior? Different fields require different insights. Your data has to make sense to you and you should be able to see if a model-output is in a valid range or totally off. Spend a very absolute minimum of 350h in your problem domain. Isuggest more. A lot more.You can decide if you want to be the method jack-of-all-trades or the expert in your field. > 350h (44 days of 8h)The attentive reader sees that this only adds up to 7500 hours so far. We have a basis now and you might want to go into a certain direction from here.Back in the olden days, the dark-unenlightened ages, we had guilds. The field of working with data also has different guilds. Types of tradesmen that solve different problems and there have been different articles on the subject.Here is how those trades differ in their requirements:The figures above illustrate exemplary careers and how much time and effort can be placed in each domain. These are simply suggestions and vary from individual to individual. For example the Data-Engineer in our example could also be called a Software Engineer that handles a lot of data and another engineer would need more experience working with databases compared to software development.As with everything in life the lines are blurred. Sometimes a Data Scientist is also does the work of a ML Engineer and vice versa. Keep in mind that the 10k budget is your entry and gives you a direction into the field and not your specialization. So you have all the freedom in the world to eventually do that PhD in Parapsychology that you’ve always wanted.10k hours is a long time and if you were to work for one year that would be 27h per day and not so feasible for the average mortal.So better 3 years of work for around 9 h might be better suited.If you’re studying on the side and look for a change in your job at some point in the future, then you might be set after a couple more years. The important part is to be consistent with it. A couple of quality hours a day over a few years will get you a long way.It does not matter, if your do your 10 thousand hours at university, in school, at seminars or at home. You have to show your work. A degree, if done right, can show this — but it doesn’t have to. It is about gaining experience in the field by solving problems and gaining proficiency with concepts.Solve a problem and put it in a portfolio, your resume or simply your GitHub-repository.Whatever works best for you. People hire you for your problem solving skills. Not for the A that you got on that one project 2 years ago. You might say that it was a really cool project altogether.That’s great! if you made a dent in the universe you should tell people about it anyways.The journey is a long one and if you enjoy what you do it is worthwhile. You will learn a lot along the way.A short Disclaimer: Always keep in mind that there are different fields of Data Science related work out in the wild.Every job has its specific purpose and requires different tool-sets.Every employer might look for different sets of skills. Good luck on your journey!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2317,Sigmoid Neuron — Deep Neural Networks,230,2019-03-07,https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7?source=collection_archive---------4-----------------------,2019,"Predicting which NHL games are playoff matches using Python and the FeatureTools library.One of the main time investments I’ve seen data scientists make when building data products is manually performing feature engineering. While tools such as auto-sklearn and Auto-Keras have been able to automate much of the model fitting process when building a predictive model, determining which features to use as input to the fitting process is usually a manual process. I recently started using the FeatureTools library, which enables data scientists to also automate feature engineering.FeatureTools can be used on shallow (classic) machine learning problems, where data is available in a structured format. The library provides functionality for performing deep feature synthesis, which approximates the transformations that a data scientists would explore when performing feature engineering. The outcome of using this tool is that you can transform data from a narrow and deep representation, to a shallow and wide representation.I’ve found that this technique works best when you have many records per item that need a prediction. For example, if you are predicting if customers will churn, the input could be a collection of session events for each customer. If you only have a single record per user, then deep feature synthesis won’t be very effective. To show how this approach works, I’ll use the NHL data set available on Kaggle. This data set includes a table of games records, and a table of play records that describe each game in more detail. The goal of the predictive model I’m building is to identify which games are playoff matches, based on the plays made during the game. With no domain knowledge applied, I was able to able to build a logistic regression classifier with a high accuracy (94%) of predicting with games were playoff games. The complete Python notebook is available on github here.The remainder of this post walks through the notebook, showing how to translate from the provided Kaggle tables into an input we can use for the FeatureTools library. The first step is to load the necessary libraries. We’ll use pandas to load the tables, framequery to manipulate the data frames, hashlib to translate strings to integers, feature tools to perform deep feature synthesis, and sklearn for model fitting.Next, we’ll load the data into pandas data frames and drop string fields that will not be used for building predictions. The result is two data frames: game_df specifies if a game was a regular or playoff match, and plays_df has details about the plays made each game.The plays data is in a narrow but deep format, meaning that each game is composed of a number of different plays with only a few features. Our goal is to transform this data into a shallow but wide format, where each game is described by a single row with hundreds of different attributes. Here’s the input data sets.The next step is to translate the pandas data frames into the Entity Set representation that FeatureTools uses as input for performing deep feature synthesis. An entity set is used to describe the relationships between different tables. In this case, the plays events are the base entity, and games are a parent entity. Before describing this relationship, we first use 1-hot encoding for the event and description fields. These started out as string fields, but were converted to hash values before performing this encoding. This is the first step shown in the snippet below. The second step is to use the transformed tables, with the 1-hot encodings, as input data frames to the entity set creation, where we define a game as a collection of plays.The output of this step is an entity set that we can use to perform deep feature synthesis. The resulting object, es, has the following properties.Once we’ve encoded the data frames as an entity set, we can perform deep feature synthesis using the dfs function in FeatureTools, as shown below. The result is the features data set, which has hundreds of features, and a single row per game. We also use framequery to assign a label to each game, where playoff games have a label of 1 and regular season games have a label of 0.This results in a shallow but wide data frame, shown below. We’ve transformed the event field from a string to a 1-hot encoded field, now with a number of different aggregate operations applied. The data is now available in a format where a single record describes each game, and we can train a model to predict if a game is a playoff match.The final step is to train and test a model. To keep things simple, I used sklearn’s logistic regression model to fit the data and evaluate the results. In the snippet below, the accuracy is evaluated without using a holdout set. When I added a holdout step to the notebook, the accuracy of the model was 94.0% with an ROC AUC of 0.984.These results seem really impressive, given that no domain knowledge about hockey was used to train the model. Instead of manually coming up with features and trying to encode knowledge about hockey, I used deep feature synthesis to create a huge number of features as input to the model fitting step. Automated feature engineering is a huge step forward for data science, enabling more automation of building predictive models. Go Sharks!Ben Weber is a principal data scientist at Zynga. We are hiring!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2809,Could Machines Become Creative?,42,2019-06-13,https://towardsdatascience.com/could-machines-become-creative-49f346dcd3a3?source=collection_archive---------25-----------------------,2019,"Editor’s note: The Towards Data Science podcast’s “Climbing the Data Science Ladder” series is hosted by Jeremie Harris. Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:Most of us believe that decisions that affect us should be reached by following a reasoning process that combines data we trust with a logic that we find acceptable.As long as human beings are making these decisions, we can probe at that reasoning to find out whether we agree with it. We can ask why we were denied that bank loan, or why a judge handed down a particular sentence, for example.But today, machine learning is automating away more and more of these important decisions. Our lives are increasingly governed by decision-making processes that we can’t interrogate or understand. Worse, machine learning algorithms can exhibit bias or make serious mistakes, so a world run by algorithms risks becoming a dystopian black-box-ocracy, potentially a worse outcome than even the most imperfect human-designed systems we have today.That’s why AI ethics and AI safety have drawn so much attention in recent years, and why I was so excited to talk to Alayna Kennedy, a data scientist at IBM whose work is focused on the ethics of machine learning, and the risks associated with ML-based decision-making. Alayna has consulted with key players in the US government’s AI effort, and has expertise applying machine learning in industry as well, through previous work on neural network modelling and fraud detection.Here were some of my biggest take-homes from the conversation:You can follow Alayna on Twitter here and you can follow me on Twitter here.We are looking for guests who have something valuable to share with our audience. If you happen to know someone who would be a good fit, please let us know here: publication@towardsdatascience.com.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1500,Pairs Trading with Cryptocurrencies,1200,2018-12-05,https://towardsdatascience.com/pairs-trading-with-cryptocurrencies-e79b4a00b015?source=collection_archive---------2-----------------------,2018,"Getting all your standard data analysis done in less than 30 seconds. The wonders of Pandas Profiling.This is a really short one, but before we get started, I just want to voice a giant thank you to everyone who read and shared my last article: Python trick 101, what every new programmer should know. The reception was absolutely bonkers. The amount of claps and views completely dwarfing my other articles. So thanks and lets get on with it!Anyone working with data in Python will be familiar with the pandas package. If you’re not, pandas is the go-to package for most rows-&-columns formatted data. If you don’t have pandas yet, make sure to install it using pip install in your preferred terminal:Now, let’s see what the default pandas implementation can do for us:For those unaware of what’s happening above:Any pandas ‘DataFrame’ has a .describe() method which returns the summary above. However, notice that in the output of this method, the categorical variables are missing. In the example above the “method” column is completely omitted from the output!Let’s see if we can do any better.(hint: … we can!)How would you like it if I told you that I could produce the following statistics with just 3 lines of Python (Actually just 1 line if we don’t count our imports.):(List of features are directly from the Pandas Profiling GitHub)Well we can. By using the Pandas Profiling package! To install the Pandas Profiling package simply use pip install in your terminal:Seasoned data analysts might scoff at this at first glance for being fluffy and flashy, but it can definitely be useful for getting a quick first-hand impression of your data:The first thing you’ll see is the Overview (see the picture above) which gives you some very high-level statistics on your data and variables, as well as warnings like high correlation between variables, high skewness and more.But this isn’t even close to everything. Scrolling down we find that there are multiple parts to this report. Simply showing the output of this 1-liner with pictures wouldn’t do it any justice, so I’ve made a GIF instead:I highly recommend you to explore the features of this package yourself — after all — it’s just one line of code and you might find it useful in your future data analysis.Take a look at my new article if you like these kinds of easily applicable things to improve your Python workflows:This was just a really quick and short one. I just discovered Pandas Profiling myself and thought I would share it!Hopefully, you found it useful!If you want to see and learn more, be sure to follow me on Medium🔍 and Twitter 🐦Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
881,Common Applications of ML Algorithms,75,2018-09-12,https://towardsdatascience.com/common-applications-of-ml-algorithms-e16f21c85773?source=collection_archive---------16-----------------------,2018,"This article compares several search algorithms applied to a Traveling Salesman Problem of 85 cities. The goal is to show intuition behind some well known and effective search algorithms to people new to the subject of optimization. I chose to build less complex algorithms and attempted to describe them as understandable as possible. If you are interested you can run them yourself using the supplied R code or create your own adaptation (scroll down for link).Given a collection of cities and the cost of travel between each pair of them, the Traveling Salesman Problem (TSP) is to find the cheapest way of visiting all of the cities and returning to the starting point. The simplicity of the statement of the problem is deceptive; the TSP is one of the most intensely studied problems in computational mathematics (source). For this project ‘cost’ is defined as the direct (i.e Euclidean) distance in KM between two cities.The 85 cities that need to be visited are all the capital cities around the world with over 1 mln inhabitants as per 2006. Today there might me more capital cities with over 1 mln people, but that is not relevant to the optimization problem itself. I made an exception for the capitals of the US (Washington), Canada (Ottawa) and the Netherlands (Amsterdam). They did not have more than 1 mln inhabitants in 2006, but I added them anyway just for fun. The dataset is build from a source file that is freely available in the ‘maps’ package in R and has information on over 40.000 cities. Of course, we start all our routes in Amsterdam.One approach could be to calculate all possible routes and choose the optimal one. For this instance of TSP with 85 cities the number of possible solutions is 1.65712e+126, or 1657120067282643529694317127122958783813299277197483101064623714896696398713440263618419048770174788491009133122012443658158080 routes (formula). This is more than the number of stars in the universe (source), or the number of grains of sand in the Sahara (source). It would take many years and a quantum computer to solve the problem using this approach. Since I don’t have either at my disposal this approach is not feasible.Another approach could be to use exact algorithms like branch-and-bound or simplex. These are very robust methods that will find the optimum solution for TSP problems of up to +/-200 (on the average PC). However, visualising these in an intuitive way is challenging, if not impossible. As the main goal of this project is to show intuition of algorithms to people who are fresh on the subject, these are not suitable. Also, these methods do not scale well so applying them to more cities (the source file contains 40.000 in total) is not feasible. Hence, less opportunity to play around with larger sets.Instead, I used another powerful line of attack to this problem: search heuristics. The advantage of these methods is that you can keep them relatively simple and intuitive, while they can still find solutions that are (close to) the optimum. Also, they scale better so applying them to a larger TSP will less likely crash your machine. A disadvantage is that you will not know how far you are from the optimum. The search heuristics I used are Nearest Neighbours, Randomized Nearest Neighbours and (repeated) 2-Opt. As I will show you, these are relatively simple but also very effective and intuitive, especially when applied in combination.Nearest Neighours (NN) is one of the simplest search heuristics out there. It is part of the family of constructive search heuristics, meaning that it gradually builds a route. Starting with one city and stopping only when all cities have been visited. It is greedy in nature; at each step it chooses the location that is closest to the current location.Applied to our problem we find a route with a total distance of 112.881 KM. Every time we run the algorithm, it will generate exactly the same solution. This might seem reassuring, but it is also a big downside of this algorithm. Because of its greedy nature, it will always go for immediate gains and miss out on opportunities that will pay out in a longer term. NN has given us a feasible solution that does not look bad at all for a first try. But can we further improve on it?2-Opt is an algorithm from the local search family. These algorithms start at an initial solution and iteratively look for improvement opportunities in the neighourhood of that solution. This initial solution can be any type of solution as long as it is a feasible one. For example the outcome of a constructive algorithm like NN or a solution build from expert knowledge.The 2-opt algorithm works as follows: take 2 arcs from the route, reconnect these arcs with each other and calculate new travel distance. If this modification has led to a shorter total travel distance the current route is updated. The algorithm continues to build on the improved route and repeats the steps. This process is repeated until no more improvements are found or until a pre-specified number of iterations is completed (100 in this implementation).For example, let us take the following route: Amsterdam — Brussels — Paris — Berlin — Copenhagen — Helsinki — London — Amsterdam. One arch could be Brussel-Paris, another could be Copenhagen-Helsinki. 2-Opt exchanges the connections in these arches, i.e. the route now runs from Brussel-Copenhagen and from Paris-Helsinki.For convenience, this visualisation shows just one iteration (i.e. the Amsterdam-Brussels arc), but there could be up to 700.000 of these iterations in my implementation. Hence, for a small instance the final solution could look quite different from the initial solution. The final solution found by NN + 2-Opt is 99.242 KM. Hurray! We improved our tour!Although 2-Opt was able to improve our tour, the initial downside of NN extension still exists. We might still be improving a local optimum solution only, while never being able to capitalize on the big long term improvement opportunities. To increase our chance of ‘stumbling’ upon one of those big improvement opportunities, we will need to diversify our initial starting solutions. This means that we might propose initial solutions that might look ‘dumb’ initially, but actually are the building block for something great.This is where the Randomized Nearest Neighbours algorithm (RNN, not to be mistaken with Recurrent Neural Network) comes in play. The only difference with the regular NN algorithm is that it is not completely ‘greedy’, since at every step in building the route it considers multiple candidates (the 3 shortest distance options in my implementation) and randomly chooses 1. You can adjust ‘greediness’ of the algorithm by increasing or decreasing the number of candidates. A less greedy algorithm will produce more variance in the generated routes.The final solution when applied to our problem has almost double the travel distance, 194.936 KM, and also the plot looks very messy. But wait! Don’t throw your computer out of the window just yet. This might just be bad luck due to the randomness in our algorithm (i.e. its stochastic nature). Or we might be very lucky and it might prove to be an excellent building block for local search… Lets investigate!Next, we apply 2-Opt to the final solution from RNN to see if in fact it is a genius building block disguised as a monstrosity.Yes!!! 2-Opt took this ugly thing we started out with and turned it into a beautiful new route of just 99.430 KM. It almost seems like a fairy tale… But wait, lets not yet hit the bars to celebrate. The route is slightly longer then the route from NN + 2Opt so we have some more work to do.The random outcome of our RNN has given us a decent building block to work with. But what if there are even better building blocks out there? Due to the stochastic nature every repetition of this process produces a different outcome, i.e. a different route to start with. What if we repeat this process many times? Maybe we will stumble upon an even better building block!This algorithm runs the RNN algorithm many times and applies 2-Opt to each outcome. By repeating this process over and over we are exploring the neigbourhood of many different starting solutions. Doing this we greatly expand out search area and we have a better chance of finding local and global optima. In my implementation I am repeating the process 100 times to ensure the GIF does not become too large.Wow! Applying this combination of algorithms has decreased our current best total travel distance by a whopping 10%! Total travel distance is now 90.414 KM. Now its really time to celebrate. This algorithm has been able to find 8 improvements on our previous best route. By introducing variance in the explored initial solutions we where able to explore a lot of different solutions and improve on them iteratively. Doing this we where able to cover a lot of ground, and also explore initial solutions that looked less promising at start. The algorithm went from something very static, greedy and without inspiration to something that is able to investigate many options and generate creative solutions to the problem.The last improvement was found in the 90th iteration, so it might be worthwhile to extend the number of iterations in a next run.Optimization problems can appear to be simple, but in fact are often very complex and have an enormous number of potential solutions. Solving them can be done with exact methods, but those often require time and a lot of computing power and especially larger optimization problems will often be to complex to solve. But, applying common sense and some creativity we can build algorithms that require less processing power and can perform very well. In fact, these solutions can even outperform the exact methods if time is a constraint. I hope this paper helps people unfamiliar with optimization create some intuition in how algorithms can work and get a sense of their massive potential.Mikko VenhuisEmail: mikkovenhuis@gmail.comRun the code yourself: https://github.com/Mik3000/SearchAlgorithmsWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1983,"Linear Discriminant Analysis (LDA) 101, using R",268,2019-01-31,https://towardsdatascience.com/linear-discriminant-analysis-lda-101-using-r-6a97217a55a6?source=collection_archive---------2-----------------------,2019,"In my other posts, I have covered topics such as: How to combine machine learning and physics, and how machine learning can be used for production optimization, supply chain management as well as anomaly detection and condition monitoring. But in this post, I will discuss some of the common pitfalls of machine learning for time series forecasting.Time series forecasting is an important area of machine learning. It is important because there are so many prediction problems that involve a time component. However, while the time component adds additional information, it also makes time series problems more difficult to handle compared to many other prediction tasks.This post will go through the task of time series forecasting using machine learning, and how to avoid some of the common pitfalls. Through a concrete example, I will demonstrate how one could seemingly have a good model and decide to put it into production, whereas in reality, the model might have no predictive power whatsoever, More specifically, I will focus on how to evaluate your model accuracy, and show how relying simply on common error metrics such as mean percentage error, R2 score etc. can be very misleading if they are applied without caution.There are several types of models that can be used for time-series forecasting. In this specific example, I used a Long short-term memory network, or in short LSTM Network, which is a special kind of neural network that make predictions according to the data of previous times. It is popular for language recognition, time series analysis and much more. However, in my experience, simpler types of models actually provide just as accurate predictions in many cases. Using models such as e.g. random forest, gradient boosting regressor and time delay neural networks, temporal information can be included through a set of delays that are added to the input, so that the data is represented at different points in time. Due to their sequential nature, TDNN’s are implemented as a feedforward neural network instead of a recurrent neural network.I usually define my neural network type of models using Keras, which is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. For other types of models I usually use Scikit-Learn, which is a free software machine learning library, It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to inter-operate with the Python numerical and scientific libraries NumPy and SciPy.However, the main topic of this article is not how to implement a time series forecasting model, but rather how to evaluate the model predictions. Due to this, I will not go into the details of model building etc., as there are plenty of other blog posts and articles covering those subjects.The example data used in this case is illustrated in the figure below. I will get back to the data in more detail later, but for now, let`s assume this data represents e.g. the yearly evolution of a stock index. The data is split into a training and test set where the first 250 days are used as training data for the model, and we then try to predict the stock index during the last part of the dataset.As I do not focus on model implementation in this article, let`s proceed directly to the process of evaluating the model accuracy. Just by visually inspecting the above figure, the model predictions seem to follow the real index closely, indicating a good accuracy. However, to be a bit more precise, we can evaluate the model accuracy by plotting the real vs. predicted values in a scatter plot as illustrated below, and also calculate the common error metric R2 score.From the model predictions, we obtain an R2 score of 0.89, and seemingly a good match between the real and predicted values. However, as I will now discuss in more detail, this metric and model evaluation can be very misleading.From the above figures and calculated error metrics, the model is apparently giving accurate predictions. However, this is not the case at all, and is simply an example of how choosing the wrong accuracy metric can be very misleading when evaluating model performance. In this example, for the sake of illustration, the data was explicitly chosen to represent data that actually cannot be predicted. More specifically, the data I called “stock index”, was actually modeled using a random walk process. As the name indicates, a random walk is a completely stochastic process. Due to this, the idea of using historical data as a training set in order to learn the behavior and predict future outcomes is simply not possible. Given this, how could it then be that the model is seemingly giving us such accurate predictions? As I will get back to in more detail, it all comes down to the (wrong) choice of accuracy metric.Time series data, as the name indicates, differ from other types of data in the sense that the temporal aspect is important. On a positive note, this gives us additional information that can be used when building our machine learning model, that not only the input features contain useful information, but also the changes in input/output over time. However, while the time component adds additional information, it also makes time series problems more difficult to handle compared to many other prediction tasks.In this specific example, I used an LSTM Network that make predictions according to the data at previous times. However, when zooming in a bit on the model predictions, as indicated in the figure below, we start to see what the model is actually doing.Time series data tend to be correlated in time, and exhibit a significant autocorrelation. In this case, that means that the index at time “t+1” is quite likely close to the index at time “t”. As illustrated in the above figure to the right, what the model is actually doing is that when predicting the value at time “t+1”, it simply uses the value at time “t” as its prediction (often referred to as the persistence model). Plotting the cross-correlation between the predicted and real value (below figure), we see a clear peak at a time lag of 1 day, indicating that the model simply uses the previous value as the prediction for the futureThis means that when evaluating the model in terms of its ability of predicting the value directly, common error metrics such as mean percentage error and R2 score both indicate a high prediction accuracy. However, as the example data is generated through a random walk process, the model cannot possibly predict future outcomes. This underlines the important fact that simply evaluating the models predictive powers through directly calculating common error metrics can be very misleading, and one can easily be fooled into being overly confident in the model accuracy.A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., “stationarized”) through the use of mathematical transformations. One such basic transformation, is to time-difference the data, as illustrated in the below figure.What this transformation does, is that rather than considering the index directly, we are calculating the difference between consecutive time steps.Defining the model to predict the difference in values between time steps rather than the value itself, is a much stronger test of the models predictive powers. In that case, it cannot simply use that the data has a strong autocorrelation, and use the value at time “t” as the prediction for “t+1”. Due to this, it provides a better test of the model and if it has learnt anything useful from the training phase, and whether analyzing historical data can actually help the model predict future changes.As being able to predict the time-differenced data, rather than the data directly, is a much stronger indication of the predictive power of the model, let us try this with our model. The results of this test are illustrated in the figure below, showing a scatter-plot of the real vs. predicted values.This figure indicates that the model is not able to predict future changes based on historical events, which is the expected result in this case, since the data is generated using a completely stochastic random walk process. Being able to predict future outcomes of a stochastic process is by definition not possible, and if someone claims to do this, one should be a bit skeptical…Your time series may actually be a random walk, and some ways to check this are as follows:This last point is key for time series forecasting. Baseline forecasts with the persistence model quickly indicate whether you can do significantly better. If you can’t, you’re probably dealing with a random walk (or close to it). The human mind is hardwired to look for patterns everywhere and we must be vigilant that we are not fooling ourselves and wasting time by developing elaborate models for random walk processes.The main point I would like to emphasize through this article, is to be very careful when evaluating your model performance in terms of prediction accuracy. As shown through the above example, even for a completely random process, where predicting future outcomes is by definition impossible, one can easily be fooled. By simply defining a model, making some predictions and calculating common accuracy metrics, one could seemingly have a good model and decide to put it into production. Whereas, in reality, the model might have no predictive power whatsoever.If you are working with time series forecasting, and perhaps consider yourself a Data Scientist, I would urge you to put an emphasis on the Scientist aspect as well. Always be skeptical to what the data is telling you, ask critical questions and never draw any rash conclusions. The scientific method should be applied in data science as in any other kind of science.In the future, I believe machine learning will be used in many more ways than we are even able to imagine today. What impact do you think it will have on the various industries? I would love to hear your thoughts in the comments below.Edit: Based on the amount of interest and feedback, I have decieded to write a follow-up article on the same topic: “How (not) to use Machine Learning for time series forecasting: The sequel”If you found this article interesting, you might also like some of my other articles:Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3094,Growing your own RNN cell : Simplified,11,2019-07-10,https://towardsdatascience.com/growing-your-own-rnn-cell-simplified-b68ba2c0f082?source=collection_archive---------17-----------------------,2019,"One of the greatest challenges to security admins of public websites and SAAS applications is to detect illegitimate activity by non-human or malicious users. Luckily, when enough usage data is available, it is usually not too hard to find out that something is strange or out of the ordinary with how a certain user “behaves”. However, it is often no less of a challenge to decide exactly how to respond to such a find.In most scenarios, the best way to go is simply to blacklist or block the suspicious user (either by blocking its IP address, session, device, account, etc). Yet, users sometimes cannot just be blacklisted until further notice, and the next question that the security admin will have to answer is “for how long should we blacklist suspicious users?”, “how do we make sure we choose the right blocking time duration?”, and “how do we test ourselves?” That is especially true when the attacker is abusing the device or IP address of a legitimate user. Typically, such questions regarding maintenance and retention policies of blacklists, are answered using quite a bit of intuition and assumption-making — which is perfectly fine when the person that is using her intuition and making assumptions is sufficiently experienced.In this post, I shall propose a more robust approach to this problem that requires less reliance on intuition and much more data. My approach is based on a methodology or statistical tool known as survival analysis or ‘time-to-event’ analysis. Although I shall focus on how survival analysis can help us deal with a very specific problem, I believe that this approach and, more importantly, the tools presented here can be valuable in a variety of contexts and similar problems.Survival analysis is mostly used by researchers and scientists in order to estimate the chance of a certain “death” event occurring at a given time. It has been traditionally used to compare or determine the survival rate of different populations over time given certain circumstances or to answer questions, such as “what are the chances that the patient will survive for 3 more years given a certain treatment” or “how many people are likely to survive 10 years after diagnosed with X”.However, survival analysis was also used to estimate the chances of other kinds of death-like events. Survival analysis was used to estimate how much time users are likely to spend on a website before they leave, how long an engine will keep on going after a certain malfunction has been discovered, or how long governments “survive” under different kinds of political regimes.The analysis presented in the chart above, for example, describes a survival curve showing that non-democratic regimes are more likely to “survive” more than 20 years in power comparing to democratic regimes which on average barely last more than 10.Survival analysis is a huge topic but I prefer to cite a few good intro sources below and move on to show how it can help us resolve the problem addressed here. However, before we move on, there is just one concept that is important to explain about survival analysis in this context or, more accurately, about the form data that it is usually based on.One of the main problems that drive scientists to use survival analysis is that their data is Censored. Suppose that we are trying to estimate the life duration of a certain kind of tree before it gets too big and collapses. Suppose, also, that we have a dataset that consists of 1000 trees that we are monitoring countrywide. Now, suppose that only 50% of the trees in our data set have collapsed and the rest 500 are still standing. How should we account for these trees? Can we report their life duration before we know its end? Should we exclude these trees from our data set because we don't know anything about their expected lifespan? Our data, in other words, is right-censored — we know where it begins (on the right side of the chart) but we don't necessarily know where at least some of it ends. I'm not going to get into the theory here (you can use one of the excellent sources cited below for this), but one of the main purposes of survival estimators is exactly to account for this kind of data. Otherwise, survival analysis would have been a simpler task.Therefore, any survival analysis estimator requires at least 2 variables or features: (1) Duration (e.g., how long the tree is or was standing before it collapses); and (2) Event (whether the “death” event has already occurred — whether the tree has already collapsed). This will be clearer shortly.So suppose that we have a web application that is often the target of bots or automated attacks. Luckily, we know how to identify requests that are generated by non-humans and malicious users and we can easily block the devices or addresses that generate them. However, we also know that these devices or addresses are also used by real users, and therefore we want to block them for as little time as we can. Our security adviser recommends setting the retention policy of our blacklist to 14 days, after which the devices will not be blocked anymore, as this is a pattern that her experience shows to be quite common. Can we test this?I believe that we can and that survival analysis is a very good tool for this job. Assuming that we have the required data, we can use survival analysis to estimate how long these devices or addresses will be taken over by evil users before they stop generating non-human malicious requests. If our security expert is right, then after 14 days, we should be safe to reactivate the devices or addresses that we have blacklisted when we detect they generate non-human or malicious requests.The data that we have consists of aggregated stats of devices or addresses that were first observed about 25 days ago as behaving in a non-human manner. Two features are of particular interest to us here: days_with_bad_req, which records the number of days that the devices or addresses generated requests that we identify as non-human, and a variable named event, which tells us whether the device or address has stopped generating non-human requests or whether the “attack” still ongoing. We treat an address or device as one that has “died” or stopped generating non-human requests when it has been more than 10 days since it made any request. So, if someone took over my IP address and generated “bad” requests for 6 days and then disappeared for 14 days, my address will have days_with_bad_req = 6 and event = 1. If he took over my IP yesterday and still using it to attack the site then it will have days_with_bad_req = 2 and event = 0.There are 2 more interesting features here that we will use later on: req_day records the numbers of requests that this device or address generates per day which suppose to give us some measurements on the intensity of its activity. bad_req_days_rate is a feature that tells us the % of the time (days) that this device or address creates non-human traffic (i.e. whether or not it is exclusively used to generate malicious requests).A first look at the data shows that malicious users actually operate for a much shorter time than 14 days, though remember that some of the numbers that are reported here might be of attacks that are still ongoing.We will first confirm this claim using a KaplanMeierFitter. KM is probably the most popular univariate survival analysis estimator and the Lifeline library version is extremely easy to use and has very useful plotting functions that can immediately give us some insights. Note the only 2 variables it requires: the duration variable — which for us is the number of days in which the device or address created non-human requests, and the event variable — which in our case is equal to whether the attack by the device or address is ongoing or whether it is considered done (i.e., it has been more than 10 days since its last activity).The chart that was plotted by the KM fitter shows that more than half of the malicious users will stop their attacks after 2 days of activity and about 90% of the attackers will be gone by the end of day 5.The Lifeline library actually has an extremely useful function named percentile() which can easily help us further interpret the survival curve and determine the required boundaries. The percentile() function can take just one param which is the required the percentile so that, for example, percentile(0.1) will give us the point in time in which 90% of the population will be ‘dead’ — recall that we use survival analysis to estimate the ‘time to death’ event.Let’s take a few useful percentiles to get a better idea of how our survival curve looks like.Well, that certainly confirms the intuitions of our security expert as the point in time past which no member of the population is likely to reach is 14 days. However, if it is really important for us to remove addresses or devices from our blacklist as soon as possible, then our analysis shows that we can also set the retention time for just 10 days to avoid about 99% of the malicious non-human requests.We can also confirm this using the predict() method which takes as a param the number of days for which we want to estimate the survival chances.In most cases, it will be sufficient to use the Kaplan-Meier estimator to get the required insight, but sometimes we might need (or want) to make more accurate estimations. For example, suppose that we have good reasons to believe that non-human actors that operate more aggressively are also likely to be gone sooner. As mentioned earlier, we have another feature named bad_req_days_rate which tells us the percentage of the days in which the device or address is used to generate malicious non-human activity. We want to check how a change in this feature (and others) change our survival function.The Lifelines library provides another very useful estimator named CoxPHFitter, which is also a very popular estimator that is used for survival regression. Unlike univariate estimators like the KMF, survival regression estimators let us add more covariates into our analysis and regress them against the duration. So let's add the feature bad_req_days_rate into our model together with the feature req_day which records the general pace or velocity in which the user, address, or device operates by counting how many requests (any request) it generates every day.This summary tells us a lot of things that it will take a textbook to explain, but note a few important points: Although the p-value of both features is statistically significant (it is smaller than 0.05), the Concordance score of our model is not very impressive, which means that our model (the covariates that we choose) will not predict well the resulting life duration we want to estimate. This usually means that we need to rethink the structure of our model or even our data. However, I will ignore this for now for the sake of explanation.The second important thing to note is the values of our two features under the exp(coef) column, which is the main value we use to interpret our model. The variable req_day has an exp(coef) = 0.89, which means that one unit increase in req_day will decrease the ‘hazard’ of death and thus increase the chance that the device or address will survive over time in about 11%. On the other hand, our second parameter, bad_req_days_rate has a positive (or > 1.0) exp(coef) — 1.04. This means that one unit increase in bad_req_days_rate (e.g. a device that makes bad requests in %51 rather than 50% of its active days) will increase the hazard of death and thus decrease the chance that the user will continue making bad req over time in 4%.In other words, as we hypothesized, a more aggressive attack lasts for a shorter period of time. We can actually use the CoxPHFitter to visualize this with the plot_covariate_groups() method.This shows us the survival function of users that have bad_req_days_rate of 75% and 100%. As expected, users that launch a much more focused attack “survive” less or die sooner.We can also see this by using the predict_survival_function(). The code below shows this by creating two users that have just one difference in their bad_req_days_rate — one has 80% and the other has 95%.Conclusion: This was aimed to be a rather short and insightful introduction to survival analysis, which I was hoping to present as a powerful tool that enables us to gain interesting insights into important aspects of our data about users’ activity. Although this is just one (interesting) use case, I tried to make this simple enough to be easily used in other contexts.good luck!My notebook is available here[1] The Lifeline library[2] SciKit-Survival is a different survival analysis python library than the one I use but a fantastic resource.[2] A few more good sources here, here . And a few short ones here and here.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
5236,Dating Texts with Decision Trees,45,2019-12-27,https://towardsdatascience.com/dating-texts-with-decision-trees-c716841a33f1?source=collection_archive---------27-----------------------,2019,"This project exists because I have noticed that there is a tremendous amount of misunderstanding about what the word “demand” actually means in an economic context, especially in the world of technology.Who demands things? What is a curve? Why does the demand curve change? Why is it wrong to say that a change in price means a change in demand? What is the difference between demand and quantity demanded? What does elasticity even mean? How can I figure out the maximum total revenue?What we are going to do in this article is explore concepts in demand using R programming to analyze and visualize some realistic data I created in an Excel file.Structurally, we will use 10 hypothetical firms to analyze the demand for our hypothetical new Software as a Service (SaaS) application that charges a monthly fee per license. There is a good mix here of firms ranging from small to large with a diverse combination of what price they are willing to pay for how many licenses.We will then examine the market demand as well as break down the concept of demand elasticity in an attempt to find where we maximize total revenue.This is the first part of a multi-part series explaining economics in a technological context through project-based work.For this work, I am using RStudio on a Mac with an R Markdown (RMD) file.All the files can be downloaded from my GitHub here.Note: If you are on a Mac, DO NOT OPEN THIS FILE WITH NUMBERS. It will automatically change the encoding of the file, and R will not be happy with you. If it happens, delete that file and download a fresh copy of the Excel file.I am also assuming that you can manage to get an R environment setup on your local machine. It is best to put all your files in a single folder wherever you are mashing the buttons.Let’s get started!Sometimes Excel files can be a little tricky to import with R.What I think is the simplest and least confusing way to do it using RStudio is to create a folder to use as a working directory on your machine, then click on the “Files” tab as shown here:From there, click on the three dots (…) all the way on the right edge. You can navigate to the folder where the data and R file are located on your machine.Next, click the “More” button with a little drop down menu then click “Set As Working Directory”. That will help make the data import a little simpler and is a good habit to have because it makes managing code and files in RStudio simpler [1]. Here’s what that looks like:With that done, go ahead and click on the actual text of “Demand_Data.xlsx” to import it from there with the “Import Dataset…” option like this:That will bring up the readxl data import window like this:It should automatically work, but double check that the data looks good. Make sure the “First Row as Names” box is checked to avoid unnecessary code to wrangle that together properly. If you want to rename the data set something other than “Demand_Data”, now would be a good time to do so in order to avoid unnecessary code down the line. However, you should just leave it alone to follow along here.With that done, let’s get to mashing some buttons!We will need the readxl and tidyverse libraries for this project. If you do not have them installed in your R environment, simply remove the “#” sign before the “install.packages…” lines of code [1]. Once they are installed, you will not need to install them again on your machine.Here’s the code:Let’s check that the data came in okay. The code I have shows the way to read in the Excel file with code rather than button clicking. Make sure your Working Directory is set to the right place so you do not have to find the entire file path.We will use the head() function to check the first few rows look good.Here’s the code:Here’s the output:When working with economic data, it is always a good idea to just make a quick and dirty graph just to make sure that there is nothing weird going on. Remember, demand curves, with rare exception, trend down and to the right [2]. Let’s just pick a random column of Quantity to plot and make sure it passes the smell test here.Here’s the code:Here’s the output:I know, it is not the prettiest graph you have ever seen. The point is that we have data that generally slopes down and to the right, which is what we want to see! Thank you Law of Demand [2]!Seeing one plot is great, but what about seeing all 10 of our individual demand curves at the same time? We will use the magic of ggplot to accomplish this.If there is one thing I have learned about ggplot over time it is that the entire package is temperamental and only likes data a very specific way. Rather than try to fight the system, we need to structure our data in a way that plays nice with ggplot’s way of doing things.So, we are going to use the stack() function to do a really neat job of wrangling our data from its current form in 10 different columns to a much smaller and more ggplot appropriate form where price and quantity data is labeled by Qd_# in 2 small columns of glory.While we’re at it, we will go ahead and combine it into a single data frame with the price data to make a third column automatically putting the right price data with the right quantity and label data.Finally, we’ll change the column names from the ones generated by the stack() function to make them a little more appropriate for our project.Here’s the code:Here’s the output:With that accomplished, we can much more easily use the facet_wrap() function with a few styling parameters to elegantly show all the individual demand curves.Here’s the code:Here’s the output:There is a lot to unpack here! Let’s think about what these 10 plots of individual demands mean for us in our project of selling SaaS license subscriptions.One observation that stands out to me is that many firms are not really buying many if any units above $5–6. As the price gets lower, most firms start to buy quite a lot more licenses. Some aren’t really buying many licenses no matter what the price is like firms 4, 8, and 10.Others like firms 6, 7, and 9 are buying a lot of licenses and buying a lot more as the price goes down.One of the fundamental concept is that individual firms have individual demands [2]. Some organizations simply value our SaaS application more than others, and lowering our price may not have much of an impact on their purchasing decision [2].As a seller of monthly licenses, we need to be worried about the market demand. Let’s take a look at that.The simplest definition for market demand is that it is the sum of all the individual firms’ demand curves [2]. This means that if we have 10 firms in our market, adding up all the quantities demanded and each price will give us the market demand curve [2].Let’s take a look at how to assemble that.Here’s the code:Here’s the output:Visualizing our data can be done with a little help from ggplot() by just plotting the sum of our quantities demanded for each price.Here’s the code:Here’s the output:When I look at this, I notice instantly that in certain ranges of this market demand curve that a change in price is not going to yield the same change in quantity demanded by the market at every point on the curve because it is not linear [2].Visually inspect this, I see four distinct regions where we have similar slopes. Let’s take a look at that next and introduce the concept of elasticity.Elasticity of demand is simply the idea of seeing how the combination of prices and quantities change between two points on a curve [2]. It can be mathematically stated as the percent change in quantity divided by the percent change in price [3].In plain language, we are looking for the slope between two points just like we would using the classic y = mx + b formula from high school algebra class by looking specifically at that m value.The business question from our point of view is:By how much does the quantity demand change for each unit of price change?What I did to illustrate the point is ranges of prices different elasticity zones. It might not be the most formal way in the world to do this, but you can really see that different areas along the market demand curve have different slopes in various ranges.Here’s the code:Here’s the output:From a programming in R with ggplot point of view, it is really important to put the vector of numbers inside the as.character() function so that ggplot will automatically know that these are categorical values and not numbers to use for math [1]. We will see this in action in the next plot.While ggplot can be frustrating at times, it does offer some great features. We are going to add on to our previous market demand code by adding color = Elasticity_Zone to the aes() function so that ggplot knows to assign a different color to each zone [1].We also will add the geom_smooth() function with the method = “lm” parameter to have it make linear models for each elasticity zone [1]. We will make some dedicated models a little later to delve into that a bit further. For right now, this is clearly showing that different sections of the demand curve can have quite different slopes and elasticity numbers.Here’s the code:Here’s the output:While using linear models inside of the ggplot is great for visualization, we need to make some dedicated models to get precise slopes to compare the elasticity of each section in a meaningful way.To do that, we will use the lm() function to create linear models. There are many ways to do this and filter the data, but the goal here is to be as explicit and readable as possible.We will be doing the same basic process four timesHere’s the code:Here’s the output:Here’s the Code:Here’s the output:Here’s the Code:Here’s the output:Here’s the Code:Here’s the output:Now that we have some detailed summary statistics for the linear models for each Elasticity Zone, we need to interpret the results in the context of the project.Just like with any linear model, we want to check that our model is of high enough quality to make a reliable interpretation. That means checking that all the p-values are statistically significant and that the R-squared values are as close to 1 as possible [1].When we look at the Coefficients section of the output of each model and in the “Pr(>|t|)” column, we see that the coefficient of Price is a number that is smaller than 0.05 in all four models. That means that we can be at least 95% sure that our results are not due to chance [1]. In fact, we can be at least 99% sure that the relationship between price and quantity demand is not due to chance because all the p-values in this column are smaller than 0.01 [1].Depending on who you ask, .7 and higher for either Multiple or Adjusted R-squared is pretty good [1]. R-squared measures how much of the dependent variable is explained by the independent variable(s) in the model [1]. In all four cases, we see we are explaining 99%+ of our dependent variable [1].With that in mind, we want to look at the “Price” row in in the “Estimate” column for our slope value [1]. While it technically would be better to switch the price and quantity around to be more proper, it is easier to understand like this and really gets us to the same place for business purposes anyway [3].Let’s just take a look at the last model of Zone 4. Here’s how you should read that:As the price decreases by 1 unit, the quantity demanded increases by 540.4 units.This aligns with the Law of Demand because as we lower our price, more units are demanded [2].Once we know that everything is significant and does a good job of explaining the results, let’s make a comparison of results based on solid evidence.The simplest way to do this is going to be creating a little table with the slope coefficients pulled out from the summary statistics.We will do this by making a data frame with labeled columns and viewing the output of the summary statistics as a table. The …coefficients[2,1]… part of the model is navigating through the output to get the second row and first column of the output of the summary statistics, which is the slope value [1]. Finally, we will round the results to 2 decimal places for our sanity and general readability.Here’s the code:Here’s the output:Again, yes, I understand that is not technically a bunch of slopes in a purely mathematical sense [3]. It is really how much the quantity demanded changes per unit of price change [3]. It is just much easier to understand the idea this way.Now it is time to quickly update the Market Demand plot from earlier by annotating the change in Quantity Demanded per unit of price so it is clear what is happening. We will need to paste some text and numbers together, position the text appropriately, and use the abs() function to get the absolute value of the numbers so they positive and easier to read.Here’s the code:Here’s the output:So what does this all really mean?When prices are high, the market is relatively more inelastic than when prices are really low [2]. We get a much smaller effect in terms of number of subscriptions demanded when we move from a price of $10 to $9 than we do from $2 to $1 where the curve is relatively more elastic [2]. In fact, it is almost a 14x difference in number of subscriptions demanded going from a point in zone 1 to a point in zone 2.The real question we need to ask here is:How does this help us find where we would maximize total revenue?Good question! Let’s quickly find out.First, we need to create a column to our market data that simply multiples price by quantity demanded to get total revenue [2].Here’s the code:Here’s the output:Let’s make a plot to see where total revenue gets maximized.Here’s the code:Here’s the output:Looking at the chart, we see that a price of $2.50 is where the maximum total revenue given our market demand data, which is the third elasticity zone. What this means is that as we raise the price from free to $2, we keep increasing revenue [2]. We maximize total revenue at a price of $2.50 with a total revenue of $1,850 [2]. As we continue raising the price from $3 and up, we actually make less revenue [2]!The result of this story is that it pays to understand the market. We started off gathering data about individual firms in the market for our SaaS subscription. What were they willing to buy at each price? We then combined that into market data. It is interesting to uncover that demand curves are not always straight lines. In different ranges of the curve, small price changes can have a range of effects on number of units demanded and, as we saw at the end, in terms of maximum total revenue.As I noted at the beginning of this article, this is the first part of many parts explaining economic concepts in an easy to digest way specifically for technology people.I hope you enjoyed this article. Please let me know any feedback you have or suggestions for future work![1] R. Kabacoff, R in Action (2nd ed.) (2015), Shelter Island, NY: Manning Publications Co.[2] F. Mishkin, The Economics of Money, Banking, & Financial Markets (9th ed.) (2010), Boston, MA: Pearson Education, Inc.[3] P. Agarwal, Price Elasticity of Demand (PED), https://www.intelligenteconomist.com/price-elasticity-of-demand/Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2425,Deep Reinforcement Learning using Unity ml-agents,280,2019-03-19,https://towardsdatascience.com/deep-reinforcement-learning-using-unity-ml-agents-8af8d407dd5a?source=collection_archive---------5-----------------------,2019,"This year can be considered the booming of Artificial Intelligence (AI). Just look at the number of startups with the term AI in their taglines; where acquisitions from big companies focused on; and the topics at the biggest tech conferences. AI is everywhere — even if just as a buzzword. But what actually is AI? That’s a rather difficult and controversial question to answer.What is clear is that data science is solving problems. Data is everywhere, and the uses we are making out of it (science) are increasing and impacting society more and more. Let’s focus on Data Science, while other philosophize on the best definition of AI.While other buzzwords keep thriving, how’s data science?The interest is not bad at all! I keep making my stand that data science is not a buzzword. Even for people now joining data science — and there are a lot of them — you just need to make a quick job search on LinkedIn and you’ll be amazed by the number of offers. Let’s start by taking a look at what has happened in 2018 and then focus on hot topics for 2019.Last year, I published an article on my expectations on Data Science trends for 2018. The main developments I mentioned were: automation of workflows, explainability, fairness, commoditization of data science and improvements in feature engineering/cleaning tools.Regarding automation, the data scientists’ job is, very often, the automation of their own work. Companies open sourcing their own automation pipelines is common nowadays. Others, just keep selling it, but every day with more competition (e.g., Microsoft Azure, H2O, Data Robot, Google). Fortunately, data science is a transversal discipline and the same algorithms that are used in healthcare can be used, with some tweaks, in agriculture. So, if a company fails in a vertical, their developments can be quickly adapted to another field.These tools are becoming regular commodities that you don’t even need to know how to code to use them. Some of them were born out of the scarcity of data science talent some years ago and were turned into profitable products afterward. This recalls one of the principles of Rework book — sell your by-products.Explainability and fairness saw great developments in 2018. There are now many more available resources. Tools that were just Python alpha versions have matured (e.g., SHAP). Also, you can easily find structured and supported books on the topic, such as Interpretable Machine Learning book, by Christoph Molnar. Understanding highly complex models is going in the right direction by decreasing barriers — Google’s What-If Tool is a great example.Feature engineering is still one of the main secret sauces of Data Science solutions — take a look at the description of the winning solution for Home Credit Default Risk in Kaggle. While much of the best features are still manually created, Feature Tools became one of the main feature engineering libraries this year, for the lazy (smart?) data scientist. The problem of these tools is that you need to have data standards across your business, i.e., if one of your clients delivers data in one format, you should make sure the second client follows the same procedure — otherwise, you’re going to have a lot of undesirable manual work.Finally, if we delivered Oscars to programming languages, Python would probably receive some of them. It is today the fastest-growing major programming language and the most wanted language for the second year in a row, according to Stack Overflow. At this rate, it is quickly becoming the most used programming language.So, what’s next? What can still be done?There is plenty to be done in the above topics. And they will continue to be some of the main focus of data scientists in 2019, and the following years. The focus will be on maturing technologies while answering the questions:But, besides these meta-questions, that are difficult to answer, what are the promising topics?Reinforcement Learning might have gone through a lot of winters during its life. However, it looks like we are approaching another spring. A great example is the fantastic performance in Dota 2. There is a lot to be done, and a lot of computational power will be needed… But, anyway, reinforcement learning is the most human-like learning behavior we currently have and it’s exciting to see its applications.We’ll most probably start seeing these proof-of-concepts turned into actual products. If you have the time, take a look at them and use OpenAI gym to develop them.GDPR’s Recital 71: The data subject should have “the right… to obtain an explanation of the decision reached… and to challenge the decision.”General Data Protection Regulation (GDPR) is in effect in EU since 25th of May 2018 and directly affects data science. The problem is: companies are still understanding the limits of this new regulation. Two of the main open topics are:Trustworthy AI has two components: (1) it should respect fundamental rights, applicable regulation and core principles and values, ensuring an “ethical purpose” and (2) it should be technically robust and reliable since, even with good intentions, a lack of technological mastery can cause unintentional harm [EU AI Ethics]As algorithms affect society more, we are entitled to make sure biases are mitigated, and their use is towards the benefit of the whole and not just a few. Fortunately, companies and institutions are working on this. The EU AI Ethics draft and the Google AI principles are perfect examples. There’s still a long way forward for ethics, but it’s now a recurrent discussed topic — and that’s good.As algorithms become more complex, and more data is readily available (every gadget now generates data, right?), fewer people will be just using their laptops to do data science. We’ll use cloud-based solutions, even for the simplest projects (e.g., Google Colab). Time is scarce, GPUs are not… Laptops are not evolving fast enough to keep the pace with the required computational power.Now, imagine you see a company with an open vacancy for the position of “Engineer” — just that. That’s great… But there are like 100 types of engineers nowadays. Is it a mechanical engineer? Aerospace? Software? “Engineer” is too generalist.One or two years ago, companies would just publish a job vacancy as “Data Scientist”. Well, it is starting to feel incomplete. And if you’re just starting in this field, becoming a general data scientist might be too overwhelming. After having a grasp on this field, you better focus on a particular topic. Take for instance Netflix, which has nine Data roles:There are a lot of specializations areas that didn’t exist before and itis becoming more important for data scientists focus on one to make a stand. It’s time to find your own if you haven’t already. From my point of view, Data Engineering skills are the most interesting ones for next years. If you don’t have them in your team, you’re probably just playing data science in Jupyter notebooks. And companies are realizing that.2019 is going to be an amazing year, again. There’s a lot to be done, and it is not just techy and nerdy! Real problems to be solved are awaiting.As a concluding remark, remember that time is our biggest asset. Every second you spend doing worthless is a second you just lost not doing something great. Pick your topic, and do not consider your work business as usual.Hugo LopesWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2089,Linear Regression,243,2019-02-11,https://towardsdatascience.com/linear-regression-91eeae7d6a2e?source=collection_archive---------12-----------------------,2019,"In this tutorial, we will walk through a few of the classifications metrics in Python’s scikit-learn and write our own functions from scratch to understand the math behind a few of them.One major area of predictive modeling in data science is classification. Classification consists of trying to predict which class a particular sample from a population comes from. For example, if we are trying to predict if a particular patient will be re-hospitalized, the two possible classes are hospital (positive) and not-hospitalized (negative). The classification model then tries to predict if each patient will be hospitalized or not hospitalized. In other words, classification is simply trying to predict which bucket (predicted positive vs predicted negative) a particular sample from the population should be placed as seen below.As you train your classification predictive model, you will want to assess how good it is. Interestingly, there are many different ways of evaluating the performance. Most data scientists that use Python for predictive modeling use the Python package called scikit-learn. Scikit-learn contains many built-in functions for analyzing the performance of models. In this tutorial, we will walk through a few of these metrics and write our own functions from scratch to understand the math behind a few of them. If you would prefer to just read about performance metrics, please see my previous post at here.This tutorial will cover the following metrics from sklearn.metrics :For a sample dataset and jupyter notebook, please visit my github here. We will write our own functions from scratch assuming a two-class classification. Note that you will need to fill in the parts tagged as # your code hereLet’s load a sample data set that has the actual labels (actual_label) and the prediction probabilities for two models (model_RF and model_LR). Here the probabilities are the probability of being class 1.In most data science projects, you will define a threshold to define which prediction probabilities are labeled as predicted positive vs predicted negative. For now let’s assume the threshold is 0.5. Let’s add two additional columns that convert the probabilities to predicted labels.Given an actual label and a predicted label, the first thing we can do is divide our samples in 4 buckets:These buckets can be represented with the following image (original source https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg) and we will reference this image in many of the calculations below.These buckets can also be displayed using a confusion matrix as shown below:We can obtain the confusion matrix (as a 2x2 array) from scikit-learn, which takes as inputs the actual labels and the predicted labelswhere there were 5047 true positives, 2360 false positives, 2832 false negatives and 5519 true negatives. Let’s define our own functions to verify confusion_matrix. Note that I filled in the first one and you need to fill in the other 3.You can check your results match withLet’s write a function that will calculate all four of these for us, and another function to duplicate confusion_matrixCheck your results match withInstead of manually comparing, let’s verify that our functions worked using Python’s built in assert and numpy’s array_equal functionsGiven these four buckets (TP, FP, FN, TN), we can calculate many other performance metrics.The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below:We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labelsYour answer should be 0.6705165630156111Define your own function that duplicates accuracy_score, using the formula above.Using accuracy as a performance metric, the RF model is more accurate (0.67) than the LR model (0.62). So should we stop here and say RF model is the best model? No! Accuracy is not always the best metric to use to assess classification models. For example, let’s say that we are trying to predict something that only happens 1 out of 100 times. We could build a model that gets 99% accuracy by saying the event never happened. However, we catch 0% of the events we care about. The 0% measure here is another performance metric known as recall.Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labelsDefine your own function that duplicates recall_score, using the formula above.One method to boost the recall is to increase the number of samples that you define as predicted positive by lowering the threshold for predicted positive. Unfortunately, this will also increase the number of false positives. Another performance metric called precision takes this into account.Precision is the fraction of predicted positives events that are actually positive as shown below:We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labelsDefine your own function that duplicates precision_score, using the formula above.In this case, it looks like RF model is better at both recall and precision. But what would you do if one model was better at recall and the other was better at precision. One method that some data scientists use is called the F1 score.The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:We can obtain the f1 score from scikit-learn, which takes as inputs the actual labels and the predicted labelsDefine your own function that duplicates f1_score, using the formula above.So far, we have assumed that we defined a threshold of 0.5 for selecting which samples are predicted as positive. If we change this threshold the performance metrics will change. As shown below:How do we assess a model if we haven’t picked a threshold? One very common method is using the receiver operating characteristic (ROC) curve.ROC curves are VERY help with understanding the balance between true-positive rate and false positive rates. Sci-kit learn has built in functions for ROC curves and for analyzing them. The inputs to these functions (roc_curve and roc_auc_score) are the actual labels and the predicted probabilities (not the predicted labels). Both roc_curve and roc_auc_score are both complicated functions, so we will not have you write these functions from scratch. Instead, we will show you how to use sci-kit learn's functions and explain the key points. Let's begin by using roc_curve to make the ROC plot.The roc_curve function returns three lists:We can plot the ROC curve for each model as shown below.There are a couple things that we can observer from this figure:To analyze the performance, we will use the area-under-curve metric.As you can see, the area under the curve for the RF model (AUC = 0.738) is better than the LR (AUC = 0.666). When I plot the ROC curve, I like to add the AUC to the legend as shown below.Overall, in this toy example the model RF wins with every performance metric.In predictive analytics, when deciding between two models it is important to pick a single performance metric. As you can see here, there are many that you can choose from (accuracy, recall, precision, f1-score, AUC, etc). Ultimately, you should use the performance metric that is most suitable for the business problem at hand. Many data scientists prefer to use the AUC to analyze each model’s performance because it does not require selecting a threshold and helps balance true positive rate and false positive rate.Please leave a comment if you have any suggestions how to improve this tutorial.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
910,The cold start problem: how to break into machine learning,4000,2018-08-17,https://towardsdatascience.com/the-cold-start-problem-how-to-break-into-machine-learning-732ee9fedf1d?source=collection_archive---------2-----------------------,2018,"Strict and clear rules… appear to us as something in the background — hidden in the medium of the understanding. - Ludwig WittgensteinDecision trees are a popular technique for classification. They’re intuitive, easy to interpret, and often perform well out-of-the-box.Tree models are paths of rules that humans can understand. In certain contexts, being able to provide an explanation for your predictions — the plaintiff’s loan application was rejected because they were facing bankruptcy proceedings, rather than, something-something-something dot-product — can be an advantage.But there exists another class of rule-generating algorithms that you might not be familiar with and which may be worth adding to your toolset, known as ruleset learners.In a recent project classifying audio by classical music composer, my ruleset implementation beat sklearn’s DecisionTreeClassifier and matched grid-search-optimized SVD and random forest.Let’s take a look at when you might want to consider ruleset models, how they work, and how you can use them in your own code — including with a new Python package I recently developed that you can use for classification.Despite their many advantages, decision trees are infamous for their tendency to overfit, fragility, and difficulty handling imbalanced datasets.Trees learn by beginning with the full training set and greedily adding whichever condition maximizes each child nodes’ class purity. As we descend down the tree, each node adds a condition, subsetting our training data into smaller and smaller subgroups:In just a few decision steps, we’ve gone from a root with thousands of examples to leaves with sample sizes as low as 29, 11, 2 — even 1.How? Think about how binary tree search scales nicely because tree depth only grows logarithmically with the number of nodes. The flip-side to this for training decision trees is that the number of nodes grows at an exponential rate with depth. So our training subsets shrink pretty quickly and eventually lack statistically valid sample sizes.For similar reasons, trees are “fragile” in the sense that small changes in the training set alter top-level rules, producing ripple-effects throughout the entire model.There are several popular techniques for dealing with tree overfitting and fragility. We can prune the tree, either by stopping growth early according to some threshold, or by reducing its size once complete. Or we could build a random forest ensemble of thousands of trees, at a bit of expense to training speed and explainability.Rulesets are similar to decision trees, but because they aren’t hierarchical, with ordered, sub-branching decisions, they have the potential to sidestep some of these downsides.Ruleset learners also tend to produce more compact models.A ruleset is simply disjunctions (or’s) of conjunctions (and’s).For example, a model describing whether your child can persuade you to go out for ice-cream cake might look like this:If:  I’m good and we have enough time to buy ice cream; or your resilience is worn thin and you don’t feel like dealing with me; or you’re feeling magnanimous, Then: Ice cream cake.We can express this ruleset symbolically:(I’m good ^ we have enough time) V (worn-out resilience ^ you just can’t even) V (magnanimous)And Pythonically:[[(behavior=good),(time=True)], [(resilience=thin),(you=can’t even)], [(feeling=magnanimous)]]Rather than breaking the training data down into subsets of subsets with recursion, rulesets grow iteratively by training on all the yet-to-be-examined training data.To get the basic flavor of how training works, let’s first take a look at a straightforward ruleset-learning algorithm known as IREP.After splitting our data into a training set and test set, we train a model on our training set with the following steps:1. Split your training set into a “growset” and a “pruneset”2. Grow a rule (information gain)3. Prune the rule (reduced error metric)4. Remove the examples the new rule covers from your training set, and repeat steps 1, 2 and 3 until you begin to make things worse (precision metric).In step 1, we take our training data and randomly split it 2/3–1/3. We’ll use the first portion for growing a rule, and the second portion for pruning it. A bit like how cross-validation requires us to set aside evaluation folds, the split ensures that we’re not pruning with the same data we just used for growing!Next, we grow a rule by greedily adding conditionals that maximize FOIL information gain (formula below). As our rule acquires more and more conditionals, it becomes increasingly stringent and rules out more and more negative-class examples. (Remember that a rule is a set of “and’s” — more “and’s” means more stringency.) We stop when the rule covers no more negative examples.Now it’s time to prune the rule we just grew. We try pruning each of its conditionals greedily in reverse order, choosing the rule that maximizes some pruning metric, such as this one:We’ve just grown and pruned our first rule! Now we iterate. From our training set, remove the examples covered by the new rule. Continue growing new rules — making our ruleset increasingly lenient — until we grow one whose precision is less than 50%.Here is a visualization of the entire process:My new ruleset package, wittgenstein, implements IREP, as well as another ruleset algorithm called RIPPER. Besides sounding like a heavy metal band, RIPPER remains the state-of-art for this technique.The algorithm is quite a bit more complicated than IREP, but here are the major differences:And is there not also the case where we play and — make up the rules as we go along?- Ludwig WittgensteinJava users who want to use a ruleset learner can use Weka’s RIPPER implementation, JRip. There are also Weka wrappers for Python and R.Python users can also try wittgenstein. (There might be other Python packages for these particular algorithms out there, but I couldn’t find any.) The github repo is here.To install from the command-line:Here’s a quick usage example, using the delightful poisonous mushroom dataset. Our goal is to produce a set of rules that can discern which mushrooms are poisonous.Let’s start by loading our dataframe into pandas:And train-test-splitting our data:Wittgenstein uses fit-predict-score syntax that’s similar to scikit-learn’s. We’ll train a RIPPER classifier, with the positive class defined as poisonous.During initialization/fitting, we can pass a few optional parameters:You can test a model using the default metric (accuracy), or by passing in your own scoring metric. Let’s import precision and recall from scikit-learn. We’ll also examine model complexity by counting the number of conditions.We can access our trained model using the clf.ruleset_ attribute. A trained ruleset model represents a list of “and’s” of “or’s”:To generate new predictions, use the predict method:We can also ask our model to tell us why it made each positive prediction that it did:Pretty cool!I ran my IREP and RIPPER implementations on repeated tests across an initial 11 categorical datasets (mostly from UCI), using scikit-learn’s DecisionTreeClassifier as well as grid-search-optimized RandomForestClassifier as baselines. (I threw out the results from two datasets for which sklearn and wittgenstein, respectively, refused to make positive predictions.)Comparing rulesets against an ensemble technique like random forest is a bit of an unfair comparison — and grid search-tuning Forest makes it even more unfair — but I wanted to see how well wittgenstein could compete with the best comparable alternative.Even though it’s a decision tree classifier, scikit-learn’s tree implementation doesn’t actually take categorical data. But that’s okay — we’ll just need to do a little preprocessing to get our data into a format that DecisionTreeClassifier will accept.First, let’s use scikit’s LabelEncoder to transform our categorical features into numerical ones:Then, we use one hot encoding to create dummy variables. Otherwise we’d be stuck with ordinal features instead of nominal ones!Preprocessing complete, we’re now ready to split our data……and train our model:To score our lovely tree:Here is the code for tuning and fitting a random forest:My package was competitive with sklearn, at least on these datasets. (A detailed Jupyter notebook with the tests can be found here.)Here is a comparison for how frequently each ruleset model beat each sklearn model, scored by precision:Here is a recall comparison:I also compared their compactness, as measured by the total number of conditions or nodes.Some areas wittgenstein did well:Potential drawbacks to consider:As with any machine learning model, your specific data and the specific problem you face determines the best tool for the job.Ruleset learners are a machine-learning approach that have been interesting to implement, and which can in some cases be useful to have in your toolset.I’d love to hear your thoughts, so please feel free to reach out to me on LinkedIn or Github![1] J. Furnkrantz and G. Widmer, Incremental Reduced Error Pruning (1994), Machine Learning 1994 Proceedings of the Eleventh Annual Conference[2] J. Ross Quinlan, MDL and Categorical Theories (continued) (1995) Machine Learning 1995 Proceedings of the Twelfth International Conference[3] W. Cohen, Fast Effective Rule Induction (1995) Machine Learning 1995 Proceedings of the Twelfth International Conference[4] E. Frank and I. H. Witten, Generating Accurate Rule Sets Without Global Optimization (1998) Machine Learning 1998 Proceedings of the Twelfth International Conference[5] T. Wang et. al, A Bayesian Framework for Learning Rule Sets for Interpretable Classification (2017) Journal of Machine Learning Research[6] Ludwig Wittgenstein, Philosophical Investigations (1958)Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
308,Game of Thrones — Season 7.5 — Episode 3,3,2018-05-01,https://towardsdatascience.com/game-of-thrones-season-7-5-episode-3-fcf1f35af7c0?source=collection_archive---------14-----------------------,2018,"The opioid epidemic has turned into one of the major public health catastrophes for this generation of Americans. Similar to what tobacco/smoking or HIV/AIDS were to earlier generations, the opioid epidemic appears to be this era’s defining public health crisis. I wanted to see if it was possible to build a model to predict opioid-related mortality on a county by county basis, since this type of model might give insights into where and how to target interventions.I had a suspicion that a decent amount of the variance in which areas of the country have been most affected by the crisis could probably be explained by demographic or economic factors, but I was actually more curious about whether or not other predictors which were more modifiable would turn out to be significant. For example, the common narrative is that the opioid crisis started when physicians started prescribing opioid painkillers much more liberally in the 1990s, in part due to pharmaceutical companies reassuring physicians that these painkillers had a low incidence of addiction and few side effects, neither of which was true. But is opioid prescribing rate still the main driver of opioid overdose deaths? Or are there are other variables that are stronger predictors by now, e.g. perhaps the volume of illegal opioids such heroin or fentanyl is stronger predictor?The goal of this project was to predict a quantitative target variable using a multiple linear regression model. The design of the project was fairly straightforward: identify an appropriate target variable, then find publicly available datasets that would provide information on what I thought would be useful and appropriate predictors. I then planned to use these independent variables/predictors to build a multiple linear regression model, and then perform feature selection in order to see which predictors were the most important. Finally, I tested the model on a holdout test data set to see how well the model performed.After exploring the CDC’s description of their public databases, I decided to use the CDC WONDER database to scrape my target variable, which was the crude mortality rate due to drug overdose per county in 2016. A few comments on the decision to use this as a target variable: I initially thought about predicting mortality rate per state, but that would only give me 50 records. The CDC MCD (multiple causes of death) database actually does allow you to query for deaths specifically due to opioid overdose (and you can be even more specific, e.g. querying for the # of deaths due to heroin vs opioid analgesics vs other types of opioids), but the numbers became so small for individual categories that it was easier to get actual numbers for overall mortality rate due to drug overdose, without specifying what type of drug. This is because when the number of total deaths per county is between 10–20, the CDC will give you the # of deaths but won’t calculate a crude or age-adjusted mortality rate, because they state that this small number results in ‘unreliable’ statistics. If the number of total deaths is between 0–9, then they won’t give a number at all and will instead state that the result is ‘suppressed’, because with a small number of deaths in a small county, individual patients could potentially be identified.Therefore, it was easiest to get data for crude mortality rate per county due to drug overdose, and the most recent dataset available was 2016. Statistically speaking, it would have been more accurate to compare age-adjusted mortality rates, however the CDC did not provide age-adjusted mortality rates for any counties where it gave a ‘Unreliable’ result (total deaths between 10–20), and without the specific breakdown of how many of these deaths were in each age bracket, I couldn’t calculate the age-adjusted mortality rate directly. It does look like there are ways to indirectly calculate the age-adjusted mortality rate, but I decided to go ahead and use the crude mortality rate and then to use median age per county as a predictor as rough way to sort of account for this. This meant that even though there are about 3000 counties in the US, because I ended up with 1000 records (exactly!) where it was possible to actually calculate a crude mortality rate for those counties.For my independent variables, broadly speaking they fell into 4 categories:DemographicEconomicMedicalGeographicThe demographic and economic data were all obtained using the US Census API to query the ACS (American Communities Survey) 5-year 2016 dataset. It’s definitely a bit of a learning curve to figure out which US Census dataset to query, and then after that how to figure out how to structure a query. The US Census offers so much information that it looks like it’s possible to get extremely granular query for the exact, specific records that you want, but the downside is that figuring out how to pull even simple data out is pretty difficult at first! Anyways, what I realized after looking at the pairplot of my predictors is that the economic data that I got has relatively high correlation with each other, e.g. % poverty and % unemployment, although they aren’t measuring exactly the same thing, they clearly do correlate with each other. I thought that this might be a big problem in causing collinearity, but in retrospect I think that it was actually okay to have all these variables — when I get to the results section, I’ll discuss that I think that engineering a variable that combines economic information might be even more useful for building a better model.The opioid prescribing rate per 100 people was pulled from the CDC, which provides tables with the annual opioid prescribing rate per 100 people per county for more than the past decade. Some of the numbers were pretty eye-popping; in Norton County, VA, there were 563 opioid prescriptions per 100 people in 2014. That’s right, more than 5 opioid prescriptions per person for that county! Numbers were not provided for many counties, so I ended up imputing the average rate for the state to represent these numbers. I loaded the prescription rate per county for 2014, 2015, and 2016, and the prescription rates for 2014 had the strongest raw correlation with mortality rate when looking at the correlation matrix, so I ended up using the opioid prescription rate for 2014 instead of the other years as my predictor. There wasn’t value in including all 3 years since the prescribing rate was very similar year to year.A prescription drug monitoring program is a state-level program (except for Missouri!) which collects and monitors information on prescriptions of certain medications, specifically controlled substances such as opioids and benzodiazepines. Different states have implemented PDMPs at different years, and in the 2000s many states started implementing electronic PDMPs so that doctors, pharmacists, and other agencies can access this data. From the prescribing provider and pharmacist point of view, one of the main goals of the PDMP is to prevent patients from ‘doctor-shopping’ and going to multiple providers to collect lots of prescriptions for opioids or benzos. If PDMPs really had a strong effect on decreasing opioid over-prescription, then hopefully that could translate into a lower opioid overdose mortality rate, and so one could hypothesize that overall the overdose mortality rate would be lower in states with older PDMPs.Finally, I thought that geographic region could also be a useful predictor of the crude mortality rate, since some parts of the country have been hit harder than others. One could argue about how granular to get, e.g. time zones, regions, divisions, even state by state (although for states the problem is that states are very different sizes). I ended up using US Census regional divisions as it seemed like a good compromise between macro/micro, as there are 10 regions. This ended up getting coded as dummy variables, and I removed the ‘New England’ region dummy variable in order to avoid the dummy trap.The model that we were assigned to use was a multiple linear regression model. This was straightforward to implement using either sklearn or statsmodels, although statsmodels provides a lot more information on the model. With all of the predictors included, I think R-squared was around 0.3. Damien then helped me transform the predictors using sklearn’s PolynomialFeatures in order to generate quadratic combinations, which then increased the number of predictors from 16 (8 quantitative predictors, 8 dummy variables) to 53, including all of the interaction terms for the quantitative predictors. This increased R-squared on the full dataset to around 0.43-ish, which clearly shows that the interaction terms do contribute information.The next step was to perform feature selection. I ended up trying two methods, backwards elimination with cross-validation, where for each round, each cross-fold voted for a variable that had a coefficient with the highest p-value, and then I removed the variable that had the most votes. The final model I selected was the one that had the lowest mean validation error. The other strategy I tried was using sklearn’s LassoCV to perform feature selection; again, I selected the Lasso model that had the lowest validation error.Interestingly, LassoCV and backwards elimination both picked ‘age’ as the most important predictor, but then after that the next four most important predictors were different. Furthermore, backwards elimination selected several of the dummy variables as fairly important, whereas lasso still included them in the final model, but they were less important. They both gave similar R2 values, about 0.41 to 0.42, and also about the same validation MSE, 0.17 to 0.18. The test MSE on the final holdout test data set was also about the same for both models; for the backwards elimination model it was 0.181, and for Lasso it was 0.184.For obtaining the predictors and target variable data, I used Selenium and BeautifulSoup for the variables where an API was not provided, and then the US Census API for their data. Cleaning the data was done with pandas, and for analysis I used sklearn and also statsmodels.I looked at the top 5 positive coefficients with the largest values for the two models. They were quite similar, as you can see below. Backwards elimination included the combination coefficient ‘median household income x opioid rx rate’, whereas lasso included ‘% poverty’, but otherwise they shared 4 coefficients in common. Unsurprisingly, opioid rx rate was one of the top five positive predictors, although since I didn’t include any data regarding illegal drug use e.g. heroin, fentanyl, etc. it’s unclear to me whether or not illegal drug use or prescription drugs are a stronger predictor of overdose mortality. I did think that the combination predictors were interesting; median household income multiplied by the percent unemployed was a strong positive predictor, stronger than either feature individually, which is a little odd because you would think that if median household income went up, that percent unemployment would go down and they would cancel each other out. But that’s probably not a true assumption, since if there are counties with a high degree of income inequality, you could have a rise in median household income (especially in the households that are earning income) where the rich become richer, but the percent unemployment could also go up since the people on the opposite end of the socioeconomic spectrum could also be losing their jobs at the same time.I also ended up refitting the backwards elimination on the full data set, and then mapping the residuals to a choropleth map. I wanted to see if there was a geographic pattern to where the model overestimated or underestimated the crude mortality rate. The choropleth maps above were created using GeoPandas, which is a module that is build to handle spatial and geographic data, and comes with plotting methods to allow for mapping. However, there is another library called Folium which allows you to create interactive maps and save them as html pages, and so I used Folium to create a residual map. Unfortunately, I can’t embed the interactive map into a Medium post, but the link is here if you’re interested. Pink means that the model overestimated the overdose mortality rate for that county, whereas green means that the model underestimated the mortality rate; the darker the color, the larger the residual error.After looking at the residual dataframe and sorting by the size of the residual, I could see that the county with the largest positive residual (meaning the model had the worst underestimate of the true crude mortality rate) was Baltimore city in Maryland. The county with the largest negative residual (meaning the model had the worst overestimate) was Saratoga County, New York.As you can see, the model underestimated the true overdose mortality rate in Baltimore by 64 people/100K! In Saratoga County, it overestimated by a smaller magnitude, but I think it’s interesting to look at the values of each feature for these two outliers, since I think it provides some insight into why the model did not work as well for these counties. For one, I did not realize that the US Census considers Baltimore city as its own county, but it apparently does, and Baltimore has a very low % Caucasian population compared to the rest of the country, and remember that % Caucasian was either the 2nd or 3rd most important predictor in the backwards elimination and lasso linear regression models. It also had a poverty rate of 18.3% in 2016, whereas the national poverty rate was 12.7%. Finally, the median age of Baltimore is younger than the national median age of 38, and again since age was the most important predictor in both models, this also may have contributed to the models underestimating the overdose mortality rate.For Saratoga county, the percentage of high school graduates or higher is clearly quite high at 94%. Similarly, the % unemployed and the % poverty rate were both national average; for poverty rate, you can see it is quite a bit lower than the national poverty rate of 12.7%.I think that looking at these outliers tells me that my model is missing some sort of overall economic wellbeing/prosperity or overall quality of life measure which probably isn’t being sufficiently captured in the interaction terms in the quadratic polynomial regression. Baltimore is infamous for its high crime rate, and I think it’s certainly possible that illegal opioids may be a driver of overdose mortality that the model can’t use because I didn’t include any data sources for that. On the other end of the spectrum, after looking up some information about Saratoga county, it is apparently at the heart of upstate New York’s ‘Tech Valley’, and there are a large number of international high-tech and electronic companies that have campuses there, which would also explain its high education rate and low poverty and unemployment rate. It’s possible that creating interaction terms that involve more economic variables (e.g. education rate x poverty rate x unemployment rate) would have been able to lend even more predictive power to the model.The other thing I noticed from looking at the residual map is that it looks like the model tends to underestimate the mortality rate urban counties (or at least counties including major cities), whereas it overestimates the mortality rate in non-urban counties. I did think about including the US Census’s 2013 Urban/Rural classification into my model, but the data provided by the Census is not a one-to-one mapping of which counties are ‘urban’ and which counties are ‘rural’, and by the time I thought of it, it was close to the deadline for this project and it would have taken too long to figure out how to code the information.My overall conclusions from this analysis is that it is possible to explain some of the variance in overdose mortality rate using the predictors that I picked, but certainly the models I produced do not explain all of the variance. The learning curves for the models indicate that they are in a high bias regime, and that I am still underfitting the data and need more features. Again, I think that finding data sources that would provide information on the flow of illegal opioids in different counties would be useful, and I think feature engineering to come up with a more holistic variable that would capture overall quality of life or economic prosperity would be useful. Also, there does seem to be a difference between how the model performs in urban vs rural counties, so adding that as a feature would probably also increase the predictive power of the model.The majority of my predictors were economic or demographic data, but if I were going to expand the model, I think what would be really interesting would be to use different public health interventions as predictor variables for the next year’s overdose mortality rate, to see whether or not the presence or absence (or age) of these interventions was a strong negative predictor for the mortality rate. Lots of states and counties are trying different interventions, e.g. issuing standing naloxone orders, using ‘Vivitrol courts’, opening methadone clinics, etc. I tried to do this by using the age of the PDMP in my model, but it was not a strong predictor. I think that one issue with this was that the PDMP website I got my information from gives the year that any PDMP was started, but electronic PDMPs that providers and pharmacists could query in real time didn’t really start until the 2000s, and so PDMPs likely weren’t able to influence the actual # of opioids released to the public until at least then. Therefore, it would probably be necessary to do more research to find a better feature for PDMPs other than just the age of the program, e.g. finding out the year that an electronic PDMP was first implemented, or even getting some measure that would indicate how much each PDMP was being used.Finally, the other thing that would be interesting to look at is how the opioid crisis has evolved over time. I think it’s entirely possible that different features have become more or less important over time, with different drivers taking over. To do this analysis, it would be necessary to create different models for each year, and to figure out what the appropriate time lag would be for some predictors; for example, it’s possible that the presence of a standing naloxone order could have a strong negative correlation on the mortality rate in a county in 1 or 2 years, but not necessarily the year that the order was issued. There could certainly be a lot more work done on this problem!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1805,A shiny Web App from LEGO— truck + trailer,28,2019-01-21,https://towardsdatascience.com/a-shiny-web-app-from-lego-truck-trailer-c977015bc6a9?source=collection_archive---------21-----------------------,2019,"In today’s multifaceted energy world, a growing number of prosumer assets are increasing the complexity of power grids. This is even more important in an ever-changing climate that more and more generates huge storms such as the Typhoon Lekima which caused 9.3 Billion in damage (5th Costliest known Pacific typhoons) and more than 90 deaths in the Philippines, Taiwan and China earlier this year, or the recent monstrous Category 5 Hurricane Dorian in the Atlantic Ocean. The director-general of the Bahamas Ministry of Tourism and Aviation, Joy Jibrilu, details the damage left in the aftermath from Hurricane Dorian and what the Bahamas will need to move forward especially on the infrastructures. (source MSBC: https://www.youtube.com/watch?v=c_8sLpTQq_E)This looks too similar to what we’ve seen in Porto Rico two years ago which suffered severe damage from the category 5 hurricane Maria. Damages cumulated to ~92 billion USD, the third most costly tropical cyclone in US history. The blackout as a result of Maria has been identified as the largest in US history and the second-largest in world history.CosmiQ Works developed an interesting data-fusion mapping approach and the first independent remote sensing assessment of the recovery of electricity and infrastructure in Puerto Rico.Analyzing the declined levels of observed brightness across the island, this IQT Lab was able to identify that 13.9% of persons still lacked power and that 13.2% of infrastructure has been lost as of May 31, 2018.Decentralized systems with solar generation, wind turbines, and electric vehicles provide promise for a decarbonized future, but also bring along challenges for both utilities and prosumers, but they need to be properly planned and operate to survive, or at least to be restarted in a timely manner after such a catastrophic disaster (towardsdatascience.com/no-fast-enough-energy-transition-without-intelligent-energy-storage).The transformation of energy grids, the emergence of new services, new players: prosummers, consum’actors and new models such as self-consumption, alters the operating requirements and constraints of the grids themselves and imposes the management of increasingly massive data that would be unworkable without recourse to AI.The energy market is moving away from a model with centralized power plants only and entering the era of distributed grids and peer-to-peer markets. Multiple elements of the energy ecosystem are evolving at a dizzying speed. We are seeing a very complex market emerging, where the distribution company needs to allow more and more renewables and flexible energy assets to be installed behind the meter while maintaining a stable local grid. At the same time, prosumers who have installed such flexible assets want to optimize their energy flow to maximize the value of their investment.A steadily growing challenge is the emergence and the accelerated growth of a decentralized generation, where private users, bigger or smaller, generate and use their own electricity from renewable sources, such as wind and solar power. This complicates supply & demand oblige utilities to buy surplus energy from private users, who produce more electricity than they consume and send it back to the grid. Since 2010, the use of solar energy has substantially increased and this exponential trend is expected to continue with photovoltaic cells, devices generating electricity from sunlight, reducing costs and increasing efficiency.The current systems have generally not been designed to take into account this diversification of energy sources, particularly the increase in renewable resources. For example, in many American jurisdictions, when demand outstrips supply, utilities activate fossil fuel-based power plants, known as “state of the art” power plants, just a couple minutes in advance to avoid a cascading disaster. This procedure is the most expensive and, but also, the most profitable part for these companies. It results in a higher electricity bill for consumers and an increase in greenhouse gas emissions into the atmosphere. These problems will be exacerbated as energy demand is expected to increase substantially in the coming years. To avoid these non-optimal (for the least) operating mode with IES, AI can enable automatic learning algorithms, combined with data on these complex networks and real-time meteorological data (from satellites, ground observations and climate models), to be exploited with the full potential to predict the electricity generated by RES, such as wind, sun and oceans.Combined with other technologies such as Big Data, the Cloud and the Internet of Things (IoT), energy storage with AI can play an important role in power grid management by improving the accessibility of power sources. renewable energies.AI can greatly help to manage electricity consumption so that big utilities or an even smaller grid with DER can sell when it’s expensive and buy when it’s cheap. Machine learning, and especially deep learning, algorithms can be applied in the energy sector in a very interesting way in this context. As the end-users are becoming “prosumers”, smart devices are proliferating, big data is available for analysis, renewable energy sources are growing, and business models and regulations are adapting.Combining it all together can help get to the point where energy flows and/or is stored at the optimal timing, direction and volume. With artificial intelligence algorithms to determine when to produce, consume, store and trade energy, to the cost-benefit of the end-user, the service provider and the grid operator. With thousands of emerging energy communities, this vision might become clearer and perhaps even the main reality in the coming 5 to 10 years.More and more sustainable communities, utilities and operators are currently under simulation or in the first phases of pilot projects. With Internet of Things (IoT) demanding more than 10 billion smart devices with over 100 million electric vehicles (buses, trucks and passenger cars), with more than 1 billion prosumers (private and industrial) having their own “production” of kWh (solar or else), all predicted by the year 2025 — it’ll be a huge challenge to maintaining reliability and secured supply and grid stability.Expectations of how DERs will evolve in the coming years are multiple. But these changes require a completely new operating paradigm, and there is no better test for technology than real life. New models involving Artificial Intelligence, Energy Storage and Renewable are already being applied at various levels in many states on all continents, not to mention Australia, California, Germany, China, Costa Rica, Israel and many other countries around the world.This is especially true when we’re dealing with a climate that is reacting to our human intervention. AI properly used with Renewable and energy storage can help us not only on reducing the impact of our energy consumption on the CO2 emissions, but also to adapt to the growing impacts of disasters (related to the man-made climate change).It’s not required to be psychic to envisage that AI and DER will be the Transformational technologies that will soon be our best friends in making the new grid model.This article is an extension of a series on Artificial Intelligence and Energy Storage by Stephane Bilodeau, ing., P.Eng, PhD, FEC. Founder & Chief Technology Officer, Smart Phases (Novacab), Fellow of Engineers Canada and expert contributor to Energy Central and Medium.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
3437,Understanding Adam : how loss functions are minimized ?,50,2019-08-09,https://towardsdatascience.com/understanding-adam-how-loss-functions-are-minimized-3a75d36ebdfc?source=collection_archive---------18-----------------------,2019,"Pandas is a very powerful and versatile Python data analysis library that expedites the preprocessing steps of data science projects. It provides numerous functions and methods that are quite useful in data analysis.In this post, I aim to cover some of the very handy operations that I use quite often. The topics that will be covered in this post are:Sample method allows you to select values randomly from a Series or DataFrame. It is useful when we want to select a random sample from a distribution. Consider we have a random variable whose values are stored in Series or columns of a DataFrame. We can select a part of it using loc or iloc methods but we need to specify the indices or a range for selection. However, using sample method, we can randomly select values. Before starting on examples, we import numpy and pandas:Let’s create a dataframe with 3 columns and 10000 rows:We can select n number of values from any column:sample() returns both the values and the indices. We specify the number of values with n parameter but we can also pass a ratio to frac parameter. For instance, 0.0005 will return 5 of 10000 values in a row:By default, sampling is done without replacement. Thus, each value can only be selected once. We can change this way of selection by setting replace parameter as True. Then values can be selected more than one time. Please note that this does not mean the sample will definitely include a value more than once. It may or may not select the same value.By default, each value has the same probability to be selected. In some cases, we may want to select randomly from a specified part of a series or dataframe. For instance, we may want to skip the first 9000 rows and want to randomly select from the remaining 1000 rows. To accomplish this, we can use weights parameter.We assign weights to each data point that indicates the probability to be selected. The weights must add up to 1.We set the weights of first 9000 to zero so the resulting sample only includes values after index 9000.To obtain reproducible samples, we can use random_state parameter. If an integer value is passed to random_state, the same sample will be produced every time the code is run.We can also select a column randomly by setting axis parameter as 1.“Where” is used to replace values in rows or columns based on a condition. The default replacement values is NaN but we can also specify the value to be put as a replacement. Let’s go over an example so that it becomes clear.Let’s create a sample dataframe first:Consider a case in which we need to replace negative values with 0. Instead of going through if statements and for loops, we can easily achieve this using where:The way “where” works is that values that fit the condition are selected and the remaining values are replaced with the specified value. where(df2>0, 0) select all the values that are greater than 0 and the remaining values are replaced with 0. Thus, where can also be considered as a mask operation.Let’s go over another example. We want to keep the negative values as is, but convert the positive values to negative (i.e. 2 is replaced with -2):To save the changes in the original dataframe, we need to set inplace parameter as True.One important point is that “where” for Pandas and NumPy are not exactly the same. We can achieve the same result but with slightly different syntax. With DataFrame.where, the values that fit the condition are selected as is and the other values are replaced with the specified value. Np.where requires to also specify the value for the ones that fit the condition. The following two lines return the same result:Thus, numpy version is more flexible. We can also change the values that fits the condition. For instance, we may want to multiply the negative values with 2:We use filtering or selecting methods a lot when working with dataframes. We can filter based on a condition like we did with where or select a part of a dataframe using row or column indices. Isin method is kind of an advanced filtering. For example, we can select the values that fit a list of conditions.Let’s create a different dataframe to use for “isin” examples:We can set one condition using a simple mask operation:isin allows to set multiple conditions in a list:We can also use tilde (~) operator with isin. Then the values that do not fit the condition will be returned:We can also pass a filter in dictionary form to isin. Boolean values will be returned based on the conditions:I think the success and prevalence of Pandas come from the versatile, powerful and easy-to-use functions to manipulate and analyze data. There are almost always multiple ways to do a task with Pandas. Since a big portion of time spent on a data science project is spent during data cleaning and preprocessing steps, Pandas is a valuable asset to have in your arsenal.Thank you for reading. Please let me know if you have any feedback.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
359,Extended Kalman Filter: Why do we need an Extended Version?,1000,2018-04-07,https://towardsdatascience.com/extended-kalman-filter-43e52b16757d?source=collection_archive---------1-----------------------,2018,"There are a lot of applications that require text classification or we can say intent classification. Nowadays, everything is required to be categorized like contents, products are often tagged by category.However, the vast majority of text classification articles and tutorials on the internet are binary text classification such as email spam filtering (spam vs. ham), sentiment analysis (positive vs. negative). Our real-world problem is much more complicated than that. Therefore, this is what I am going to explain in this blog. Classifying the text into multiple categories.Problem Statement:I have developed this classifier for my GSoC project(Owasp SKF-Chatbot). The problem statement is the Security Knowledge framework have a knowledge base for different vulnerabilities. It provides the description, solution and code example for different vulnerabilities. So, I need to classify the user’s query whether he is asking for a description, solution or code example.I used Python and Jupyter Notebook to develop our system, relying on Scikit-Learn for the machine learning components.Preparation of Data-sets:For any problem related to classification or machine learning the first thing we required is the data that too correctly formatted. So, firstly I will explain how I prepare the data-set for intent classification.You can check data.json here. I am going to prepare the dataset in CSV format as it will be easy to train the model.My text classification is for question answer kind of system. So, I need to generate questions for that I extracted out all the titles in a list.So, the questions need to be classified on the basis of description, solution, and code. So, I made three lists one for each to store the questions.So, Now the data is prepared. You can check the complete data here.And for the better understanding of how to prepare data-set, you can also check this jupyter notebook example.Now, we have prepared data 😄...It’s time to do some magic with it. Let’s see how we classify the intents.Let’ s import some libraries:Firstly, we will extract the data from the CSV file and will store it in the data frame.In this step, we will prepare the data to feed it to the algorithm. Firstly, we get the complete data in ‘y’ and set the column for that data usingAfter, that ‘pd.notnull’ is being used for checking the data in the question column is null or not if it’ s null the entire row will be removed.This step is important for getting the high-quality clean data. Because, If we have good data we will have good results. 😃We will make a new column ‘category_id’ which will give a number to classes. Like for description it will be 0, solution 1 and code 2.After removing duplicates we will get something like thisYou can check the code snippet of data_prepare below.I have used the Multinomial Naive_Bayes algorithm for prediction because I find it easy to implement and it has high accuracy.The OneVsRest strategy can be used for multi-label learning, where a classifier is used to predict multiple labels for instance. Naive Bayes supports multi-class, but we are in a multi-label scenario, therefore, we wrap Naive Bayes in the OneVsRestClassifier.OneVsRest multi-label strategyThe Multi-label algorithm accepts a binary mask over multiple labels. The result for each prediction will be an array of 0s and 1s marking which class labels apply to each row input sample.For the better understanding of the following code snippet and Multinomial Naive_bayes try this.A short overview is: Here, I have divided my data into test data and train data and then feed that data into the model.I have also tried other algorithms or models like Linear SVC, Logistic Regression, and Random Forest. You can have a look at that here.This will give us the final prediction.At last, we enter the question and pass it to the predict function and wait for the magic. 😝Pretty, accurate isn’t it?You can check this notebook for the better understanding of intent classification.Thanks for reading! Please do clap if you liked it, comment(reviews or doubts) and Share it 😄You can connect with me on Github, Linkedin, Twitter 😄Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1170,Tableau-like Drag and Drop GUI Visualization in R,1200,2018-10-05,https://towardsdatascience.com/tableau-esque-drag-and-drop-gui-visualization-in-r-901ee9f2fe3f?source=collection_archive---------7-----------------------,2018,"If you are into Data Science or Software Engineering or any related field, you may have come across the term “JSON”, and if you’re a newbie, you might be confused. In this post, I will try to introduce JSON without assuming any prior knowledge and explain the concept of JSON with simple examples. Let’s get started.What is JSON?JSON stands for JavaScript Object Notation. Don’t get carried away by the jargon, it’s actually straightforward to understand and use. As the word “notation” might hint, JSON is simply a way of representing data independent of a platform — this just means that it is something like a PDF (which is the same across different platforms like mobile, desktop and web) for data. The JSON format was specified by Douglas Crockford, and the filename extension is, as you can guess, .json.Thus, the PDF for data travels across platforms and maintains consistency in representation and storage. It is widely used today and therefore, crucial in the fields of Data Science and Software Engineering.Why JSON?JSON sounds cool, but what is the motivation or purpose behind it?As highlighted above, JSON being independent of a platform is one of the major choices of format for transfer of data between computers that exchange tons of data each day.An alternative to JSON is XML (extensible markup language), but JSON is better in many ways. While both are human-readable and machine-readable, JSON is much easier to read and is also faster for computers to process. Furthermore, JSON is processed (or parsed) with a JavaScript parser (which is built-into most web-browsers) while XML requires a separate XML parser. This is where the “JavaScirpt” comes into play in “JSON.”You can read more about the differences here and here.How to JSON?After you’ve understood what JSON is and the rationale behind it, you can now jump into writing some JSON code. JSON syntax is very similar to JavaScript so it would be familiar if you have prior experience with JavaScript.Let us work through an example to understand writing JSON. Let’s say that you are the leader of your neighbourhood and maintain a database for all the people in it. Consider a scenario when Mr Jodhn Appleseed moves into your neighbourhood. You may want to store information like his first name, last name, date of birth, marital status, etc. Let’s use JSON for this!When you’re writing JSON, you’re essentially a match-maker! Yes, really! But instead of people, you match data. In JSON, data is stored as key-value pairs — every data item has a key through which you can modify, add or delete the data item.Let’s start by adding the first name and last names:As you can notice, the values in the left column are the keys (“first name”, “last name”) and the values in the right column are the respective values (“John”, “Appleseed”). A colon separates them. The values encompassed with double quotes are of type String — which essentially means that they are meant to be text and not refer to something else (e.g. a number, variable in another part of the file, etc.).Note that in JSON, all keys must be strings —so must be enclosed with the double quotes. Also, there’s a comma after each key-value pair except for the last one, indicating that a new item is being recorded.Now, let’s add his age:Note that there are no double quotes around the number 30. Intuitively, the data type of such data is number, and hence you can perform mathematical operations on them (when you retrieve the information). In JSON, this data type (number) can take on any numerical value — decimal or integer or any other type. Notice also how I added a comma after “Appleseed” as I added another item below it.Let’s do something fun now. Let’s try to add his house, which would have its address, owner information, and city. But how do we add these things into the JSON file for John? Things like owner info are attributes of the house and not John, so it doesn’t make sense to add this info directly into the JSON file for John. Worry not, JSON has an interesting data-type to handle this!In JSON, a value can also be an Object (something that is a key-value pair too). This object is just like another JSON file — enclosed with curly braces and containing key-value pairs, except that it is within our original JSON file instead of having a file of its own.As you may notice, the house is an Object, which contains the keys address, owner and market price. The data in address is also an Object, containing the keys house no, street and city. Thus, it is possible to nest objects within objects, and this allows for a more clear representation of data, as shown above.Now, let’s add info about his friends. We might do this by adding “friend1” and name, “friend2” and name, and so on but this would quickly become boring. JSON provides a data type for storing such info effectively, and it is called an array. It is an ordered collection of items — which can be of any data type.Let’s say he has three friends: Charles, Mark and Darren. Our JSON file would now look something like this:Notice that the array is enclosed with square braces and we wrote each item in a newline followed by a comma except for the last one. The new line is not necessary, but it helps the readability of the code for humans.Lastly, let’s add his marital status. We could do something like ""married"":""yes"" but JSON provides a special data type for all dichotomous choices: a boolean. It can only take on two values: true or false. Intuitively, it can’t be both at the same time. Assume that John is a bachelor. Let’s add this final piece of information to our file! Our file now looks like this:And, you’ve familiarised yourself with JSON. In this article, we understood what JSON is, why it is useful and finally, how to do (some) JSON. In doing so, we learnt about the JSON data types (String, Number, Array, Object and Boolean).The file above is a GitHub gist, and you can follow this link to download the file and play around with it, add some more info or make JSON files for new people. You can check if your code is valid JSON and can also format it using this tool.I hope this article helped you get introduced to JSON. Let me know how your journey is by responding to this story.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3100,AI is Reimagining Travel Personalisation,25,2019-07-10,https://towardsdatascience.com/ai-is-reimagining-travel-personalisation-c72685faa378?source=collection_archive---------26-----------------------,2019,"Recently I was working on an Image classification task where first I wanted to capture the region of interest from the image before feeding it into the model. I tried a technique called cluster-based image segmentation which helped me to improve my model performance by a certain level. Let us see what it is and some sample codes to do cluster segmentation, you can find the Jupyter Notebook at the bottom.Imagine that you are going to cross the road, what you do before you cross the road?First, you see both sides of the road to determine the approaching vehicles and other environmental objects, then you do some amazing estimation of approaching speed and decide on when and how to cross the road. All these happens within a fraction of time, how amazing isn’t it.How amazing our brain is, by determining the shapes of different objects, it able to detect the multiple objects in the same snapshot.Let me explain furthermore, assume we have our Image Classification model which is able to classify the apple and orange with more than 95% accuracy. When we input an Image that contains both apple and orange the prediction accuracy will go down. As the number of objects in the image increases the classification models' performances will go down. That is where the object localization comes into play.Before we detect objects in an image and classify it, the model needs to understand what is in the image, this is where Image Segmentation helps. It creates a pixel-wise mask for the objects in an image which helps models to understand the shape of objects and their position in the image at a more granular level.Image segmentation is broadly categorized into two main categories.In the first image, we can see that detected objects all are men. In semantic segmentation, we consider all those pixels belong to one class, so we represent them all by one color. On the other hand in instance segmentation, those pixels belong to the same class but we represent different instances of the same class with different colors.Based on the approach we use segmentation can be divided into many narrower categories.I am only going to give an example of cluster-based segmentation in this article as I promised at the top.Recall your understanding of clustering algorithms. Clustering algorithms are used to group closer the data points that are more similar to each other, from other group data points.Now think of an image that holds apple and orange. Most of the pixel points in apple should be red/green, which is different from the pixel values of orange. If we can cluster these points we can distinguish each object from one another right. That’s how the cluster-based segmentation works. Let’s see some code samples now.As it is visible to our naked eye there are five color segments in the ImageLet us see whether we can cluster them using our KMeans algorithm from scikit-learnWow, that works!!! We are able to cluster all five parts. This is how the cluster segmentation works.There are many advanced techniques like Mask R-CNN to do more granular level segmentation. Let us see those topics in some other article. I hope you now have some level of Understanding of Image Segmentation.You can find the notebook for the above example at https://github.com/Mathanraj-Sharma/sample-for-medium-article/blob/master/cluster-based-segmentation-skimage/cluster-based-segmentation.ipynbWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4212,Practical Coding in TensorFlow 2.0,51,2019-10-18,https://towardsdatascience.com/practical-coding-in-tensorflow-2-0-fafd2d3863f6?source=collection_archive---------17-----------------------,2019,"Stock market is a place where people arrange a transaction for buying or selling shares of publicly listed companyIndonesia has been named as the Largest Economy in Southeast Asia by the World Bank. As one of the four most populous countries in the world with more than 200 million people living in this country, Indonesia has a bargaining power to emerge as one of the biggest economies in the world. Those promising prospect of Indonesia economic condition makes this country as one of the best choices to put your investment on.Indonesia stock markets with more than 600 listed enterprises still on the uptrends before the coronavirus outbreak. Some of the listed enterprises are state-owned enterprises and for this case study, I will only analyze 17 state-owned enterprises which been a member of the latest IDX-BUMN20 Index.Stock used for analysis:ANTM.JK, BBNI.JK, BBRI.JK, BBTN.JK, BJBR.JK, BMRI.JK, ELSA.JK, JSMR.JK, KAEF.JK, PGAS.JK, PTBA.JK, PTPP.JK, SMGR.JK, TINS.JK, TLKM.JK, WIKA.JK, WSKT.JKThe assumption for this case study:We will import stock price data using getSymbols() function from the quantmod package. getSymbols() function is a particular function to load and manage financial data from various sources, but for this example, we will use Yahoo Finance as our data source.In default, we will have every stock price data assigned to their stock code as xts object (ex: ANTM.JK prices will be stored in ANTM.JK variable as xts object). To make it easier for analysis, we should gather all these separate data into a single xts object.We already made a portfolio, and it stored as single xts object in a variable named Index. The data that we store in this variable is still in closing price format for every market day, since most of the portfolio analytics method need to feed by return data, we have to calculate its Return first before taking a step into portfolio analysis.Fortunately, in R we don’t have to manually calculate every stock return day by day, we just need to call Return. calculate () function, and it will give us the return output in xts object format.Since nobody can see the future, most of the stock market investor relies on two things before deciding which stock they will buy, first is fundamental analysis and second is technical analysis. Fundamental Analysis involves the use of economic and financial data (e.g., production, consumption, disposable income, etc.) to forecast prices in the future, whereas Technical Analysis is based primarily (and often solely) on the study of patterns in the price data itself [1]. This article will cover only some of the Technical Analysis methods available.For the first analysis, we will try to look at every stock performance in term of their reward & risk in annualized return, since Investors sometimes want to see if their investments are well-rewarded compared to its risk, and Sharpe Ratio is a great calculation that can measure it all. Higher Sharpe ratio means our investments are well-rewarded compared to the risk that we have during the investing period. For calculation, we assume Indonesia Risk-Free Rate is 4.5% yearly.Annualized means we recalculate the number in a certain period on a yearly basis, for example, the return of ANTM.JK is -5.35%/Year during the 2013–2020 periods. Our portfolio annualized performance show there are varies from one stock to the others. The worst performer in our portfolio is PGAS.JK which has a negative return and swing too much during the periods makes PGAS.JK Sharpe Ratio is the lowest among others. The best performer in our portfolio is BBRI.JK which shows the highest return, smallest standard deviation, and highest Sharpe Ratio.If we compare the line chart between the best performer and the worst performer stock in our portfolio, we can obviously see the differences between them. BBRI.JK has more consistent movement and steadily increase from year to year, in the opposite PGAS.JK movement fluctuates with a massive span between its peak and valley.We can also make a general view about how our stock performance compared to Risk-Free rate Asset (4.5% annual return) using label plot in ggplot2 package.If we see in the big picture, during this period, 10 companies are well-worthy to be invested for, that means if we put our investment into these stocks, we will gain much more than if we put our investments into the risk-free assets. Using this plot, we know how vital diversification is in investment, some of our investments will have a high return, but other investments are losing its value than when we buy it.Now we will start to analyze those stocks as a portfolio and we will see how its performance if we hold all of those stocks in the same proportion from 2013 until July 2020.First of all, we will look at our portfolio daily return distribution. Using this distribution, we will see how often our portfolio gives us a positive return and how frequent it is losing its value.The daily return distribution plot shows our portfolio averaging positive returns near zero with some time. It can give more than 10% return in a day, but on the other hand, it possible to lose more than 10% of its value just in a day. We will prove it using time-series daily return plot and find out when our portfolio losing so much and when it gains the highest return.From the time-series plot, we can obviously see most of our return will be in the range of ±5%, even though there’s some outlier and the most recent is coming due to the coronavirus effect in the first half of 2020.Using only daily return is not enough to display how well our portfolio performance. Luckily, there is charts.PerformanceSummary() function in R that will produce portfolio cumulative return, daily return, and drawdown plot as its output.Performance Summary Chart sums up every information we need in analyzing our portfolio performance during the periods. From the cumulative return chart, we know our portfolio will still have more than 100% return until the 26th of July despite the fact that some stocks have negative returns and coronavirus effects that mostly disrupt every stock market activity in the world. From the daily return chart, we will get information about which period our portfolio return fluctuated so much and using this information we can develop an insight about how the major events in certain period can disrupt stock market transaction and make a plan if there will be any other major events in the future. From the drawdown chart, we gain the knowledge about how our portfolio performs against downtrend and how long it can recover so we can have a proper strategy on how we hold the stocks if in the future downtrends are happen.Technical analysis is about predicting the movement of the stock price using historical patterns and market statistics. This article has discussed how to use R-Computation to extract the pattern and gain insight into how our portfolio movement and performance during a certain period.For future investment, we can use the analysis result as the basis for every action that we will take. As an example, we can subset this kind of analysis into a smaller period and revise our portfolio composition based on the shape ratio.There’s also a method to find the optimum weight of our portfolio based on historical data. Hopefully, it can be discussed in the other articles.Notes: We can reconstruct every code in this article for every stock and every period as long as the data is available in the data source. Just try to change it to your portfolio composition and see how your portfolio was doing in a certain period.[1] Schwarger, Jack.D., & Etzkorn, M. (2017), A Complete Guide to The Futures Market, 2nd edition, John Wiley & Sons, Inc., Hoboken, New Jersey.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2635,Real-Time Dashboards to Support eSports Spectating,22,2019-05-23,https://towardsdatascience.com/real-time-dashboards-to-support-esports-spectating-48cfa1e43274?source=collection_archive---------15-----------------------,2019,"Over the course of a single weekend last January, two of the world’s leading AI companies found themselves caught up in embarrassing errors. Facebook’s automatic translation inadvertently translated “President Xi” to “Mr. Shithole” (in Burmese), while Apple drew criticism after Siri responded to queries about the Israel with references to “the Zionist occupation state.”I don’t envy the relevant PR teams, and obviously Facebook and Apple made mistakes. But the real lesson here isn’t about Silicon Valley bias or antisemitism or cultural sensitivity. It’s about AI. Specifically, don’t use AI unless you can tolerate stunningly embarrassing errors.As long as AI makes any errors at all, it will make embarrassing errors. That shouldn’t be surprising. To an AI system, actions are either correct or incorrect. When Google Photos labels your friends as “friends”, that’s correct. Everything else is incorrect. And as far as the AI is concerned, everything else is equally incorrect.Of course, that’s not exactly how the world works. When Google Photos incorrectly labeled two black people as “gorillas”, the consequences were a touch more significant than when it labels a bunch of plantain as “bananas.”AI designers can, of course, try to work around this. They can give the system 1 point for a correct label, 0 points for an incorrect one, and negative 1000 points for a label that sparks a geopolitical crisis. That may fix the problem at hand, but in the long term it’s a fool’s errand. Crisis-inducing errors are products of context and social nuance that an AI program is unlikely to capture.Despite AI’s propensity to generate mortifying blunders, it continues to be a tremendously valuable family of technologies. And so, it’s worth reviewing scenarios where it’s successfully deployed.One obvious example is domains where embarrassing errors are entirely acceptable. Advertising is a great example. As long the advertising platform continues to print obscene amounts of money, embarrassing errors can be shrugged off. When a website shows you an ad for a product that you’ve already purchased, that’s an embarrassing error. Perhaps you’ll screenshot it and tweet to all your followers about how dumb online advertisers are. But that’s as far as it goes. As far as the advertiser is concerned, the only harm is that they showed you an ad for something you’re not going to buy. Put differently, the only problem with an embarrassing error is that it’s an error.In contexts where embarrassing errors aren’t acceptable, the best approach is to provide a layer of insulation between the model and users. This is what Google ultimately did in the aforementioned case: they implemented a hack that ignores the image recognition model when it produces labels like “gorilla”, “chimpanzee” or “ape.” The crucial point is that they accepted that the model would continue to produce embarrassing errors regardless of attempts to fix it, so they designed an external process to ameliorate those errors.If the system doesn’t need to be fully automated, one effective approach is to use AI for decision support rather than as a standalone decision maker. This model may be particularly important in healthcare. IBM’s Watson Health reported some promising accuracy metrics, but its tendency to make embarrassing, dangerous errors shredded its credibility with physicians. In one memorable example, it recommended giving drugs that worsen bleeding to a patient with severe bleeding. Had Watson been more explicitly framed as a tool that assists physicians by surfacing treatments for consideration, the errors could have been more tolerable.There are many open questions about how AI will impact society. The answers to those questions will be shaped by how system designers and consumers handle embarrassing errors. Imagine, for instance, that self-driving cars get to the point where they’re statistically safer than human drivers, but the few fatal crashes are the result of inexplicable malfunctions. Will passengers trust an AI that is safer than a human driver but also more likely to crash in conditions that a human would have handled well? Time will tell.AI brings a lot of benefits, and undoubtedly, we’ll continue to see it used in all sorts of contexts. But before you decide to add AI to your business, take a moment and ponder how you’ll respond to your fancy new tool insulting the President of China.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1358,"Quantum Computing Notes, for a Python Programmer: Complex Numbers",451,2018-11-11,https://towardsdatascience.com/quantum-computing-notes-for-a-python-programmer-complex-numbers-2e1a574bdb88?source=collection_archive---------5-----------------------,2018,"Natural language processing has exploded in the last decade. With the rise in both computing power and availability of data, the market for mining human speech for insight is strong.As NLP proliferates, however, we have to pay close attention to the space into which it’s expanding. Are we creating an Anglophone feedback loop with the data we collect and the tools we use to analyze it?NLP techniques, by and large, were invented by native English speakers. English tends toward what linguists call an analytic language. That means it has a low morpheme-per-word count and lacks the kind of grammatical markers that other languages use for things like tense, person, mood, and aspect, which it expresses instead through word order and associations with other words.For example, in English, the following sentences are not equivalent:Klaus eats the salad.The salad eats Klaus.But in German (a fusional language), they would both mean “Klaus is eating a salad,” because the fusional morphology indicating case marking makes the word order less strictly necessary:Klaus isst den Salat.Den Salat isst Klaus.Even more meanings per word can occur in agglutinative and polysynthetic languages:But most NLP libraries are optimized for tokenizing, lemmatizing, and tagging parts of speech in English and English-like European languages.Although resources have been expanding, there are still fewer options for languages other than English. In part, this conundrum stems from the bias towards applying NLP to the languages with the most data. English is still the language of the internet, and it — along with the tongues of other affluent, connected countries — is, therefore, overrepresented in the datasets. Without enough data, using traditional high-dimensional NLP techniques on smaller languages won’t be effective.Since NLP is often used to provide high-value market insights and spur investment, this lack of attention can lead to a self-perpetuating cycle in which low-resource languages continue to receive fewer services, and thus generate less data, than better-attended ones. Ultimately, they may lose speakers, and the world will lose linguistic diversity.Yet as we know on the project level, “[g]reat care should be taken not to hastily remove or change values, especially if the sample size is small.” Max Kuhn and Kjell Johnson, Applied Predictive Modeling 33 (5th ed. 2016). “Also, the outlying data may be an indication of a special part of the population under study that is just starting to be sampled.” Ibid. 34. This advice is no less true from the macro-level perspective of cross-linguistic data science. For historically understudied languages, this incipient exploration can make an enormous difference — for the intellectuals who study it, the businesses who can profit from that insight, and the communities who benefit from having their words acknowledged and examined.The gap must first be narrowed at the data-collection stage. Some NLP practitioners have begun implementing machine learning much earlier in the linguistic research process to improve the chances that the corpus will be usable — and used. Ensuring that field linguists and data scientists communicate to produce mutually beneficial, machine-readable corpora is an important consideration.Other techniques may seek to make the best of a sparse dataset by implementing methods that do not require such high-dimensional feature creation. Semi- and unsupervised techniques can be used to skirt Hughes phenomenon. To adapt existing models to entirely new structures requires not only linguistic knowledge but creative thinking. Neural techniques are beneficial in this scenario. One group has already used deep learning to revive Seneca. Others are following suit, with Google’s help.Machine learning gives us an unprecedented opportunity to preserve and explore understudied languages that no generation before has ever had. Those who rise to the challenge will enrich not only themselves, but the collective understanding of our shared linguistic heritage.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
5592,Predicting Market Rank for Airbnb Listings,11,2020-02-04,https://towardsdatascience.com/predicting-market-rank-for-airbnb-listings-59009a886d6?source=collection_archive---------44-----------------------,2020,"Last week I wrote an overview of Linear Regression and what’s happening under the hood of OLS regression from statsmodels. This post will serve as a high-level overview of Logistic Regression to perform classification tasks. Logistic Regression is a great first model to learn when introduced to classification.Supervised Learning is a term referring to machine learning algorithms that have the ability to “learn” from a labeled (ground truth) dataset. The data needs to be labeled so the supervised learning algorithms can evaluate their performance. The performance is evaluated by comparing the predictions with the actual labels for the training data. By doing this, the algorithms can improve their performance in classification.Supervised learning is made up of two categories, classification and regression. Regression helps us in forecasting values such as, how much something will sell for. Classification helps us determine which class something belongs to, e.g., male or female.In supervised learning, the loss function is used to evaluate the predictions from the algorithm with the actual labels. There are different types of loss functions used for classification and regression, they serve the same purpose, provide the model with the ability to evaluate how right or wrong a prediction is.With logistic regression, we are not trying to predict a continuous value, we’re modeling the probability that an input variable belongs to the first/default class. This is where the sigmoid function comes in.Where e is Euler’s number and t is the continuous output from the linear function. For a refresher, the linear equation:Where b0(beta0) is the intercept and b1(beta1) is the coefficient for that input value. By passing the linear equation into the sigmoid function, it will output a probability between 0 and 1. Our equation will become:Then, by multiplying the numerator and denominator by e to the linear equation, our equation turns into:Once we’ve obtained the probability of class one given our betas, we can calculate the probability for class 0.When we have the probabilities, we can compute the odds ratio by dividing the probability of class 1 by the probability of class 0 (or all other classes).By computing the log of both sides, we will get the log of the odds that we predict the first class.A derivative is a function’s rate of change. When working with linear functions, we can easily calculate the derivative by taking two points, and dividing the change in y by the change in x.When it comes to nonlinear functions, the function’s rate of change is always changing. If we have larger values for the change in x (Δ𝑥), the derivative will represent the rate of change less. However, if we were to decrease Δ𝑥, our function should converge. The number that the function converges on is our derivative. Here’s a helpful link on the rules for derivatives. Our derivative formula becomes:The term gradient descent refers to minimizing the cost function with the use of partial derivatives in order to find the regression line that best represents the data. Using gradient descent, we can determine how to change the regression line when both m (slope) and b (intercept) can be changed. This will help us change the regression line by the slope and the intercept to get the greatest decrease in cost. Using the RSS (residual sum of squares) we can calculate how to change both variables, our cost function becomes:Where J is just another way of saying residual sum of squares. J varies as the slope and intercept variables of the regression line change.To get the gradient of a function, we can calculate the partial derivatives of each variable.Using the derivative chain rule, our partial derivatives become:By subtracting the partial derivatives (formulas on right above) from the previous slope and intercept values, we can descend towards the minimum.MLE in logistic regression helps in finding which coefficients minimize the error within the predicted probabilities, i.e., find which underlying parameters maximize the likelihood of a probability that a value belongs to the default class. MLE looks at the conditional probabilities from each variable as an independent probability, and computes its total probability.When calculating the MLE, in order to simplify things we use the log-likelihood. We can also use the negative log-likelihood since we are looking for minimums.Taking the classes in our dataset (yi), and reward or penalize the model depending on how close the probability (pi) is to the actual class (yi). It will return a high value if the probability value is high and close to the actual class. For example, if we had a probability of .98 and the actual class was 1, we’d see a higher score than a probability of .18 and the actual class 1.Since it is more difficult to take a derivative of multiplication than addition, we take the log of the likelihood.Since we’re trying to minimize the error of a probability occurring, our cost function minimizes the negative log-likelihood.Now, we can take the derivative (rate of change) of the negative log-likelihood and set it zero in order to find the gradient and update the parameters.When modeling, we tend to see overfitting and underfitting. Overfitting refers to when the model is fit to the noise in the data, and isn’t able to successfully generalize the data. Overfit models tend to perform significantly better on the train set than the test set. Underfitting refers to when the model is too simple, and not able to capture the information in the data.With regression, as well as many other models, we evaluate the performance by measuring the error. Regularization works to minimize the function by reducing either a model’s residuals or the size of the coefficients. The three main types of regularization are:Also known as L2 Norm Regularization, minimizes the cost function by adding a penalty term λ (lambda) to the magnitude of the coefficients squared. Ridge tends to shrink the coefficients while helping reduce the complexity of the model. In the formula above, the first term is the sum of squares, the second is the cost function, and the final term adds a penalty using the squares of the coefficients.Lasso regularization also adds a penalty term to the magnitude of the coefficients but they are not squared. This method of regularization works with the sum of the absolute values. In the formula above, the first two terms are the sum of squares and the cost function. The final term adds a penalty using the absolute values of the coefficients.This method is a combination of ridge and lasso regularization. Elastic Net allows us to specify how much of each regularization term to use.Unlike linear regression, there doesn’t need to be a linear relationship between the dependent and independent variables. The 5 assumptions for logistic regression are:1.) Binary / OrdinalBinary logistic regression — dependent variable is binaryOrdinal logistic regression — dependent variable is ordinal2.) IndependenceThe observations are independent of each other.3.) Little/No MulticollinearityThere needs to be little or no multicollinearity between the independent variables.4.) LinearityWhile there does not need to be a linear relationship between the dependent and independent variables, there does need to be a linear relationship between the independent variables and the log odds.5.) Large Sample SizeThere generally needs to be large sample size when performing logistic regression. A good “rule of thumb” is to have a minimum of 10 cases with the least frequent outcome for each independent variable.Let’s go ahead and see how the concepts above can be easily implemented with Sklearn. Once again, I will be using the infamous titanic dataset. The dataset was obtained from Kaggle. The goal being to predict whether a given person survived or not.There are 177 out of 891 missing values in the Age column. For the purposes of this pipeline tutorial, I am going to go ahead and fill in the missing Age values with the mean age. There are 687 out of 891 missing values in the Cabin column. I am removing this feature since approximately 77% of values are missing. The Embarked feature is only missing 2 values so we can fill these with the most common value. The Name and Ticket features both hold unique values to each passenger and will not be needed for predictive classification so they will also be dropped.Now that we’ve handled the missing values in the dataset, we can move on to defining the continuous and categorical variables.This returned a higher accuracy score than I expected using the default parameters. Remember how logistic regression models probabilities? We can see the probability estimates with predict_proba.Each row in the array above tells us the probability of that output being a 0 (left) and 1 (right). For the first row of the probability estimates, there is approximately a 94% chance of the output being 0 and approximately a 6% chance it is a 1.In order to find the most optimal inverse regularization strength, we can create a list with Numpy containing 1000 values from 1 to 1000. In case the model performs the better with a really high regularization strength, we will insert 0 and 0.0001 to this list. By running a for loop and fitting a logistic regression model with each C value, we can store the each score and c value in a dictionary for reference.This only increased our model’s performance by a small amount. However, the training accuracy decreasing and the testing accuracy increasing tells us the model may be overfit. Let’s take a look at the confusion matrix.If we wanted to protect the model from false negatives, we could increase the threshold to be higher than the default 0.5.There you have it, hopefully this explanation of logistic regression cleared some things up for you. We went over quite a few formulas but luckily there are libraries and modules that perform a majority of these steps for us.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4989,When the statistic used changes the AFL age narrative for 2020,12,2019-12-09,https://towardsdatascience.com/when-the-statistic-used-changes-the-afl-age-narrative-for-2020-954e938076ef?source=collection_archive---------41-----------------------,2019,"Allow me to preface with the fact that I am nowhere near an expert in Machine Learning. I am not writing this article to pretend that I am either. Talk to me in 15 years and then I might consider myself to be approaching the expert title. I am on a path of learning something new and implementing it in real world situations every single day. This mindset is needed in the field and is a given for success.Like almost every other path to success, it all begins with a healthy mindset. If you cannot believe in yourself to be successful, then who will?It is extremely easy to compare yourself to the Ian Goodfellows, Andrew Ngs, and Lex Fridmans of the world. I do it all the time. However, the only thing they really have when compared to me is time — which is why they are who they are today. They decided to forge their own path, put in the work, deal with the stresses that come with that work and let time take its course. You and I —nothing more than rookies when compared to Ian, Andrew, and Lex— must do the same.Patience mixed with long-term thinking will always yield success. If you want success quick, you can probably do it. Discussing the sustainability of that success is another conversation, however. Your mindset is key in deterring short term gratification for long term gains. This is the best sacrifice you can make for yourself, because you get to choose that sacrifice. My guess is that when you see those long term gains finally manifest, you won’t look back at what you did as a sacrifice.With all of this being said, I would like to also hand off some of my favorite free resources for this field in hopes that your career will benefit from it.I was appalled when I saw Anthony’s article on TDS. This YouTube channel covers nearly every key topic in Machine Learning that technology firms want out of a Machine Learning Engineer. If you want to learn more about the channel, I recommend you read article I mentioned above.Reddit’s communities span across every single topic or interest you can find, with the field of Machine Learning being no exception. This resource is not like Machine Learning University where it is strictly tutorials and case studies, but rather serves as a broader community for those who feel isolated from the field. While learning new tasks and skills is extremely important, having a community to share what you learn is equally as valuable to your growth as an ML Engineer.Lex Fridman, as I mentioned above, is one of the few at the forefront of AI research. His podcast — formally called the Artificial Podcast with Lex Fridman — The Lex Fridman Podcast brings on various scientists of all disciplines and fields including but not limited to AI, Computer Science, Physics, Mathematics, and so much more. His YouTube channel, which hosts the video portion of the podcast, also contains his Deep Learning lectures at MIT — which I find to be extremely valuable.This podcast is one of the few that make me think hard with every episode regardless of who is speaking. The podcast is available wherever you listen to your podcasts.sentdex is a YouTube channel with various types of Python programming tutorials ranging from all levels of experience. When I first started to learn Python, his channel served as an exceptional resource to lay the foundations I am currently operating from. Whether it’s becoming more familiar with Python, learning how to use Google Cloud, or building a Neural Network from scratch, setdex has you covered from these topics to everywhere in between. For those who are self taught, I heavily recommend using this channel as your “search engine” for topics you may not be familiar with.Tech with Tim is a YouTube Channel with various tutorials and projects to learn and get inspiration from. Many of his projects are in Python, so if you are newer to the language this alongside with sentdex will be an invaluable combination to your growth as a Python programmer. Tim’s projects utilize a lot of AI as well as some of them focused around game development for those interested in blending the two fields together.To become an expert in anything, it takes time, patience, and a sense of community. I hope that these resources (alongside my motivational introduction) can help you push forward during trying times and achieve your greatest dreams.Thank you for reading.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1302,Pandas DataFrame: Playing with CSV files,496,2018-11-02,https://towardsdatascience.com/pandas-dataframe-playing-with-csv-files-944225d19ff?source=collection_archive---------0-----------------------,2018,"There’s a fine line between telling stories with data and telling lies. Before I tell you how to spot a top-notch data analyst and boost your analytical excellence, let me scare you a little.Humans brains are pattern-finding powerhouses… but those patterns don’t always have much to do with reality. We are the sort of species that finds rabbits in clouds and Elvis’s face in a potato chip.Take a moment to consider the Rorschach test — the one where people are shown random inkblots and asked what they see — and you’ll appreciate just how eagerly the mind injects spurious interpretations into randomness.Psychologists have a pretty name for this tendency to conjure false meaning out of nothing: apophenia. Give humans a vague stimulus and we’ll find faces, butterflies, and a reason to allocate budget to our favorite project or launch an AI system.Uh-oh.There’s plenty of random noise in most datasets, so what are the chances there’s no apophenia going on with your analytics? Can you really trust your interpretation of the data?What the mind does with inkblots it also does with data.To make matters worse, the more ways there are to slice-and-dice those datasets and the more complex they are, the more vague they are as stimuli. That means they’re practically begging you to see false nonsense in them.Complex datasets practically beg you to find false meaning in them.Are you sure your latest data epiphany isn’t an apophany in disguise?If that sounds dismal, I’m not done yet. Taking data analysis courses can pour fuel on that psychological fire. Students are conditioned to expect that looking at data yields real meaning because every homework exploratory analysis exercise has buried treasure in it. Very few professors have the heart to send you on wild goose chases (for your own good!) and it’s hard to grade open-ended assignments, so you usually don’t get enough exposure to them as a student.Students grow up believing that every dataset is ready to cough up a nugget of solid truth.Data storytelling is just a hop, skip, and jump away from outright lying with data. Setting aside the issue of whether the patterns are real, let’s talk about multiple interpretations. Just because you see a bat shape in that inkblot doesn’t mean that there isn’t also a butterfly, a pelvis, or a pair of foxes in it. If I hadn’t mentioned the foxes, would you have seen them? Probably not. Psychological mechanisms related to motivation and attention have stacked the deck against you. It takes a special sort of skill to release the bat interpretation and force yourself to see a superposition of meanings.Once people glom on to their favorite “insight”, they’ll struggle to unsee it.The trouble is that once people glom on to their favorite “insight”, they’ll struggle to unsee it in favor of others. People tend to believe most strongly in whichever interpretation captured their attention first and each additional meaning reduces their motivation to keep searching. Juggling multiple potential stories without overweighting your favorite is a mental muscle that takes hard work to build. Alas, not every analyst has the discipline for it. In fact, many are incentivized to “prove” one side of a story through data exploration. Why grow skills that only get in the way of engorging your data science paycheck?There are ways to prove things with data (honestly and rigorously)— my data-splitting article will tell you more — but exploratory data analysis (EDA) is not one of them. Open-ended data exploration is always a fishing expedition. What determines the color of your lightsaber is what you’re fishing for.If you join the dark side, you’re fishing for evidence to support a theory you already “know” to be true (so you can sell it to some naive victim). You might not even realize that your lightsaber is red if you genuinely believe in data objectivity and your own unbiasedness.Open-ended data exploration is always a fishing expedition.With a sufficiently complex (vague) dataset, you’ll find a pattern you can spin as support for your favorite story. That’s the beauty of the Rorschach test, after all. Unfortunately, it’s worse with data than with inkblots because the more mathemagical your method (p-hacking, anyone?), the more legitimate and convincing you’ll sound to those who don’t know any better.Those who reject the dark side also go fishing, but they’re after something else: inspiration. They’re looking for patterns that might be interesting or compelling, but they know better than to take them as evidence. Instead, they practice a sort of open-minded analytics zen with the discipline to be mindful of as many interpretations as possible.The best analysts challenge themselves to find as many interpretations as possible.This takes a sharp eye and a humble, unsticky mind. Rather than tricking their stakeholders into seeing only one side of a story, they challenge themselves to do the creative thinking required to digest the same data into as many stories as possible. They present their findings in a way that inspires rigorous follow-up without causing their leadership team to run overconfidently off a cliff.Open-mindedness gives data analysis a chance to be worthwhile.As an added bonus, the discipline to look for multiple interpretations is an analyst’s secret weapon for not snoozing past the real treasures buried in the data. If you’re distracted by a falsehood you believe in, confirmation bias makes it hard to notice evidence that points in the opposite direction. Why bother analyzing anything if your conclusions are determined in advance? Open-mindedness gives the whole endeavor a chance to be worthwhile.If you liked my other articles about analytics, here are the traits you’re already looking for in a great analyst:In addition to all that, this article suggests you look for analysts with three more traits:Finally, if you’re a leader, turn a critical eye inward and make sure that you’re giving your people the right incentives. Are you looking for a data analyst or a data spin doctor? These take different mindsets (and skillsets!), so choose wisely and reward the right behaviors.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
5637,Analyzing Fan Sentiments After NBA Trade Deadline,30,2020-02-08,https://towardsdatascience.com/analyzing-fan-sentiments-after-nba-trade-deadline-1b73f72d0ca2?source=collection_archive---------36-----------------------,2020,"When we talk about data science, we usually refer to the data analysis through summarization, visualizations, sophisticated algorithms that learn patterns in data (machine learning), and other fancy tools. When we discuss the term with software developers, we also hear a lot of Python, the popular programming language.But why is Python so popular and special in the data science world? There are many reasons, and an important one is the Python ecosystem and libraries that make data science seem natural to Python.One of these libraries is pandas, which every data science in the world uses, used, or at least heard of (if you are a data scientist who never used pandas, scream in comments).Pandas is an essential part of the ecosystem that many other data science tools build on top or provide specific functionalities for pandas.This guide introduces pandas for developers and aims to cover the what, why, and how of pandas’ most commonly used features.Before we get started, if you want to access the full source code for this project to follow along, you can download the project’s source code from GitHub.Some links may be affiliate links. We may get paid if you buy something or take an action after clicking one of these.Pandas, in the past couple of years, have established itself as one of the best libraries for data analysis. It is such the importance and power of pandas that one of the first things data scientists do when receiving a data set is to load it into pandas to begin understanding it.Pandas provides them with anything they need for cleaning, transforming, and analyzing data. It gives so many features for analysts that it can make entire books about them.Here are some of the things you can do with pandas:Pandas is an external library, and as such, requires installation for your project. The library name is pandas, and you can install it using your favorite package manager for Python. In my case, I’m using pipenv as I’m not a big fan of conda, but the process applies the same.If you checked out my code from GitHub, simply run the following command as all dependencies are present on the PipfileIf you want to follow along with this tutorial from a new project, you can simply install pandas by running:And I highly recommend using a Jupyter notebook as I did for experimenting and testing on the data, so you will have to install that as well by running:If you need more information about how to use Jupyter notebooks, you can check this guide.The last thing before starting is to import the library.It’s not mandatory to provide an alias, but it’s very common to use pd, it makes it straightforward, and it’s how people do it, so let’s follow along.Pandas has two basic building blocks for all its functionality: Series and DataFrames. In simple terms, a Series is a column, and DataFrames is a multidimensional dataset with multiple Series.Though you can manually create DataFrames in pandas, which makes it easy to use is its ability to directly load data from multiple sources such as CSV, JSON, or databases.Reading CSV files is as simple as one line of code with pandas.Importing a CSV file is as simple as calling one function read_csv with the file name. The second line of code calls the function head, which prints the first five rows of data. More on that later.Reading JSON files is not more complicated than reading a CSV, it’s a matter of a simple function call:Reading data from a SQL database requires some additional steps compared to reading CSV or JSON files. This is because pandas doesn’t support databases natively but instead relies on third-party libraries to establish the connection.Once the connection to the database is ready, you can work directly with pandas using the function read_sql_query.For our scenario, we will read data from SQLLite, but other database engines are supported, such as MySQL, Postgres, etc.Note: If you are running this code on your own and not using the tutorial’s provided project, make sure you have pysqlite3 installed before continuing.As discussed, first, we need to establish a connection to the database.Later, we can directly run SQL queries in pandas to retrieve information. If you need help with SQL, I can recommend the complete SQL bootcamp course on Udemy.In our case, we will just fetch data from one table.So far, we read data by using the default values and works great for most cases. However, there may be circumstances in which you need to change how to read and parse the data. Pandas reading functions are complete and offer tons of customizations. You can read about them on the official docs .One important and highly used parameter is the index_col which allows you to specify the column or columns used as the index of the DataFrame. Thing that is very useful when slicing or selecting data, as we will do later on in the guide.Sometimes we perform changes to our datasets to facilitate the data analysis. However, those changes are lost unless we save them. Pandas provides simple methods to store a DataFrame into a file or database.Note that we can store the data on any type of file regardless of the format we first loaded.When you first load your data set, you want to understand its structure and start making sense of the data. In this section, we will cover basic pandas functions that will help you exactly on that.The first function we will cover is the head function, which we already introduced. This function prints on the screen the first n rows of data, and it is handy to explore the columns and types of a dataset for the first time.You can run this function with no arguments, in which case will default to 5 rows:Or you can pass the number of rows you would like to see:If you want to see the very last rows, you can use the function tail. As with head, you can provide how many rows you would like to see:Another interesting function you probably need to run after loading your data is the info function, which provides essential information about your datasets, such as the number of rows and columns, the number of non-null values, data types, and memory usage.If you are trying to know just how many columns and rows of your data has, then you can get the shape attribute of the DataFrame:(note that shape is an attribute and not a function)If you are looking to understand your dataset’s values better, the function describe will provide a statistical summary of each column, with information like count, mean, standard deviation, etc.DataFrames expose hundreds of methods to work on data sets, way too many to know them all. However, some operations frequently occur during data analysis tasks. We will review all those methods that are crucial to understanding for all data scientists.Duplicates can distort our results, so it’s essential to deal with them. Pandas offers some utilities to handle duplicates like drop_duplicates, which automatically removes all duplicate values from the DataFrame.Note that for our example, we loaded a new file that contains two duplicates, and here are the results:Keep in mind that drop_duplicates won’t affect the original DataFrame (by default), and instead, it will return a new DataFrame with unique values. You can alternatively pass the argument inplace=True to modify the original instead.There are several ways to rename the data frame columns in pandas but one of the most useful and easy ways is using the rename function:Similar to drop_column, rename will return a new DataFrame with the updates.rename also supports renaming more than one column at a time:Missing data in a real-life scenario can be a big problem. Fortunately, pandas is design to detect the missing values or NA (Not Available) values. To detect the null values, we use the isnull() function:The isnull function returns a new DataFrame with the shape of the original one with boolean values (True or False), indicating whether a particular cell is null or not. This can be very helpful to perform calculations or replace empty values.Sometimes we can’t work with missing values, so it’s best to simply remove the entire rows. You can achieve that with the function dropna.Which results in:Judging by the results, most of our records contain at least one empty column. It wouldn’t be wise in this scenario to simply drop all these rows from the DataFrame.So far, we have been working with entire datasets or performing basic filtering like removing empty rows. But how can we intentionally select data from the DataFrames?There are two ways to access or slice a pandas DataFrame, either by column or by rows.By columnExtracting data by columns on a DataFrame is super easy. You simply use [] with the series (column) name as follows:In which case it would output:Note that the result is a DataSeries and not a DataFrame. Sometimes because some methods and properties differ between both, we want to transform a DataSeries into a DataFrame, we do that by using the [[]] notation:Which outputs:By row When selecting data by row, we have more options than by columns. We can access a row by indices or perform a search in the dataset by querying or applying conditionals.We will start with the property loc, allowing access to a group of rows and columns by label(s) or a boolean array.Note that if we try to use loc with labels, you need to have your DataFrame indexed as we did in the case of the drinks DataFrame above.Additionally, you need to consider that looking by index requires an exact match and it is case sensitive, so in our case, Argentina is correct but Arg or argentina will raise an exception.The output for the code is:Interestingly enough, you can also access multiple rows at the same time using the same techniques we do for lists, so for example:This will return a new DataFrame with all the results between Argentina and Austria inclusive.How to filter data with no labels? You can use indices using the property iloc, which is equivalent to slicing on a Python list.Which will produce the same result as above, selecting from index 6 (Argentina) until index 10-1 (Austria).It is also possible to assign an increment, the same way as we do with lists.Sometimes you need to show only the rows that meet some conditions like displaying the countries with more than 320 beer servings. For that you can use conditional selections.Here is that example in code:Let’s break it down. df_drinks['beer_servings'] > 320 is a special conditional, as it applies to a complete DataSeries. The result will be a new DataSeries with boolean values representing the result of the conditional for each row in the original DataFrame.Then, by using df_drinks[] of a boolean DataSeries, we return a new DataFrame filtering out all the rows with False value on the conditional DataSeries.I wasn’t too surprised about the results, except for the case of Venezuela and Namibia.Note: in some other tutorials you may find the same code but leaving .loc aside, e.g.:And though in this case, that code works, there is a corner case in which it won’t, so better to be explicit than implicit and use .loc. You can read more about this on this StackOverflow thread.Statistics and math are among the most important skills that a data scientist should have in his belt. Fortunately, pandas makes it easy for us to perform statistical calculations on our dataset.This section will cover how to perform some statistical calculations on the data set such as the mean, groupby, and count functions.Starting by using the mean() function, which will return the mean values of the numeric DataSeries:The mean() function has returned the mean values of only the numerical columns, making sense since you can’t perform a statistical calculation on a text.We can also perform this calculation on DataSeries level, in which case we will return a single computed number.Let’s say that you want to calculate the mean of some columns based on a specific group so for that goal you have to use a pandas function called groupby() function.By using groupby we can perform calculations grouped by sets of data, for example the mean of each continent.Same as mean, you can calculate the median , min , max , sum , and much more.Another popular function is to count the number of rows/columns, which can be done by using count.By specifying the axis=0 (default value) we count the number of rows per column. However, we can also invert the axis (axis=1) and count the number of columns per row.This section will discuss some of the most common statistical functions in pandas, a must-have in any data scientist’s arsenal.Let’s start with the cov function that will calculate the column’s pairwise covariance. See the below code:The cov functions have calculated the covariance values of the pairwise covariance of all the columns.Another commonly used pandas function to calculate the change percentage of the value is the pct_change function. This function will calculate the percentage change of the current values compared to the previous element.First, we’ve created a data series index using the date_range() function and a data frame containing just random values for demonstration purposes and finally applied the percentage change function on that data.Let’s see now how to calculate the relationship of two values in what’s known as the correlation. The used function called corr() function and it calculates the relationship of all columns in your data frame and doesn’t include the null values, not available values, and non-numeric data such as texts.We’ve used the previous random values of the covariance example to calculate the correlation of the columns.Learning how to rank your data based on a specific standard like ranking the movies dataset based on their rating score is one of the most commonly used things in data science when dealing with datasets. The below example will use the rank function in Pandas:We’ve created a new column called “‘revenue_rank’” with the ranking assigned by the rank function based on the “Worldwide Gross”.Pandas can’t plot by itself, but it provides helper methods that would use plotting engines such as matplotlib or seaborn to perform the task.For our guide, we will use matplotlib , if you are running the code on your own, make sure that you have the library installed. If you are using the code from GitHub, this is already a project dependency.Scatter plot is a diagram that displays points based on two variables and luckily Pandas makes that easy for us to create a scatter plot just by a simple function called plot() function:The first chart shows the survival status spread by age and fare, painting with red those who did not survive and the others in green. To set the colors is necessary to generate a color map depending on the status. This can be done in a better way which we will analyze later.The plot function is doing all the heavy work by rendering the chart.Line chart is a series of values that are connected with a line and it is one of the most used charts. Pandas also able to display this kind of plot very easily. See the following code:A line plot showing the number of survivors by age.Bar plot is one of the most charts in the data visualization field and essentially represents categorical data with a rectangular bar. Pandas as well offer this kind of chart. See the following code:You can also build histograms.Or much more complex charts. Here is an interesting article on building charts for this particular dataset.Data scientists need first to explore, clean, and transform their data before going to the visualization process. Pandas makes it easy to perform these actions with just some simple commands and can be used to plot the data instead of using the other libraries such as matplotlib and seaborn. Still, you may find yourself in need to learn these visualization libraries for more complex charts.You can learn more on how to use pandas, plotting and much more on the Data Science Bootcamp on Udemy.Thanks for reading!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4872,Visual Kinship Verification: A Python Pandas Tutorial,28,2019-12-02,https://towardsdatascience.com/demo-for-rfiw-2020-task-1-kinship-verification-8a8ed7081bcc?source=collection_archive---------42-----------------------,2019,"As a data scientist, I often work on anti-fraud investigation missions. Exploration is therefore an essential part of the investigation. It allows one to become familiar with the subject of the analysis. I will detail here a simple, fast, efficient and reproducible way for you to get a global idea of the images you have. This is my first article so do not hesitate to ask your questions and make your comments. Enjoy 😉This algorithm uses Python 3.6.8 and the libraries keras (version 2.2.4), pandas (version 0.24.1), scikit-learn (0.22.2.post1), numpy (version 1.18.2) and matplotlib (version 3.0.3).To install Python, if you are on Windows or Mac, you can download the installer here. For linux users, the following bash command will install Python 3.6:sudo apt-get install python3.6For libraries, the following commands in your terminal (bash, powershell, etc.) will install them: pip install keras==2.2.4 pandas==0.24.1 scikit-learn==0.22.2.post1 matplotlib==3.0.3 numpy==1.18.2Finally, the visualisation of the results is done using Tableau Software. It is a paid software but you can try it for free here.We can now import the libraries and specify the path to the images:For this example, I chose to use the EuroSAT (RGB) dataset available here.The first step is to find a way to richly describe images. A quick and simple method is to use a neural network already trained on a general classification task as an encoder. Our goal will not be to classify but rather to group similar images together using the features extracted by the network. The ResNet50 residual neural network trained on the ImageNet dataset is a very good start.Want to learn more about residual neural networks? The first part of this article explains it very well.Why a ResNet50 trained on the ImageNet dataset? The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories with a typical category, such as “balloon” or “strawberry”, consisting of several hundred images. Thus, a neural network trained to classify ImageNet images and achieving a good score will have learned to correctly differentiate the shapes and characteristics of the images. This faculty will be precious to us. The problem is that our ResNet50 as it stands only knows how to predict one class from the list of ImageNet classes. To overcome this, we have to remove the last layer of the network used for classification. The output of the network will then be a vector of characteristics of dimension 2048.We humans find it very difficult to grasp more than 3 dimensions. So imagine 2048…One of my preferred methods for visualizing large vectors is the t-SNE algorithm. t-SNE stands for t-distributed Stochastic Neighbor Embedding. It is a machine learning algorithm for visualization based on Stochastic Neighbor Embedding. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback-Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map.Like many algorithms, the parameters must be chosen carefully. This article explains very well the influence of the different parameters of the t-SNE.In our case, the following values of the parameters allowed us to obtain a satisfactory result.Once the projection has been made, we can visualise the result.A simple scatter plot can already be used to identify some groupings.The disadvantage of this simple graph is that it does not allow you to view images represented by dots. For this, more advanced visualisation solutions are required such as Power BI or Tableau Software which is detailed below.Tableau is a paid data visualisation software that is rather simple to use and allows a large number of different types of visualisation, including interactive visualisations.We must first save the results of the t-SNE.Then, in Tableau Software, click “New Data Source” > “Text file” and select your CSV file to open.Once the data has been loaded, go to “Sheet 1”.Here, right-click on “X” > “Convert to Dimension”. Do the same with “Y”.Add “X” and “Y” to the “columns” and “rows” fields respectively by dragging and dropping them. Click on the small arrow that appears when you move the mouse over “X” in the “columns” field, and then click “Continuous”. Do the same with “Y”. You should get a visual similar to the scatter plot.In anticipation of later, drag and drop “image file paths” into the “detail” box in the “Marks” pane.To make the visual more aesthetic, you can make the grid, the axes and the name of the sheet disappear. To do this, right-click on the grid > “Format”, then in the “Format Font” pane on the right, click on the “Lines” icon and deactivate the different lines by selecting “None” for each one.Then, with a right click on the axes, you can deactivate the “Show header” option. Right-click on the title to activate the “Hide title” option.Finally, in the “Marks” pane, you can modify the shape of the points, their size, colour, etc. Personally, I like to make them appear as small squares with an opacity of 75%. This reminds me of small polaroid images that we would have sorted and put together on a table.You can also make the name of the image appear by dragging and dropping “image file names” into “tooltips” and then, by double-clicking “Tooltip”, you can edit the content of the tooltip and modify it to keep only “<ATTR(Image file names)>”.We can now switch to the “Dashboard” tab by clicking on the “New Dashboard” icon at the bottom next to Sheet 1. Let’s increase the size in the “Size” pane by changing “Width” and “Height” to 1600px and 900px respectively. You can then drag and drop “Sheet 1” from the “Sheets” tab to the dashboard. Right click on the title to make it disappear.In the “Objects” pane, drag and drop “Web Page” to the right of “Sheet 1” on the dashboard. Then click “OK” leaving the URL blank.This web page is a way to display images. The purpose is to display the images in the web page by hovering over a square. To do this, we will launch a local web server. In your terminal, go to the directory to be served and type cd <path where UIC_dataset folder is>, then launch a Python web server by typing python -m http.server --bind 127.0.0.1. Your server is now launched.In the “Dashboard” tab at the top > “Actions…” > “Add Action >” > “Go to URL…”. In “Run action on:” select “Hover”. In “URL”, enter the URL “http://127.0.0.1:8000/<Image file paths>” literally. In “URL Target”, select “Web Page Object”. Finally, click “OK”.Note: If you are working in Windows, you may need to change the “image file paths” by replacing “\” to “/”. To do this, in “Sheet 1”, right click at the bottom of the “Data” pane, then “Create Calculated Field…”. There type name “path” and enter the calculation formula as below. Then in “Dashboard” > “Actions” > double-click on “Hyperlink1”, change the URL with “http://127.0.0.1:8000/<path>”.And voilà! Your images will appear as you fly over them.So we can see that similar images have been grouped together. Some clusters can be seen, such as the “Residential” cluster at the top right or the clearly detached “SeaLake” cluster at the bottom.So we saw a simple and effective way to get a good idea of an image dataset. This idea will be very precious for you in your further analysis work.I hope you enjoyed this article and that it made you want to learn more about data analysis. Feel free to ask questions and comments.The code is available as a Jupyter notebook on my gitlab to which you can contribute if you wish.More articles will come, including one on cleaning up plain text emails. Stay tuned and see you soon 😉Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1330,Want Clusters? How Many Will You Have?,531,2018-11-05,https://towardsdatascience.com/want-clusters-how-many-will-you-have-8737f4ba9bf2?source=collection_archive---------21-----------------------,2018,"Data Science is the study of algorithms.I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.How many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.We sometimes end up using correlation or tree-based methods to find out the important features.Can we add some structure to it?This post is about some of the most common feature selection techniques one can use while working with data.Before we proceed, we need to answer this question. Why don’t we give all the features to the ML algorithm and let it decide which feature is important?So there are three reasons why we don’t:If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won’t generalize to the new samples. And thus we learn absolutely nothing.We want our models to be simple and explainable. We lose explainability when we have a lot of features.Most of the times, we will have many non-informative features. For Example, Name or ID variables. Poor-quality input will produce Poor-Quality output.Also, a large number of features make a model bulky, time-taking, and harder to implement in production.We select only useful features.Fortunately, Scikit-learn has made it pretty much easy for us to make the feature selection. There are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into three major bucketsSo enough of theory let us start with our five feature selection methods.We will try to do this using a dataset to understand it better.I am going to be using a football player dataset to find out what makes a good player great?Don’t worry if you don’t understand football terminologies. I will try to keep it at a minimum.Here is the Kaggle Kernel with the code to try out yourself.We have done some basic preprocessing such as removing Nulls and one hot encoding. And converting the problem to a classification problem using:Here we use High Overall as a proxy for a great player.Our dataset(X) looks like below and has 223 columns.This is a filter-based method.We check the absolute value of the Pearson’s correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion.This is another filter-based method.In this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.Let us create a small example of how we calculate the chi-squared statistic for a sample.So let’s say we have 75 Right-Forwards in our dataset and 25 Non-Right-Forwards. We observe that 40 of the Right-Forwards are good, and 35 are not good. Does this signify that the player being right forward affects the overall performance?We calculate the chi-squared value:To do this, we first find out the values we would expect to be falling in each bucket if there was indeed independence between the two categorical variables.This is simple. We multiply the row sum and the column sum for each cell and divide it by total observations.so Good and NotRightforward Bucket Expected value= 25(Row Sum)*60(Column Sum)/100(Total Observations)Why is this expected? Since there are 25% notRightforwards in the data, we would expect 25% of the 60 good players we observed in that cell. Thus 15 players.Then we could just use the below formula to sum over all the 4 cells:I won’t show it here, but the chi-squared statistic also works in a hand-wavy way with non-negative numerical and categorical features.We can get chi-squared features from our dataset as:This is a wrapper based method. As I said before, wrapper methods consider the selection of a set of features as a search problem.From sklearn Documentation:The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.As you would have guessed, we could use any estimator with the method. In this case, we use LogisticRegression , and the RFE observes the coef_ attribute of the LogisticRegression objectThis is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.For example, Lasso and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero.Here we use Lasso to select variables.This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.We can also use RandomForest to select features based on feature importance.We calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.We could also have used a LightGBM. Or an XGBoost object as long it has a feature_importances_ attribute.Why use one, when we can have all?The answer is sometimes it won’t be possible with a lot of data and time crunch.But whenever possible, why not do this?We check if we get a feature based on all the methods. In this case, as we can see Reactions and LongPassing are excellent attributes to have in a high rated player. And as expected Ballcontrol and Finishing occupy the top spot too.Feature engineering and feature selection are critical parts of any machine learning pipeline.We strive for accuracy in our models, and one cannot get to a good accuracy without revisiting these pieces again and again.In this article, I tried to explain some of the most used feature selection techniques as well as my workflow when it comes to feature selection.I also tried to provide some intuition into these methods, but you should probably try to see more into it and try to incorporate these methods into your work.Do read my post on feature engineering too if you are interested.If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
681,Convolutional Neural Networks from the ground up,5900,2018-06-16,https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1?source=collection_archive---------2-----------------------,2018,"I recently started reading about how I could work with Images in Python. When I came across OpenCV which allows import and manipulation of images in Python, I started to wonder if information could be extracted out of those images using Machine Learning and used in some way.We’ve all seen that we can search online on the basis of certain filters one of which is color. I got inspired to actually write the code that can extract colors out of images and filter the images based on those colors.In this article, I explain how I understood the basics of OpenCV, extracted colors from images using KMeans algorithm and filtered images from a collection of images based on RGB values of colors. The complete notebook is available at this repository. The sample_image.jpg was clicked by me and the other 5 images in the folder images were taken from Unsplash.We import the basic libraries including matplotlib.pyplot and numpy. To extract the count, we will use Counter from the collections library. To use OpenCV, we will use cv2. KMeans algorithm is part of the sklearn's cluster subpackage. To compare colors we first convert them to lab using rgb2lab and then calculate similarity using deltaE_cie76. Finally, to combine paths while reading files from a directory, we import os.To read any image, we use the method cv2.imread() and specify the complete path of the image which gets imported into the notebook as a Numpy array. We can then plot it using the pyplot’s method imshow().The shape of the array is (3456, 4608, 3). The first two values match the pixels of the image. Third value is set to 3 as each pixel is represented as a combination of three colors, Red, Blue and Green.The color of the image looks a bit off. This is because, by default, OpenCV reads image in the sequence Blue Green Red (BGR). Thus, to view the actual image we need to convert the rendering to Red Green Blue (RGB).The method cvtColor allows us to convert the image rendering to a different color space. To move from BGR color space to RGB, we use the method cv2.COLOR_BGR2RGB.In some situations, we might want to have black and white images. In such cases, we can express images as Gray. We now use the conversion space as cv2.COLOR_BGR2GRAY and show the output with the colormap as gray.We can also resize the image to a given dimension. We use the method resize provided by cv2. The first argument is the image we want to resize, and the second argument is the width and height defined within parentheses.Now let’s move to identifying the colors from an image and displaying the top colors as a pie chart.We’d first define a function that will convert RGB to hex so that we can use them as labels for our pie chart.On reading the color which is in RGB space, we return a string. {:02x} simply displays the hex value for the respective color.Next, we define a method that will help us get an image into Python in the RGB space.We supply the path of the image as the argument . First, we read the file using imread and then change its color space before returning it.We now define the complete code as a method that we can call to extract the top colors from the image and display them as a pie chart. I’ve named the method as get_colors and it takes 3 arguments:Let’s break down this method for better understanding.First, we resize the image to the size 600 x 400. It is not required to resize it to a smaller size but we do so to lessen the pixels which’ll reduce the time needed to extract the colors from the image. KMeans expects the input to be of two dimensions, so we use Numpy’s reshape function to reshape the image data.KMeans algorithm creates clusters based on the supplied count of clusters. In our case, it will form clusters of colors and these clusters will be our top colors. We then fit and predict on the same image to extract the prediction into the variable labels.We use Counter to get count of all labels. To find the colors, we use clf.cluster_centers_. The ordered_colors iterates over the keys present in count, and then divides each value by 255. We could have directly divided each value by 255 but that would have disrupted the order.Next, we get the hex and rgb colors. As we divided each color by 255 before, we now multiply it by 255 again while finding the colors. If show_chart is True, we plot a pie chart with each pie chart portion defined using count.values(), labels as hex_colors and colors as ordered_colors. We finally return the rgb_colors which we’ll use at a later stage.Voila!! We’re all set!!Let’s just call this method as get_colors(get_image(‘sample_image.jpg’), 8, True) and our pie chart appears with top 8 colors of the image.This opens the doors for many superior applications such as searching for colors in a Search Engine, or looking for a piece of clothing that has a certain color in it.We’ve just identified the majority 8 colors that exist in our image. Let’s try and implement a search mechanism that can filter images based on the color supplied by us.We’ll now dive into the code of filtering a set of five images based on the color we’d like. For our use case, we’ll supply the RGB values for the colors Green, Blue and Yellow and let our system filter the images.The images are in the folder images. We define COLORS as a dictionary of colors. Then, we read all images in that folder and save their values in the images array.We first show all the images in the folder using the below mentioned for loop.We split the area into subplots equal to the number of images. The method takes the arguments as number of rows = 1, number of columns = all images i.e. 5 in our case and the index.We now define a method match_image_by_color to filter all images that match the selected color.We first extract the image colors using our previously defined method get_colors in RGB format. We use the method rgb2lab to convert the selected color to a format we can compare. The for loop simply iterates over all the colors retrieved from the image.For each color, the loop changes it to lab, finds the delta (basically difference) between the selected color and the color in iteration and if the delta is less than the threshold, the image is selected as matching with the color. We need to calculate the delta and compare it to the threshold because for each color there are many shades and we cannot always exactly match the selected color with the colors in the image.By saying green, the user can mean light green, green or dark green. We need to scan through all possibilities.If we extract say 5 colors from an image, even if one color matches with the selected color, we select that image. The threshold basically defines how different can the colors of the image and selected color be.Let’s consider the case where we are trying to find images with color Green. If the threshold is too high, we might start seeing blue images in our search. Similarly, on the other hand, if the threshold is too low, then green might not even match images that have dark green in them. It’s all based on what is required in the situation at hand and we can modify the values accordingly. We need to carefully set the threshold value.We define a function show_selected_images that iterates over all images, calls the above function to filter them based on color and displays them on the screen using imshow.We will now simply call this method and let it plot the results.We call the method as follows. We will just replace the variable selected_color with COLORS['GREEN'] for Green, COLORS['BLUE'] for Blue, and COLORS['YELLOW'] for Yellow. We set the threshold value to be 60 and total colors to be extracted from image to be 5.In this article, we discussed the methodology to extract colors from an image using KMeans algorithm and then use this to search images based on colors.Hope you liked my work. Please feel free to share your thoughts and suggestions.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3286,How Many DVDs is my Big Data?,1,2019-07-26,https://towardsdatascience.com/how-many-dvds-is-my-big-data-5b3ccddba37b?source=collection_archive---------32-----------------------,2019,"You might expect this to be an easy question to answer. It turns out not to be. Exploring it reveals fundamental issues with how we collect and publish data. We take for granted the value of making data-based decisions and expect we can extend this practice to public policy — but that may not currently be possible.Deaths would seem to be the easiest possible statistic to measure. Almost all countries record mortality. The United States has a relatively complex multilevel structure of governments, but even so, each state maintains a Department of Vital Statistics that records deaths, issues death certificates, and forwards statistics to the Federal government. The Center for Disease Control’s National Center for Health Statistics publicizes death numbers. But its most recent data brief, Number 355 issued in January 2020, only covers deaths through 2018.Putting together a polished data set and results requires an average of 15 months. The completed data set has many dimensions useful for research: breakdown by age, cause of death, etc. This level of detail is unnecessary to answer basic questions, so the CDC has an early release program — but that requires submitting an application and institutional review, barriers that are inappropriate for answering basic questions, and still only makes available data 6 months after a year’s end. Since this is still insufficient, CDC offers State and National Provisional Counts. At the time of writing (May 17, 2020), the provisional data covers 2018 and up to March 2019 (14 months behind). And the historical archive covers up to 2015 (where is the data for 2016 and 2017)?Right now, due to the intense scrutiny on the COVID-19 epidemic, we can actually find some better and more detailed (non-COVID) death reporting from February through May as part of COVID reports). But comparing that data with the prior period in 2019 remains out of reach.Perhaps counting deaths in the United States is too difficult a problem. Let’s instead turn to a simpler problem — counting deaths in a single state. Since the state issues the death certificates, a State should, you might think, easily be able to answer how many deaths happened in a given timeframe, or at least how many death certificates it issued in that time — a sufficiently close proxy. I live in Texas so I consulted the Texas Department of Health and Human Services, whose Department of Vital Statistics maintains that data in easily accessible web records… for up to 2016.Resigning myself to ask for the data in the old-fashioned way, I emailed the State Department of Vital Statistics, receiving this reply:Good Afternoon, Mr. Rostcheck,Thank you for your request regarding Texas mortality data. At this time we are unable to process your request due to limited resources and COVID19 response. We can either put your request on a waitlist or you are welcome to check back in with us at a later time. Additionally, the most recent year of finalized data we can provide counts by month for is 2017. The most recent year of non-finalized data we can provide these counts for is 2018.Please let us know if you have any questions.Thank you,The Vital Events Data Management TeamTexas Department of State Health ServicesCenter for Health Statistics[Note that although they note that the most recent data that Texas’s own authoritative center for health statistics can provide is through 2018, the CDC’s State and National Provisional Counts has Texas’ data through March 2019. Note also that Texas is actually one of the most efficiently run states in the United States.]In industry, we are used to making data-based decisions. CEOs, COOs, and CFOs regularly track key metrics such as inventory, sales, and customer pipeline. At least for the USA, the above experience hints that basic metrics are not easily available, or at least not made public, for key government functions. If we were to seek key metrics about other civic functions, such as the numbers of drivers’ licenses or park attendees, would we find any better or more transparent data? And if we can’t find the data to answer extremely simple questions like “Did more people die two months ago than died for that same month in the prior year?” can we legitimately expect to be able to answer more complicated questions like “Is this public health intervention working?” Even in what we think of as highly transparent societies under extreme scrutiny, key metrics — such as the number of COVID-19 deaths in New York and Colorado — are revised up and down by significant amounts. We may simply lack the fundamental data infrastructure we need to actually make timely data-based decisions in public policy.What can we do about that? A few thoughts come to mind:Do you have other ideas for how to influence availability of basic operational metrics in government? If so, please comment below.Thanks to Aaron Baker and Shelby Williams for help in exploring this problem.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1965,Scaling Machine Learning from 0 to millions of users — part 1,961,2019-01-29,https://towardsdatascience.com/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849?source=collection_archive---------5-----------------------,2019,"An end-to-end example of how to build a system that can search objects semantically. By Hamel Husain & Ho-Hsiang WuThe power of modern search engines is undeniable: you can summon knowledge from the internet at a moment’s notice. Unfortunately, this superpower isn’t omnipresent. There are many situations where search is relegated to strict keyword search, or when the objects aren’t text, search may not be available. Furthermore, strict keyword search doesn’t allow the user to search semantically, which means information is not as discoverable.Today, we share a reproducible, minimally viable product that illustrates how you can enable semantic search for arbitrary objects! Concretely, we will show you how to create a system that searches python code semantically — but this approach can be generalized to other entities (such as pictures or sound clips).Why is semantic search so exciting? Consider the below example:The search query presented is “Ping REST api and return results”. However, the search returns reasonable results even though the code & comments found do not contain the words Ping, REST or api.This illustrates the power of semantic search: we can search content for its meaning in addition to keywords, and maximize the chances the user will find the information they are looking for. The implications of semantic search are profound — for example, such a procedure would allow developers to search for code in repositories even if they are not familiar with the syntax or fail to anticipate the right keywords. More importantly, you can generalize this approach to objects such as pictures, audio and other things that we haven’t thought of yet.If this is not exciting enough, here is a live demonstration of what you will be able to build by the end of this tutorial:Before diving into the technical details, it is useful to provide you with a high-level intuition of how we will accomplish semantic search. The central idea is to represent both text and the object we want to search (code) in a shared vector space, as illustrated below:The goal is to map code into the vector space of natural language, such that (text, code) pairs that describe the same concept are close neighbors, whereas unrelated (text, code) pairs are further apart, measured by cosine similarity.There are many ways to accomplish this goal, however, we will demonstrate the approach of taking a pre-trained model that extracts features from code and fine-tuning this model to project latent code features into a vector space of natural language. One warning: We use the term vector and embedding interchangeably throughout this tutorial.The techniques presented in this blog post are old and have been significantly refined in a subsequent project called CodeSearchNet, with an associated paper.I recommend looking at the aforementioned project for a more modern approach to this topic, as in retrospect this blog post is somewhat of an ugly hack.We recommend familiarity with the following items prior to reading this tutorial:This tutorial will be broken into 5 concrete steps. These steps are illustrated below and will be a useful reference as you progress throughout the tutorial. After completing the tutorial, it will be useful to revisit this diagram to reinforce how all the steps fit together.Each step 1–5 corresponds to a Jupyter notebook here. We will explore each step in more detail below.Part 1 notebookThe folks at Google collect and store data from open-source GitHub repositories on BigQuery. This is a great open dataset for all kinds of interesting data-science projects, including this one! When you sign up for a Google Cloud account, they give you $300 which is more than enough to query the data for this exercise. Getting this data is super convenient, as you can use SQL queries to select what type of files you are looking for as well as other meta-data about repos such as commits, stars, etc.The steps to acquire this data are outlined in this notebook. Luckily, some awesome people on the Kubeflow team at Google have gone through these steps and have graciously hosted the data for this exercise, which is also described in this notebook.After collecting this data, we need to parse these files into (code, docstring) pairs. For this tutorial, one unit of code will be either a top-level function or a method. We want to gather these pairs as training data for a model that will summarize code (more on that later). We also want to strip the code of all comments and only retain the code. This might seem like a daunting task, however, there is an amazing library called ast in Python’s standard library that can be used to extract functions, methods and, docstrings. We can remove comments from code by converting code into an AST and then back from that representation to code, using the Astor package. Understanding of ASTs or how these tools work is not required for this tutorial, but are very interesting topics!To prepare this data for modeling, we separate the data into train, validation and test sets. We also maintain files (which we name “lineage”) to keep track of the original source of each (code, docstring) pair. Finally, we apply the same transforms to code that does not contain a docstring and save that separately, as we will want the ability to search this code as well!Part 2 notebookConceptually, building a sequence-to-sequence model to summarize code is identical to the GitHub issue summarizer we presented previously — instead of issue bodies we use python code, and instead of issue titles, we use docstrings.However, unlike GitHub issue text, code is not natural language. To fully exploit the information within code, we could introduce domain-specific optimizations like tree-based LSTMs and syntax-aware tokenization. For this tutorial, we are going to keep things simple and treat code like natural language (and still get reasonable results).Building a function summarizer is a very cool project on its own, but we aren’t going to spend too much time focusing on this (but we encourage you to do so!). The entire end-to-end training procedure for this model is described in this notebook. We do not discuss the pre-processing or architecture for this model as it is identical to the issue summarizer.Our motivation for training this model is not to use it for the task of summarizing code, but rather as a general purpose feature extractor for code. Technically speaking, this step is optional as we are only going through these steps to initialize the model weights for a related downstream task. In a later step, we will extract the encoder from this model and fine tune it for another task. Below is a screenshot of some example outputs of this model:We can see that while the results aren’t perfect, there is strong evidence that the model has learned to extract some semantic meaning from code, which is our main goal for this task. We can evaluate these models quantitatively using the BLEU metric, which is also discussed in this notebook.It should be noted that training a seq2seq model to summarize code is not the only technique you can use to build a feature extractor for code. For example, you could also train a GAN and use the discriminator as a feature extractor. However, these other approaches are outside the scope of this tutorial.Part 3 notebookNow that we have built a mechanism for representing code as a vector, we need a similar mechanism for encoding natural language phrases like those found in docstrings and search queries.There are a plethora of general purpose pre-trained models that will generate high-quality embeddings of phrases (also called sentence embeddings). This article provides a great overview of the landscape. For example, Google’s universal sentence encoder works very well for many use cases and is available on Tensorflow Hub.Despite the convenience of these pre-trained models, it can be advantageous to train a model that captures the domain-specific vocabulary and semantics of docstrings. There are many techniques one can use to create sentence embeddings. These range from simple approaches, like averaging word vectors to more sophisticated techniques like those used in the construction of the universal sentence encoder.For this tutorial, we will leverage a neural language model using an AWD LSTM to generate embeddings for sentences. I know that might sound intimidating, but the wonderful fast.ai library provides abstractions that allow you to leverage this technology without worrying about too many details. Below is a snippet of code that we use to build this model. For more context on how this code works, see this notebook.It is important to carefully consider the corpus you use for training when building a language model. Ideally, you want to use a corpus that is of a similar domain to your downstream problem so you can adequately capture the relevant semantics and vocabulary. For example, a great corpus for this problem would be stack overflow data, since that is a forum that contains an extremely rich discussion of code. However, in order to keep this tutorial simple, we re-use the set of docstrings as our corpus. This is sub-optimal as discussions on stack overflow often contain richer semantic information than what is in a one-line docstring. We leave it as an exercise for the reader to examine the impact on the final outcome by using an alternate corpus.After we train the language model, our next task is to use this model to generate an embedding for each sentence. A common way of doing this is to summarize the hidden states of the language model, such as the concat pooling approach found in this paper. However, to keep things simple we will simply average across all of the hidden states. We can extract the average across hidden states from a fast.ai language model with this line of code:A good way to evaluate sentence embeddings is to measure the efficacy of these embeddings on downstream tasks like sentiment analysis, textual similarity etc. You can often use general-purpose benchmarks such as the examples outlined here to measure the quality of your embeddings. However, these generalized benchmarks may not be appropriate for this problem since our data is very domain specific. Unfortunately, we have not designed a set of downstream tasks for this domain that we can open source yet. In the absence of such downstream tasks, we can at least sanity check that these embeddings contain semantic information by examining the similarity between phrases we know should be similar. The below screenshot illustrates some examples where we search the vectorized docstrings for similarity against user-supplied phrases (taken from this notebook):It should be noted that this is only a sanity check — a more rigorous approach is to measure the impact of these embeddings on a variety of downstream tasks and use that to form a more objective opinion about the quality of your embeddings. More discussion about this topic can be found in this notebook.Part 4 notebookAt this point, it might be useful to revisit the diagram introduced at the beginning of this tutorial to review where you are. In that diagram you will find this representation of part 4:Most of the pieces for this step come from prior steps in this tutorial. In this step, we will fine-tune the seq2seq model from part 2 to predict docstring embeddings instead of docstrings. Below is code that we use to extract the encoder from the seq2seq model and add dense layers for fine-tuning:After we train the frozen version of this model, we unfreeze all layers and train the model for several epochs. This helps fine-tune the model a little more towards this task. You can see the full training procedure in this notebook.Finally, we want to vectorize the code so we can build a search index. For evaluation purposes, we will also vectorize code that does not contain a docstring in order to see how well this procedure generalizes to data we have not seen yet. Below is a code snippet (taken from this notebook) that accomplishes this task. Note that we use the ktext library to apply the same pre-processing steps that we learned on the training set to this data.After collecting the vectorized code, we are ready to proceed to the last and final step!Part 5 notebookIn this step, we will build a search index using the artifacts we created in prior steps, which is illustrated below:In part 4, we vectorized all the code that did not contain any docstrings. The next step is to place these vectors into a search index where nearest neighbors can be quickly retrieved. A good python library for fast nearest neighbors lookups is nmslib. To enjoy fast lookups using nmslib you must precompute the search index like so:Now that you have built your search index of code vectors, you need a way to turn a string (query) into a vector. To do this you will use the language model from Part 3. To make this process easy we provided a helper class in lang_model_utils.py called Query2Emb, which is demonstrated in this notebook.Finally, once we are able to turn strings into query vectors, we can retrieve the nearest neighbors for that vector like so:The search index will return two items (1) a list of indexes which are integer positions of the nearest neighbors in the dataset (2) distances of these neighbors from your query vector (in this case we defined our index to use cosine distance). Once you have this information, it is straightforward to build semantic search. An example of how you can do this is outlined in the below code:Finally, this notebook shows you how to use the search_engine object above to create an interactive demo that looks like this:Congratulations! You have just learned how to create semantic search. I hope it was worth the journey.Even though this tutorial describes how to create semantic search for code, you can use similar techniques to search video, audio, and other objects. Instead of using a model that extracts features from code (part 2), you need to train or find a pre-trained model that extracts features from your object of choice. The only prerequisite is that you need a sufficiently large dataset with natural language annotations (such as transcripts for audio, or captions for photos).We believe you can use the ideas you learned in this tutorial to create your own search and would love to hear from you to see what you create (see the getting in touch section below).We hope you enjoyed this blog post. Please feel free to get in touch with us:Mockup search UI was designed by Justin Palmer (you can see some of his other work here). Also thanks to the following people for their review and input: Ike Okonkwo, David Shinn, Kam Leung.Any ideas or opinions presented in this article are our own. Any ideas or techniques presented do not necessarily foreshadow future products of GitHub. The purpose of this blog is for educational purposes only.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2437,Fake News Classification via Anomaly Detection,51,2019-04-30,https://towardsdatascience.com/fake-news-classification-via-anomaly-detection-765c4c71d539?source=collection_archive---------15-----------------------,2019,"Over the past few articles on GradientCrescent, we’ve spent a significant period of time exploring the field of online learning, a highly reactive family of reinforcement learning algorithms behind many of the latest achievements in general AI. Online learning belongs to the sample-based learning class of approaches, reliant allow for the determination of state values simply through repeated observations, eliminating the need for transition dynamics. Unlike their offline counterparts, online learning approaches allow for the incremental updates of the values of states and actions during an environmental episode, allowing for constant, incremental performance improvements to be observed.Beyond Temporal Difference learning (TD), we’ve discussed the theory and practical implementations of Q-learning, an evolution of TD designed to allow for incremental estimations and improvement of state-action values. Q-learning has been made famous as becoming the backbone of reinforcement learning approaches to simulated game environments, such as those observed in OpenAI’s gyms. As we’ve already covered theoretical aspects of Q-learning in past articles, they will not be repeated here.In our previous implementation of OpenAI’s Miss Pacman gym environment, we relied on a set of observation instances (states) of individual game frames as the inputs for our training process. However, this method is partially flawed, as it fails to take into account many of the idiosyncrasies of the Atari game environment, including:Together, these problems can serve to drastically reduce agent performance, as some instances of data essentially become out-of-domain, or completely irrelevant to the actual game environment. Furthermore, these problems would only become compounded with more complex game environments and real-world applications, such as in autonomous driving. This was observed in our previous implementation as a high level of variance and flat-lining in performance during training.To overcome these problems, we can utilize a couple of techniques first introduced by the Deepmind team in 2015.Let’s implement examine the effects of these techniques on the more complex Atari Space Invader’s environment.Our Google Colaboratory implementation is written in Python utilizing Tensorflow Core, and can be found on the GradientCrescent Github. We’ve converted our code to be TF2 compliant, using the new compat package. To start with, let’s briefly go over the actions required for our Q-learning implementation.For reference, let’s first demonstrate the results using the vanilla data input approach, which is essentially the same as that observed in our previous implementation for Miss Pacman. After training our agent for 800 episodes, we observe the following reward distribution.Note how the variation in performance exhibits high variation, with a very limited amount of improvement observed after 650 episodes.Likewise, the performance of our agent is less than stellar, with almost no evasion behavior being detected. If you look closely, you’ll notice the blinking in both the outgoing and incoming laser trails — this is an intentional part of the game environment, and results in certain frames having no projectiles in them at all, or only one set of projectiles visible. This means that elements of our input data are highly misleading, and negatively affect agent performance.Let’s go over our improved implementation.We start by importing all of the necessary packages, including the OpenAI gym environments and Tensorflow core.Next, we define a preprocessing function to crop the images from our gym environment and convert them into one-dimensional tensors. We’ve seen this before in our Pong automation implementation.Next, let’s initialize the gym environment, and inspect a few screens of gameplay, and also understand the 9 actions available within the gamespace. Naturally, this information is not available to our agent.You should observe the following:We can take this chance to compare our original and preprocessed input images:Next, we introduce input stacking and input composition into our preprocessing pipeline. Upon a new episode, we start off by taking two of our input frames, and returning an element-wise maximum summation maxframe of the two (note that technically this is not necessary, as the two frames are the same, but serves for good practice). The stacked frames are stored in a deque, which automatically removes older entries as new entries are introduced. Initially, we copy the preprocessed maxframe to fill out our deque. As the episode progresses, we create new maxframes by taking the new frame, element-wise maximum summing it with the most recent entry in our deque, and then appending the new maxframe to our deque. We then stack the frames at the very end of the process.Next, let’s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input images, flattens and feeds them to a fully-connected layer, and outputs the probabilities of taking each action in the game space. As previously mentioned, there’s no activation layer here, as the presence of one would result in a binary output distribution.Let’s also take this chance to define our hyperparameters for our model and training process. Note that the X_shape is now (None, 88, 80, 4), on account of our stacked frames.Recall, that Q-learning requires us to select actions with the highest action values. To ensure that we still visit every single possible state-action combination, we’ll have our agent follow an epsilon-greedy policy, with an exploration rate of 5%. We’ll set this exploration rate to decay with time, as we eventually assume all combinations have already been explored — any exploration after that point would simply result in the forced selection of sub-optimal actions.Recall from the equations above, that the update function for Q-learning requires the following:To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we’ll draw data in minibatches during training. Let’s go ahead and create our buffer and a simple sampling function:Next, let’s copy the weight parameters of our original network into a target network. This dual-network approach allows us to generate data during the training process using an existing policy while still optimizing our parameters for the next policy iteration.Finally, we’ll also define our loss. This is simply the squared difference of our target action (with the highest action value) and our predicted action. We’ll use an ADAM optimizer to minimize our loss during training.With all of our code defined, let’s run our network and go over the training process. We’ve defined most of this in the initial summary, but let’s recall for posterity.Once training is complete, we can plot the reward distribution against incremental episodes. The first 800 episodes are shown below:Notice how the core variation in reward distribution has decreased significantly, allowing for a much more consistent inter-episode distribution to be observed, and increases in performance to become more statistically significant. A visible increase in performance can be observed from 550 episodes onwards, a full 100 episodes earlier than that of the vanilla data approach, validating our hypothesis.To evaluate our results within the confinement of the Colaboratory environment, we can record an entire episode and display it within a virtual display using a wrapped based on the IPython library:We then run a new session of our environment using our model and record it.Let’s inspect a few rounds of gameplay.Our agent has learned to behave both defensively and offensively, utilizing cover and evasion effectively. The drastic difference in behavior between the two episodes can be attributed to how Q-learning works —actions selected earlier on gain a Q-value, which tend to be favoured with an epsilon-greedy policy. With further training, we would expect these two behaviours to converge.That wraps up this introduction to optimizing Q-learning. In our next article, we’ll take all we’ve learned and move on from the world of Atari to tackling one of the most well known FPS games in the world.We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.ReferencesSutton et. al, Reinforcement LearningWhite et. al, Fundamentals of Reinforcement Learning, University of AlbertaSilva et. al, Reinforcement Learning, UCLRavichandiran et. al, Hands-On Reinforcement Learning with PythonTakeshi et. al, GithubWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1047,Multivolume rendering in Jupyter with ,725,2018-09-14,https://towardsdatascience.com/multivolume-rendering-in-jupyter-with-ipyvolume-cross-language-3d-visualization-64389047634a?source=collection_archive---------5-----------------------,2018,"Python’s superior flexibility and ease of use are what make it one of the most popular programming language, especially for Data Scientists. A big part of that is how simple it is to work with large datasets.Every technology company today is building up a data strategy. They’ve all realised that having the right data: insightful, clean, and as much of it as possible, gives them a key competitive advantage. Data, if used effectively, can offer deep, beneath the surface insights that can’t be discovered anywhere else.Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python!A CSV file is the most common way to store your data. You’ll find that most of the data coming from Kaggle competitions is stored in this way. We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists.Check out the code below. When we run csv.reader() all of our CSV data becomes accessible. The csvreader.next() function reads a single line from the CSV; every time you call it, it moves to the next line. We can also loop through every row of the csv using a for-loop as with for row in csvreader . Make sure that you have the same number of columns in each row, otherwise, you’ll likely end up running into some errors when working with your list of lists.Writing to CSV in Python is just as easy. Set up your field names in a single list, and your data in a list of lists. This time we’ll create a writer() object and use it to write our data to file very similarly to how we did the reading.Of course, installing the wonderful Pandas library will make working with your data far easier once you’ve read it into a variable. Reading from CSV is a single line as is writing it back to file!We can even use Pandas to convert from CSV to a list of dictionaries with a quick one-liner. Once you have the data formatted as a list of dictionaries, we’ll use the dicttoxml library to convert it to XML format. We’ll also save it to file as a JSON!JSON provides a clean and easily readable format because it maintains a dictionary-style structure. Just like CSV, Python has a built-in module for JSON that makes reading and writing super easy! When we read in the CSV, it will become a dictionary. We then write that dictionary to file.And as we saw before, once we have our data you can easily convert to CSV via pandas or use the built-in Python CSV module. When converting to XML, the dicttoxml library is always our friend.XML is a bit of a different beast from CSV and JSON. Generally, CSV and JSON are widely used due to their simplicity. They’re both easy and fast to read, write, and interpret as a human. There’s no extra work involved and parsing a JSON or CSV is very lightweight.XML on the other hand tends to be a bit heavier. You’re sending more data, which means you need more bandwidth, more storage space, and more run time. But XML does come with a few extra features over JSON and CSV: you can use namespaces to build and share standard structures, better representation for inheritance, and an industry standardised way of representing your data with XML schema, DTD, etc.To read in the XML data, we’ll use Python’s built-in XML module with sub-module ElementTree. From there, we can convert the ElementTree object to a dictionary using the xmltodictlibrary. Once we have a dictionary, we can convert to CSV, JSON, or Pandas Dataframe like we saw above!Follow me on twitter where I post all about the latest and greatest AI, Technology, and Science! Connect with me on LinkedIn too!Want to learn more about coding in Python? The Python Crash Course book is the best resource out there for learning how to code in Python!And just a heads up, I support this blog with Amazon affiliate links to great books, because sharing great books helps everyone! As an Amazon Associate I earn from qualifying purchases.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
4791,Finding the Complexity of Words,51,2019-11-26,https://towardsdatascience.com/finding-the-complexity-of-words-6c487cea562?source=collection_archive---------34-----------------------,2019,"This is an informal take on the technological choice that we are often faced with when designing Chatbots, i.e. should we build a Chatbot or a Natural Language Search (NLS), or a may be a mix of both? The primary motivation of both is to make enterprise data and applications (more) accessible to every company employee — to foster knowledge sharing and collaboration. With this objective, we explore two integration architectures:Much has been said and written about Chatbots. However, most of this discussion is focused around Consumer facing bots — the multi-million dollar bot that will radically transform your company’s image and allow you to save a few million on the sidelines. It goes without saying that building such a singular bot also requires investment in the order of a few hundred thousand dollars. Still, if the math adds up; a gain is a gain — no questions asked!In this post, we will focus on another type of bots which is slowly rearing its head — the ‘internal’ employee facing bot within an enterprise.If you work for any company with say 10000+ employees, you would have seen a few bots mushrooming here and there in your company allowing (or at least promising to allow) you easy access to Finance, HR, IT Services Desk, etc. systems. We refer to these bots as the employee-facing bots. This trend is also highlighted by Gartner in their research report: “The deployment of Chatbots is no longer just customer-facing, but also increasingly facing employees within an enterprise”These employee facing bots vary from the above consumer facing ones in at least the following 4 characteristics:The problem of this scattered landscape is that while it works at the experimentation level, operationalizing and maintaining it is rather impossible. The diverse technology stack further implies that you will not benefit from any volume discounts for the commercial products, as an organization. As such, this approach is not scalable at all; and the bot death rate will soon start competing with the number of new bots coming up in your organization.For now, let us focus on an interesting technological choice that we are often faced with when scoping/designing such bots, i. e.,should we build a Chatbot or a Natural Language Search (NLS) tool, or a may be a mix of both?Let us try to position the two technologies:Chatbots: are conversational.In its very basic form, given a question posed by a human (we are a bit far from bot-to-bot communication) in natural language, the bot replies with its corresponding answer, preferably also in natural language.(There are other types of bots as well, which are more action oriented, e.g. to raise a service request or purchase order. In this article, we focus on bots and search tools whose primary goal is information retrieval).Natural Language Search (NLS): The best way to introduce Search is of course to think of Google. We google every day, however today we are basically leveraging ‘keyword-based search’. Reflecting a bit will remind you of how we split our query into the key phrases/keywords before entering them in Google.With NLS, we are talking about a world where we will no longer need to use this, but input a query in the form of a sentence directly into Google, just as you would do while talking to a friend today. Just in case you haven’t tried it, Google does support NLS today. It basically uses something referred to “knowledge graphs”, which is a huge manually curated knowledge base capturing relationships between real-work entities and events; and as such reaches its limits soon. For instance, try the following sequence of queries:As can be noticed from the screenshots, the 2nd follow-up query reverts back to the usual webpages-based list response, although the answer “smoke and poisonous air” of the follow-up question is clearly there in its KnowledgeBase (KB). (Disclaimer: Google’s inner working changes every day, but I hope that the difference between keyword-based search and NLS is clear from the narrative.) So far, we have only explored the query part of NLS.For the answering part, the answer should also be a sentence in natural language, and not the paragraph excerpts that Google displays currently. Generating such answers/sentences is covered by a sub-domain of NLP known as Natural Language Generation (NLG) .In an ideal world, without technological limitations, there would be no need to have this discussion; i.e. the need to differentiate between Chatbots and NLS. Given a user query in natural language, the bot is able to do the following 4 things:The problem is that there are multiple technological limitations in enabling the above workflow today. The below illustration tries to position the different technologies.Current Chatbot engines are in the 2nd column, where we need to provide a set of questions, question variations, and their corresponding answers — to build a bot.The questions can be grouped into ‘intents’. Question variations, referred to as ‘utterances’ in bot terminology, refer to sample variations in which the same question can be posed by end-users. The idea is to provide 5–10 such utterances (for each question) as input, based on which the bot will hopefully be able understand 50 different variations of the question. Most bot engines use statistical NLP techniques, e.g. tf-idf, n-grams, Bag-of-Words, text classification (SVM) — to map the end-user queries to an intent.This works to a reasonable extent; however, the main challenge is with respect to hardcoded answers, which need to be provided in advance, and which in turn limit the scope of questions that the bot can answer.A more scalable approach would be to use NLS here. With such an integration, it would be sufficient to just point the bot to a KB (document repository), and the bot would be able to retrieve the answer to the user query at run-time by searching the KB documents — the 3rd column.The technology limitation here is that a search over documents is likely to be much less accurate than mapping a user query to an intent. In addition, most current search techniques work on relevance measures (similarity scores). As such, the search output is a list of relevant paragraphs and not the ‘exact’ answer in natural language — NLG is not there yet.To summarize, neither Chatbots nor NLS is at a level of maturity today where once can replace the other. The two are complementary and in the sequel, we will highlight a coupe of interesting approaches to integrate them.A standalone Chatbot is often not sufficient. A Chatbot extended with Search capabilities will make sense in most cases.Plan for a 3-tier Chatbot architecture as illustrated below:1. A Chatbot with its pre-defined Q&A set remains the entry point — think of it as the 1st line of defense.2. If the bot encounters a user query which cannot be mapped to one of its pre-configured intents, it performs a NLS over its KB. This is the 2nd line of defense.3. If the user is not satisfied even with search results, plan for a final handover to a live agent.The 3-tier strategy is very robust and we have found it to be very useful in Customer Care, Help Desk, etc. type of scenarios where the scope of user queries can be quite generic. It makes sense to retain the Chatbot as the entry point, as at least for those cases where the user query can be matched to an existing intent; the answer will be precise and to the point — improving user satisfaction. At the same time, we cannot keep adding Q&As to the Chatbots Q&A base endlessly, as then the intent recognition accuracy will suffer. Hence, a tiered architecture as the one above is recommended.We will provide more details of our NLS stack in a subsequent blog post. For now, suffice it to say that we are using a customized solution integrating ElasticSearch with DRQA (presented at Berlin Buzzwords, 2019, link), which is neural network architecture to answer open domain questions by Facebook Research.In this scenario, the primary goal of the Chatbot is to ‘Search’. Integrating a Chatbot with the search allows the user to search in a more conversational manner.The users can interact with the bot to:Conversational Search is increasingly becoming popular as a means to query SQL databases, e.g. the Power BI — NLP Search, Salesforce Photon.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
318,What Getting A Job In Data Science Might Look Like,3300,2018-03-27,https://towardsdatascience.com/what-getting-a-job-in-data-science-might-look-like-f94ddb788a5e?source=collection_archive---------0-----------------------,2018,"The data is increasing with every single click on the internet. In order to make sense of this huge data and use it for company’s benefits etc, we need help from different Data Science techniques.Every single day people buy and sell things online, with a single mouse click, but in order to keep the customers engaged with the website or to improve customer’s experience, companies use Data science/Machine Learning, i.e. on amazon website, when you are looking for a product, you see number recommendations. These recommendations generate through Machine Learning algorithms. It learns from user’s past activities and purchases. The companies store the data of every click customer make, every reviews customer read, every story customer share on social media etc, and use this data to learn about their customer or create a platform to help new customers.When you are shopping online, do you ever feel like, why they have made this thing in certain way or why is this stuff showing here? or thought, how does this thing know what am I looking for? There is only one answer to all these questions and that is- Data Science. E-commerce is one of the biggest consumer of Data Science/Machine Learning techniques and those who does not implement these techniques obviously are on fall.In this post, we will discuss about 5 main projects which, an E-commerce company should do, in order to enhance the customer experience as well as their revenue or business.Do you remember seeing recommendations on Amazon, Netflix or any eCommerce website? In the past few years recommendation system has taken over the internet based businesses by adding values to many businesses.IntroductionBefore understanding the benefits of recommendation systems in eCommerce, let’s clear the basics of recommendation system.Wikipedia Definition,A recommendation system is a subclass of information filtering system that seeks to predict the “rating” or “preference” a user would give to an item.The recommendation system is more than what above definition describes. It is used to filter choices for particular user on the basis of their past searches or other customer’s search or purchase data. It gives users a personalized view on the eCommerce website and help them to select relevant products. E.g. - While looking for a new phone on Amazon website, there is a possibility that you might want to buy a phone cover too. Amazon will decide this possibility by analyzing previous purchases or searches data of their customers.Popular Recommendation TechniquesThere are a number of ways to setup a recommendation system. Each of these techniques filter or provide recommendation in different manner. There are three main and known techniques as below-In the Collaborative Filtering, recommendations will be given on the basis of collected data about user’s activities on the website and by finding similarity between their activities with other user’s. It is the most popular technique among eCommerce companies as this particular technique does not need to know about the item before recommending it to the customer. It just try to find the similarities between different user’s interests.Unlike Collaborative filtering, Content Based Filtering provides recommendations on the basis of user’s profile and the item description. This technique can filter out products for users on the basis of what they liked in the past.The Hybrid Recommendation System is the combination of Collaborative and Content Based Filtering. Hybrid technique can be used in many different ways. We can make predictions separately using Collaborative and Content Based Filtering, later combine their results or make predictions using one of technique use its results as the input for another technique. One of the best example of Hybrid is Netflix.As now, we have a clear picture about what are Recommendation Systems, we will further discuss how do they add values to businesses.Importance of recommendations in eCommerce siteThere are a number of eCommerce websites and some of them are hard to differentiate as they sell similar kind of products. Here eCommerce businesses will need to think how they can keep their customers engaged with the website/product. I bet most of you must be thinking why are we talking about this here in Recommendation system.Imagine, you are shopping online for clothes on a e-commerce website1. The website1 doesn’t have any recommendation system implemented and for that reason as a user you have to go through a lot of different products. This might put customer off from website1, as it is very time consuming to shop on website1. On the other hand, their competitor website2 has recommendation system, resultant website2 will become more engaging than website1. Every time user click on a product, he or she will see similar or related products as recommendation on the website.It has been observed that the more engaging a website is, the more people will shop there. This will eventually increase the revenue of the eCommerce company.Many of you might have heard of the term “Valuable Customer”. What does it mean? What is it that make a customer valuable?IntroductionWikipedia DefinitionCustomer lifetime value is a prediction of the net profit attributed to the entire future relationship with a customer.The definition clearly states that Customer lifetime value modelling is, calculating how much a customer can bring to the revenue of a company during his/her lifetime. Moreover, it is a calculated figure which is predicted by the customer’s purchase and interaction history with the eCommerce website(or any other businesses)Before we try to understand why is it important for a business to know a customer’s value, let’s see how it can be calculated.Calculate Customer Lifetime ValueThere are a number of articles which describe the steps of calculating customer lifetime value. In order to keep it simple here we will discuss the formula which was used in the optimizesmart article.In the article states the basic formula to calculate customer lifetime value i.e(Average Order Value) x (Number of Repeat Orders) x (Average Customer life span)Average Order Value- Average value of all previous ordersNumber of Repeat Sales- Number of times, the orders were placedAverage Customer life Span- How long a person remains your customerImportance of Customer Lifetime Value in eCommerce siteThe customer lifetime value is a predicted amount which customer will bring into the company. But how much a single customer can bring in and why do we care about this?Lets say a company has 2k regular customers, by calculating future cash flow for all these customers, the company can predict the future revenue. Why do companies want to know their future revenue? The companies decide their strategies for future work e.g. how much they can take up or how much extra work they need to do etc, on the basis of their predicted future revenue. Not just this, but the companies can also decide on which customer to focus on. Say, customer ‘A’ would bring in 5k in revenue in the next ten years, whereas customer ‘B’, who will only bring 1K. Looking at the numbers, the companies will decide marketing strategy and will try to retain the incoming cash flow from the customer ‘A’ .Moreover, CLV helps eCommerce businesses in many ways -It is one of the essential metric which needs to be taken into account in any eCommerce business. It helps businesses in deciding their spends and know about their loyal customers.Churn Model is one of the project, which every eCommerce business should consider implementing in order to add the values to their businesses. As Churn model is related to customer retention, we need to first understand what is customer retention?Customer RetentionWikipedia definition,Customer retention refers to the ability of a company or product to retain its customers over some specified period.Customer retention is an important aspect for business but why? Once a customer go to a eCommerce website and order something there is a possibility that he/she would come back and buy more things too(only if they are happy). Customer retention helps in generating higher customer lifetime value. It is good to have new customers but existing customers bring more revenue than the new ones.There are a number of benefits of having loyal customers -As now we know how does customer retention benefits the businesses, we will now try understand how can we achieve customer retention.There are many ways to achieve customer retention but the most commonly used model is- Churn Model.Churn ModelChurn Model helps identifying customers who are most likely to switch to different eCommerce website. Once identified the companies can take actions in order to keep its existing customers. Now the question is, how does Churn model identify these customers? The model can be used to calculate the churn rate and depending on the nature of business, different metrics can be used. Few common metrics are -Importance of Churn Model in eCommerceThe Churn Model benefits businesses in many ways. Few advantages of implementing Churn Model in eCommerce are -You can get more information on Churn Model here.Most of the eCommerce businesses focus on acquiring more customers and generating more revenue. In order to achieve their targets, the companies want their site to be efficient. The efficiency won’t be able to save the business if the companies will fail to provide the security.IntroductionAccording to Wikipedia article,Fraud is a billion-dollar business and it is increasing every year.The PwC global economic crime survey of 2016 suggests that more than one in three (36%) of organizations experienced economic crime.Seeing that the Fraud risk is so high, the another project that online businesses should consider implementing is, Online Fraud Detection. Living in a digital world where millions of transactions happen with every single click, it seems easy to get mugged online.There are a number of ways a fraud can happen online -The list of online fraud is huge and fraudsters are getting smarter day by day. So, in order to have a successful eCommerce business, the companies will need to consider implementing the security measurements. For example, ordering a product online and not receiving the product which was shown online. The customer who ordered the product will not use the website again and probably will provide bad reviews. This can eventually put new users off and can also affect the business revenue.Now the question is how can these companies detect the frauds? With the help of Data Science and Machine Learning Techniques, these fraudsters can be found easily. In order to use Data Science techniques, the companies will have to come up with a list of any probable frauds. Some examples of suspicious behaviors indicating potential fraud are -The above suspicious behaviors can be detected using DS/ML. Some of the common techniques that are used -Importance of Fraud Detection in eCommerceAny company who cares about their customer’s security and business credibility will definitely consider having Fraud Detection System within their company. The Fraud Detection System can help companies in various ways -We saw how online companies and their customer can suffer because of Online Fraud and how this Data Science/Machine Learning can help.Many companies use content marketing in order to attract customers but in order to retain loyal customers, it is important to provide best service possible. What does improved customer service mean here? and How can it be achieved? Also, how does data science helps in improving customer service?Businesses are running customer services since a long time. The traditional approach of customer service is to contact customers through emails, posts and telephones and ask them to provide feedback on company’s products and their services. These days companies especially online businesses, have ratings and reviews section on their websites for their products. But, it is not easy to read each and every given review online manually. Not just this, but sometimes it becomes difficult to make sense of those reviews also, e.g the reviews containing incorrect spellings or shorthand words etc. This is where Data Science comes into picture.Using Data Science techniques e.g NLP(Natural Language Processing) the ratings and reviews from the website, can be extracted. This technique helps to retrieve user reviews and understand why bad reviews were given. For example- WordClouds are a popular way of displaying how important words are in a collection of texts and N-grams helps looking for words association. These techniques and others help Data Scientists making sense of reviews.Once the reviews are extracted, Data Scientists can further segregate them and do Sentiment Analysis. With this information, eCommerce can efficiently maximize user satisfaction by prioritizing product updates that will have the greatest positive impact.Through out this post we discussed different projects which eCommerce companies definitely should consider implementing. These projects can add values to their business with customer retention, good reviews, increased brand value, an improved customer service and a good recommendations for the customer will give customer a better experience but will also help companies to sell more products. There are a number of other projects too but these 5 are essentials for any eCommerce business.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2154,Understand TextRank for Keyword Extraction by Python,718,2019-02-18,https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0?source=collection_archive---------1-----------------------,2019,"Twitter is an online social network with over 330 million active monthly users as of February 2018. Users on twitter create short messages called tweets to be shared with other twitter users who interact by retweeting and responding. Twitter employs a message size restriction of 280 characters or less which forces the users to stay focused on the message they wish to disseminate. This very characteristic makes messages on twitter very good candidates for the Machine Learning (ML) task of sentiment analysis. Sentiment Analysis falls under Natural Language Processing (NLP) which is a branch of ML that deals with how computers process and analyze human language.The training data was obtained from Sentiment140 and is made up of about 1.6 million random tweets with corresponding binary labels. 0 for Negative sentiment and 1 for Positive sentiment. In this blog post, we’ll use a Naive Bayes Classifier to learn the correct labels from this training set and do a binary classification. In a nut shell, the Naive Bayes theorem calculates the probability of a certain event happening based on the joint probabilistic distributions of certain other events. I downloaded the test dataset using twitter’s API and will be use to test the model’s real world performance. Full documentation and terms of the API are available at developer.twitter.com/en/docs.In the first section of this blog post we’ll go through standard preprocessing steps performed on text data for a sentiment analysis ML task. The second section is covered in a subsequent blog post where we combine the preprocessing into one step within a ML pipeline. This first post goes through the trouble of explaining what is happening under the hood when we use a pipeline to handle preprocessing.We’ll use Jupyter Notebook with Python for our workflow.Explore distribution of label types.Output below:After training the model we make sentiment predictions on the raw twitter data but before we can do that, we need to download and do some basic data cleaning. You download tweets off the twitter RESTful API by keyword and get a stream of historical tweets back along with the metadata. I used Tweepy which is a python wrapper library for the twitter API that gives you more control on how you query the API. You can see the code on GitHub.The keyword used to download historical tweets was “Paul Ryan” who is the speaker of the US House of Representative at the time of writing. My choice for this keyword is arbitrary but I feel is good enough to return some polarizing tweets. This is important because the tweets themselves can act as their own benchmark. One of the obstacles of NLP is evaluating a model’s performance considering that language is relative and always changing.We’ll keep the location data and use it later to map out our results on a geographic map.NOTE : Whatever text preprocessing we do on the train data must also be done on the test and our raw data.I created a function that uses regex to do bulk formatting for every tweet in the dataset. Sample output is included under the code.We drop duplicate tweets as they bring no new information to the dataset and are also computationally inefficient. We also visualize the most common words in the corpus.We’ll convert each message which is represented by a list of tokens into a vector that a machine learning model can understand.To do this we use the Bag Of Words model which is a three step process.Each vector will have as many dimensions as there are unique words in the tweeter corpus.We will first use SciKit Learn’s CountVectorizer function which converts a collection of text documents to a matrix of token counts.Imagine this as a 2-D matrix where 1-D is the entire vocabulary contained in the messages and the other dimension is one column per tweet.Since there are so many messages, we can expect a lot of zero counts for the presence of every word in the data but SciKit Learn will output a Sparse Matrix. Below is an example — code and output — of a vectorized sentence.TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).IDF(t) = log_e(Total number of documents / Number of documents with term t in it).Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
4962,Model Averaging: A Robust Way to Deal with Model Uncertainty,31,2019-12-07,https://towardsdatascience.com/model-averaging-a-robust-way-to-deal-with-model-uncertainty-a604c4ab2050?source=collection_archive---------14-----------------------,2019,"In production the stakes are high. People are going to be reading the outputs from the model. And the outputs better make sense.Recently my team and I created a NLP classifier and put it into production on a large insurance dataset. It uses TfidfVectorizer and LinearSVC to classify free-text.But I quickly realised just that putting something into production is so different to the theory.In production, I think, it’s really important to get the probability of a model prediction. For example, if your model classifies something with a probability of 50% someone should investigate that prediction. If they find a mistake you’ve prevented the model from disrupting a pivotal system in the company.But obtaining the probability of a prediction is not always so straight forward.In this article I’ll walk through a way you can extract the probabilities from a SVM classifier in scikit-learn.This was critical for putting our model into production.Not only do we want the predicted class label, we also want the probability of that prediction. Scikit-learn has an interesting section on this topic in their documentation.We’ll need to create a regressor (or calibrator) that maps the output of the classifier to a value between 0 and 1. This calibrator will then give us the probability of each prediction.Essentially the calibrator will try to predict:where f is the output of the classifier.Or more plainly: Given the output of our classifier, what is the probability that we are 100% certain about this output?This paper by John Platt notes that a sigmoid function could be used as a regressor. We obtain the following:To find A and B we can use Maximum Likelihood Estimation. This would involve minimising the negative log likelihood:where ti is the target probability.The code for this is really simple. Scikit-learn hides most of the complexity behinds layers of abstraction.All you need to do is this:CalibratedClassifierCV will fit the training data using a k-fold cross validation approach. The default is 5-fold. See more information here.Then we’ll extract the average probability of the predicted class across all the k-folds. predict_proba is the function we'll need here. While the function itself seems like an unfinished sentence, it is incredibly useful.The get the predicted class we can simply use the predict function.How well does the calibrator fit the data? How can we tell?To do that we can use the sklearn.metrics.brier_score_loss. More information can be found here.If this score is very high then we cannot look at the probability outputs from the calibrator — they are useless. Instead we’ll need to look into better methods of fitting the calibrator. This article has some good approaches to fitting the calibrator.That’s it really!I’m sure I’ll have more to add as we continue to maintain this model. I wanted to share this little trick that helped us put our model into production effectively. Hopefully someone out there finds it useful.If I’ve made a mistake or you’re interested in reaching out please feel free to reach to me on twitter.Originally published at https://spiyer99.github.io on September 30, 2020.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4712,Bayesian Hyperparameter Optimization,15,2019-11-22,https://towardsdatascience.com/bayesian-hyperparameter-optimization-17dc5834112d?source=collection_archive---------26-----------------------,2019,"I’m diverging from the previous articles in the series. I’m going to review two tools that are heads and shoulders above the others. The design and beautiful visualizations do not come cheap. That doesn’t mean we can’t admire them and use them as a bar to which we strive. I will start with DataRobot. It’s an enterprise tool that you may find yourself having access to through work or school.I have experience using this tool and love it for the business cases for which I use it. My business case is to have a straightforward interface for a non-data scientist to run and deploy models in an automated way. DataRobot adds new features on a regular cadence, each built nicely within the existing user experience. I could go on about the benefits, but I will control my inner fan-girl.To keep things even with the other tools, I will focus on the most basic tasks to run a simple .csv file with autoML without any manual interventions or hyper-parameter tuning.Straight up, DataRobot is outside of the budget range of the individual data scientist. The implementation and cost are definitely in the realm of businesses. AWS Marketplace offers a one-year subscription for $98,000. Pocket change, I’m sure. But if you use AWS govCloud, it is $9.33/hr (it varies). Interesting.To keep parity across the tools in this series, I will stick to the Kaggle training file. Contradictory, My Dear Watson. Detecting contradiction and entailment in the multilingual text using TPUs. In this Getting Started Competition, we’re classifying pairs of sentences (consisting of a premise and a hypothesis) into three categories — entailment, contradiction, or neutral.6 Columns x 13k+ rows — Stanford NLP documentationYou create a project by uploading a dataset. This interface is where you begin.After the data is loaded, there are opportunities to change datatypes or remove features. There are some data distribution data. A bonus is that there are warnings if there might be data leakage. If data leakage is detected, DataRobot removes that feature from the final training dataset.Once you choose your target, you hit the big Start button with Modeling Mode set to AutoPilot. When you do that, you will see progress on the right side. As models are trained, they become available on the leaderboard as they complete.One good thing about having access to the early model results is that you can review for significant issues. Many times some data issues become glaringly apparent with the Insights, and I could halt the process and try again. This quick and easy review helps with rapid iteration.The leaderboard begins to fill with the completed models. You can choose several valid metrics in the dropdown. There are also some helpful tags to let you know WHY the leaders are up at the top.You can compare the models against each other.One tab I use often is speed versus accuracy. There are times when you are scoring millions of records when speed trumps accuracy if the accuracy drop is minor.The Insights tab is handy. You can quickly see if one of your features is popping. It’s up to your business expertise to know if that’s appropriate or not. This tab is where I find data issues early in the autoML model training. If I see something that doesn’t seem correct, I can iterate faster than waiting for the entire process to finish.DataRobot model explainability is the best of the tools I have reviewed so far. Each prediction is assigned which features influenced the final score, indicating not only strength but also direction.Not to be underestimated, documentation can be a real drain on your time. For this simple dataset, DataRobot generates a 7000+ word document with all of the charts, model parameters, and challenger model details. This documentation is a unique feature that I haven’t found in any other tools, though I have asked for it when asked. All done with a single click.To loosely compare results between tools, I reran the dataset in classification mode. The metrics are just slightly higher than Azure. For the most part, the model results are similar.For my business case, this is the top of the pile so far. Head-to-head in image processing or time-series may provide different results. That would be a challenge for another series.The ease of use, visualizations, access to challenger model details, model explainability, and the automated documentation stand out from the others. Of course, you are paying dearly for this.Next, I will show you H2O.ai Driverless AI. In my opinion, they are the closest comparison to DataRobot at this time. They have gone to great lengths to get top data visualization designers on the project so I’m expecting great things.If you missed one of the articles in the series, I have posted them below.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4066,How to fight Breast Cancer with Deep Learning?,68,2019-10-04,https://towardsdatascience.com/how-to-fight-breast-cancer-with-deep-learning-ab28e42e4250?source=collection_archive---------15-----------------------,2019,"Everything should be made as simple as possible, but not simpler ~ Albert EinsteinLong story short — in the article I published last week, I was forecasting the electoral result of two candidates running in the Polish presidential election.ForecastDuda: 50.97%Trzaskowski: 49.03%The official resultDuda: 51.03%Trzaskowski: 48.97%For me, the accuracy of the forecast was jaw-dropping.In this article, I am providing a summary of what went to plan, what didn’t and what insights can derive from these results.I performed an analysis of the historical data dating back to 2005. After plotting myriad of graphs, investigating different types of correlations, I decided on the final model.If you’re a data freak enjoying staring at graphs, or a data scientist being skeptical about the feature choice, have a look at my previous work.I decided to break down the analysis into county-by-county data to account for local variations and preferences. This gave me two datasets including electoral results in each county. One including the first ballot data, the second including run-off.In run-offs it all comes down to persuading the people, who didn’t vote for you in the first round. The increase in the voter base is crucial in securing the edge over your competitor.Note: It is important to note that the voter base is measured as a percentage of total eligible voters in the county, not as a percentage of votes cast.Since 2005, candidates of only two major parties have been making it to the run-offs — Law and Justice (PiS) and Civic Platform (PO). I investigated the model by running the above regression on the past three elections. The results were as follows:Adjusting for 2020 trends, predicted coefficients were:Similarly in the case of PiS:Finally, I assumed uniform distribution across these coefficients and ran simulations of elections, where coefficients were drawn and realizations of electoral results were recorded.The median score for president Duda in these simulations was 50.97% with almost 65% median turnout.As mentioned, president Duda received 51.03% of the votes securing his second term. Seeing this result being extremely close to my prediction, I looked further to see the actual regression coefficients.Comparing the voter base increase in each county gave me the following results:We can immediately see that the beta 0 intercept has been bigger in both instances. This means both candidates were able to maintain the uniform level of support over the entire country. Disparity between true intercepts and the ones predicted is mostly seen in the turnout — 68.18% vs. the predicted turnout of 65.49%.Beta 1 coefficients fall within the predicted boundaries reflecting parties’ ability to mobilise unused electorate capacity. High beta 1 for Trzaskowski (PO) comes from a high correlation of results in the first round with Szymon Hołownia, effectively counting Hołownia’s electorate as PO.Below I include actual increases alongside model’s predicted bounds.Interestingly, beta 2 ended up being well below expectations for both candidates. For president Duda this is due to an almost even split of Krzysztof Bosak’s voter base between PiS and PO. In the case of Rafał Trzaskowski it’s caused by a low turnout of Robert Biedroń’s and Władysław Kosiniak-Kamysz’s voters. This is in line however, with decreasing ability to mobilise third party electorate, as beta 2 coefficients have been constantly decreasing for both parties.This is an experimental approach, which does not link to any specific method published in the scientific community. Yet, taking into account the polarisation of Polish society, historical trends appear to play major role when it comes to predicting electoral outcomes.Note, that on an election-by-election basis, the support for PiS & PO candidates increases in relative terms. Given the political environment I talked about in my previous article, preferences change modestly. Hence, using previous elections could give us a lot of insight.Additionally, traditional surveys are prone to respondents’ behavioural biases! Before the run-off, Rafał Trzaskowski tended to lead the polls including undecided voters, whereas Duda led the ones without them.Therefore, I deeply believe that traditional surveys should be adjusted to account for political landscape and historical trends. At the moment, they seem to be solely copy-pasting respondents’ responses without any deeper analysis.Do you think there’s something missing from this analysis?What do you think about the validity of my assumptions?Maybe you’d like to chat about this model in more detail?Feel free to get in touch!I performed all analyses using Python in Jupyter Notebook.I downloaded and cleaned electoral data from pkw.gov.pl. Please, feel free to explore my project more in depth by visiting my GitHub page!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
842,We need to hear from you!,93,2018-09-06,https://towardsdatascience.com/we-need-to-hear-from-you-dc6d46fb605a?source=collection_archive---------15-----------------------,2018,"tl;dr: Shut up and take me to the mind map!Humanity is reaching an inflection point in terms of its technological development. At the same time, we are reevaluating our place on Earth and rethinking how to build a fairer society. Can artificial intelligence (AI, machine learning, statistical learning or however you want to call it) serve to tackle societal and environmental challenges? Yes, absolutely. In fact, the same algorithms used to recommend products on an e-commerce website, or choose the ads shown to you, can be applied to solve real human problems. All data scientists, from aspiring ones to researchers, have the opportunity (and even the responsibility) to take advantage of the current data revolution to improve our world.But let’s be clear, this is a complex endeavour, which requires cross-disciplinary and multi-sector collaborations involving governments, NGOs, private organizations and academia. Otherwise, it will be too easy to fall for the AI hype instead of understanding it as a tool to augment our abilities. Only solutions backed up by research and deep understanding of the respective problem domain will be robust (remember the scientific method) and effective in the long term.It requires cross-disciplinary and multi-sector collaborations, involving governments, NGOs, private organizations and academia, to use AI for good.But is academic research the only way to use AI for good? How can we as data scientists help? My initial impulse was to browse the web and look for existing initiatives. This browsing-the-web exercise got a bit more serious than initially planned (it’s cold outside anyways). Turns out, there are many caring people and great initiatives related, in some or other way, to the Data Science and AI for good movement. This is very encouraging, but there is still so much to be done!It’s been a few years since I started using mind maps. They are powerful tools for knowledge management and sharing. I prepared the Data Science and AI for good mind map to collect my findings. It aims to presents an overview of the existing “for good” initiatives around the world, from organizations and bootcamps to seed funding opportunities for startups. This mind map is powered by two amazing pieces of open source software: the TiddlyWiki and its TiddlyMap extension. The TiddlyWiki + TiddlyMap combo allows the creation of interactive wikis in JavaScript with integrated concept maps. Moreover, the application can be used as a single HTML file which can be conveniently hosted on GitHub pages.Without further ado, the Data Science and AI for good mind map can be found here. On the right panel, you will find the map itself (choose the Map tab) and on the left the content associated with each topic or node. Open a node by double clicking it. The Live tab provides a less cluttered view of the map so you might find it easier to navigate.You will find useful resources regardless of what stage of your career you are in or the amount of time you are willing to commit. For instance, go to the root node if you are just looking for inspiration and interesting reads. The child nodes (the ones in dark green color) cover the following topics:Now that you know the different ways you can serve the common good with data and technology, what are you waiting for? I’ve picked a couple of projects for the coming months. If you know of any resources that are missing and deserve to be added, please reach out via email or by using the issues section of the corresponding GitHub repository.PS: A similar TW+TM web application is being prepared with a larger mind map exploring the landscape of AI and data science, full of free and valuable resources. Stay tuned!Original post: https://carlgogo.github.io/ai4good_mindmap/Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
224,TensorFlow Object Detection in Action,524,2018-03-02,https://towardsdatascience.com/tensorflow-object-detection-in-action-4aca394d51b1?source=collection_archive---------4-----------------------,2018,"What is Linear Regression?Linear regression is a statistical method of finding the relationship between independent and dependent variables. Let us take a simple dataset to explain the linear regression model.Why do we call them as Independent and Dependent variables?If you take our example dataset, the “Years of Experience” columns are independent variables and the “Salary in 1000$” column values are dependent variables. Our independent variables are independent because we cannot mathematically determine the years of experience. But, we can determine / predict salary column values (Dependent Variables) based on years of experience. If you look at the data, the dependent column values (Salary in 1000$) are increasing / decreasing based on years of experience.Sum of Squared Errors (SSE)In order to fit the best intercept line between the points in the above scatter plots, we use a metric called “Sum of Squared Errors” (SSE) and compare the lines to find out the best fit by reducing errors. The errors are sum difference between actual value and predicted value.To find the errors for each dependent value, we need to use the formula below.The sum of squared errors SSE output is 5226.19. To do the best fit of line intercept, we need to apply a linear regression model to reduce the SSE value at minimum as possible. To identify a slope intercept, we use the equationy = mx + b,We will use Ordinary Least Squares method to find the best line intercept (b) slope (m)Ordinary Least Squares (OLS) MethodTo use OLS method, we apply the below formula to find the equationWe need to calculate slope ‘m’ and line intercept ‘b’. Below is the simpler table to calculate those values.m = 1037.8 / 216.19m = 4.80b = 45.44 - 4.80 * 7.56 = 9.15Hence, y = mx + b → 4.80x + 9.15 y = 4.80x + 9.15Let’s compare our OLS method result with MS-Excel. Yes, we can test our linear regression best line fit in Microsoft Excel.Wonderful! Our OLS method is pretty much the same as MS-Excel’s output of ‘y’.Let us calculate SSE again by using our output equation.Now Sum of Squared Error got reduced significantly from 5226.19 to 245.38.Ordinary Least Square method looks simple and computation is easy. But, this OLS method will work for both univariate dataset which is single independent variables and single dependent variables and multi-variate dataset. Multi-variate dataset contains a single independent variables set and multiple dependent variables sets, require us to use a machine learning algorithm called “Gradient Descent”.Gradient Descent AlgorithmGradient descent algorithm’s main objective is to minimise the cost function. It is one of the best optimisation algorithms to minimise errors (difference of actual value and predicted value).Let’s represent the hypothesis h, which is function or a learning algorithm.The goal is similar like the above operation that we did to find out a best fit of intercept line ‘y’ in the slope ‘m’. Using Gradient descent algorithm also, we will figure out a minimal cost function by applying various parameters for theta 0 and theta 1 and see the slope intercept until it reaches convergence.In a real world example, it is similar to find out a best direction to take a step downhill.We take a step towards the direction to get down. From the each step, you look out the direction again to get down faster and downhill quickly. The similar approach is using in this algorithm to minimise cost function.We can measure the accuracy of our hypothesis function by using a cost function and the formula isGradient Descent for Linear RegressionWhy do we use partial derivative in the equation? Partial derivatives represents the rate of change of the functions as the variable change. In our case we change values for theta 0 and theta 1 and identifies the rate of change. To apply rate of change values for theta 0 and theta 1, the below are the equations for theta 0 and theta 1 to apply it on each epoch.To find the best minimum, repeat steps to apply various values for theta 0 and theta 1. In other words, repeat steps until convergence.Types of Gradient Descent AlgorithmsThere are three types of Gradient Descent Algorithms:1. Batch Gradient Descent2. Stochastic Gradient Descent3. Mini-Batch Gradient DescentBatch Gradient DescentStochastic Gradient Descent (SGD)Mini-Batch Gradient DescentSummary:In a summary, explained about the following topics in detail.Reference:Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1528,Using Starbucks app user data to predict effective offers,14,2018-12-11,https://towardsdatascience.com/using-starbucks-app-user-data-to-predict-effective-offers-20b799f3a6d5?source=collection_archive---------11-----------------------,2018,"Data with imbalanced classes is the common problem in machine learning. Unless handled in a suitable manner, it may lead to a model that pretends as the best performing model, while being biased towards specific classes.For example, consider the following dataset that contains 10 classes, but the occurrence of each class is unequally distributed. A model trained using this data set is biased towards C1, C2, C4, and C5, but seldom predicts classes C7, C8, C9, and C10.This is the popular and the successful approach, which creates new samples of minority classes than merely duplicating the samples. Consider the previous scenario where we have only 3 samples (C10_D_1, C10_D_2 and C10_D_3) of C10 (since we have only 3 samples, all three are considered as neighbors), and SMOTE creates lines between 3 data points and picks synthetic data points (C10_S_1 and C10_S_2) from the lines. The following diagram illustrates the graphical explanation of what we have discussed.This can be performed using the extended version of scikit-learn called “scikit-learn-contrib,” below code segment shows using SMOTE using scikit-learn-contrib.SMOTE algorithm has some property values to optimize; unfortunately, due to the unviability of the score and transform method for SMOTE, the Pipeline and RandomizedSearchCV cannot be utilized to perform automated optimizing. Thus, we need to manually block and play properties values to optimize. For more details on SMOTE and properties, refer [2].This approach duplicates a sample for the minority classes to balance the classes. Here we discuss in a step-wise manner to achieve oversampling.Step 1: Identifying frequency (frequency_of_majority_class) of the dominant class in the data.Step 2: Divide the dataset into two (dataset contains dominant classes (DF_FOR_MAJORITY) and minor classes (DF_FOR_MINORITY)).Step 3: Get the list of minor classes.Step 4: Duplicate sample of minority classes using the re-sample method.Here we use n_samples = frequency_of_majority_class to specify the number of samples to generate.Step 5: Concatenate DF_FOR_MAJORITY and over sampled data (DF_FOR_MINORITY_OVERSAMPLED).The following table will be the resulting output due to oversampling.This approach removes samples of majority classes and attempts to match with the number of samples of minority classes.The difference is, in step 1, the frequency of the minor class is considered instead of identifying the frequency of the dominant class.Note: This approach is not usually recommended as it tends to lose the useful behavior of majority classes.We discussed three approaches to resolve imbalanced data issues, and depending on the problem context, we should pick the right one. It ensures that advanced analytics solutions are meaningful and not biased specific behavior.[1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, “SMOTE: synthetic minority over-sampling technique,” Journal of artificial intelligence research, 321–357, 2002.[2] Lemaitre G., Nogueira F., and Adidas C. (2016). “Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning”. CoRR abs/1609.06570.[3] Kubat, M., and Matwin, S. (1997). Addressing the Curse of Imbalanced Training Sets: One Sided Selection. In: Proceedings of the Fourteenth International Conference on Machine Learning, pp. 179–186 Nashville, Tennesse. Morgan Kaufmann.[4] Ling, C., and Li, C. (1998). Data Mining for Direct Marketing Problems and Solutions. In: Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98) New York, NY. AAAI Press.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4868,Addresses and Barangays: Geotagging with Google Maps API + PhilGIS,25,2019-12-02,https://towardsdatascience.com/addresses-and-barangays-geotagging-with-google-maps-api-philgis-73d575c0e8e1?source=collection_archive---------35-----------------------,2019,"Fastai is a library built on top of PyTorch for deep learning applications. Their mission is to make deep learning easier to use and getting more people from all backgrounds involved. They also provide free courses for Fastai.Fastai v2 was released in August, I will use it to build and train a deep learning model to classify different sports fields on Colab in just a few lines of codes.First, I need to collect images for the model to learn. I wish to have a model classifying different sports fields: baseball, basketball, soccer, tennis, and American football. I searched and downloaded the images and saved them in separate folders and uploaded to Google Drive.Once the dataset is ready, I can start the work on Colab.First upgrade fastai,and import fastai.vision,then mount the Google Drive and setup the pathNow we are ready to go.Fastai provide a mid-level API: DataBlock to deal with data, it is quite easy to use.Different blocks can be used, in this case, we used ImageBlock as the x and CategoryBlock as the label. It is also possible to use other blocks such as MultiCategoryBlock, MaskBlock, PointBlock, BBoxBlock, BBoxLblBlock for different applications.I used get_image_files function in fastai to get the path of the images as the x and used parent_label method to find the folder names as the label. There are some built-in functions in fastai, you can also write your own function to do this.Then I used RandomSplitter to split the training and validation dataset.Then used RandomResizedCrop to resize the image to 224 and aug_transforms() for data augmentation.This is kind of a template of Fastai DataBlock, it is quite flexible, you can change it based on your situation. You can find the detailed tutorial here.Once we have the DataBlock ready, we can create dataloader:and check the labels using vocaband show some of the images with lablesNow we are ready for training.I created a CNN learner using resnet34:and used lr_find() to find a suitable learning ratethen we can train the model using fit_one_cycleOnly after 5 epochs, we can achieve >90% accuracy.Now we can unfreeze the network and train the whole network.Let’s it, only after a few epochs, we achieve 95% accuracy.We can also interpret the model. The confusion matrix can be plotted using this :We can also use this to see the image with the highest loss.This article shows a simple and quick example of image classification using fastai. It is possible to create more complicated and interesting applications using this powerful library.Thanks for reading, happy coding.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1735,Machine Learning — Probability & Statistics,1100,2019-01-07,https://towardsdatascience.com/machine-learning-probability-statistics-f830f8c09326?source=collection_archive---------5-----------------------,2019,"Why is data science sexy? It has something to do with so many new applications and entire new industries come into being from the judicious use of copious amounts of data. Examples include speech recognition, object recognition in computer vision, robots and self-driving cars, bioinformatics, neuroscience, the discovery of exoplanets and an understanding of the origins of the universe, and the assembling of inexpensive but winning baseball teams. In each of these instances, the data scientist is central to the whole enterprise. He/she must combine knowledge of the application area with statistical expertise and implement it all using the latest in computer science ideas.In the end, sexiness comes down to being effective. I recently read Sebastian Gutierrez’s “Data Scientists at Work”, in which he interviewed 16 data scientists across 16 different industries to understand both how they think about it theoretically and also very practically what problems they’re solving, how data’s helping, and what it takes to be successful. All 16 interviewees are at the forefront of understanding and extracting value from data across an array of public and private organizational types — from startups and mature corporations to primary research groups and humanitarian nonprofits — and across a diverse range of industries — advertising, e-commerce, email marketing, enterprise cloud computing, fashion, industrial internet, internet television and entertainment, music, nonprofit, neurobiology, newspapers and media, professional and social networks, retail, sales intelligence, and venture capital.In particular, Sebastian asked open-ended questions so that the personalities and spontaneous thought processes of each interviewee would shine through clearly and accurately. The practitioners in this book share their thoughts on what data science means to them and how they think about it, their suggestions on how to join the field, and their wisdom won through experience on what a data scientist must understand deeply to be successful within the field.In this post, I want to share the best answers that these data scientists gave for the question:“What advice would you give to someone starting out in data science?”“Creativity and caring. You have to really like something to be willing to think about it hard for a long time. Also, some level of skepticism. So that’s one thing I like about PhD students — five years is enough time for you to have a discovery, and then for you to realize all of the things that you did wrong along the way. It’s great for you intellectually to go back and forth from thinking “cold fusion” to realizing, “Oh, I actually screwed this up entirely,” and thus making a series of mistakes and fixing them. I do think that the process of going through a PhD is useful for giving you that skepticism about what looks like a sure thing, particularly in research. I think that’s useful because, otherwise, you could easily too quickly go down a wrong path — just because your first encounter with the path looked so promising.And although it’s a boring answer, the truth is you need to actually have technical depth. Data science is not yet a field, so there are no credentials in it yet. It’s very easy to get a Wikipedia-level understanding of, say, machine learning. For actually doing it, though, you really need to know what the right tool is for the right job, and you need to have a good understanding of all the limitations of each tool. There’s no shortcut for that sort of experience. You have to make many mistakes. You have to find yourself shoehorning a classification problem into a clustering problem, or a clustering problem into a hypothesis testing problem.Once you find yourself trying something out, confident that it’s the right thing, then finally realizing you were totally dead wrong, and experiencing that many times over — that’s really a level of experience that unfortunately there’s not a shortcut for. You just have to do it and keep making mistakes at it, which is another thing I like about people who have been working in the field for several years. It takes a long time to become an expert in something. It takes years of mistakes. This has been true for centuries. There’s a quote from the famous physicist Niels Bohr, who posits that the way you become an expert in a field is to make every mistake possible in that field.”“I would say to always bite the bullet with regard to understanding the basics of the data first before you do anything else, even though it’s not sexy and not as fun. In other words, put effort into understanding how the data is captured, understand exactly how each data field is defined, and understand when data is missing. If the data is missing, does that mean something in and of itself? Is it missing only in certain situations? These little, teeny nuanced data gotchas will really get you. They really will.You can use the most sophisticated algorithm under the sun, but it’s the same old junk-in–junk-out thing. You cannot turn a blind eye to the raw data, no matter how excited you are to get to the fun part of the modeling. Dot your i’s, cross your t’s, and check everything you can about the underlying data before you go down the path of developing a model.Another thing I’ve learned over time is that a mix of algorithms is almost always better than one single algorithm in the context of a system, because different techniques exploit different aspects of the patterns in the data, especially in complex large data sets. So while you can take one particular algorithm and iterate and iterate to make it better, I have almost always seen that a combination of algorithms tends to do better than just one algorithm.”“I always give the same advice, as I get asked this question often. My take on it is that if you’re an undergrad, study a specialty where you can take as many math and physics courses as you can. And it has to be the right courses, unfortunately. What I’m going to say is going to sound paradoxical, but majors in engineering or physics are probably more appropriate than say math, computer science, or economics. Of course, you need to learn to program, so you need to take a large number of classes in computer science to learn the mechanics of how to program. Then, later, do a graduate program in data science. Take undergrad machine learning, AI, or computer vision courses, because you need to get exposed to those techniques. Then, after that, take all the math and physics courses you can take. Especially the continuous applied mathematics courses like optimization, because they prepare you for what’s really challenging.It depends where you want to go because there are a lot of different jobs in the context of data science or AI. People should really think about what they want to do and then study those subjects. Right now the hot topic is deep learning, and what that means is learning and understanding classic work on neural nets, learning about optimization, learning about linear algebra, and similar topics. This helps you learn the underlying mathematical techniques and general concepts we confront every day.”“For the person still deciding what to study I would say STEM fields are no-brainers, and in particular the ‘TEM ones. Studying a STEM subject will give you tools to test and understand the world. That’s how I see math, statistics, and machine learning. I’m not super interested in math per se, I’m interested in using math to describe things. These are tool sets after all, so even if you’re not stoked on math or statistics, it’s still super worth it to invest in them and think about how to apply it in the things you’re really passionate about.For the person who’s trying to transition like I did, I would say, for one, it’s hard. Be aware that it’s difficult to change industries and you are going to have to work hard at it. That’s not unique to data science — that’s life. Not having any connections in the field is tough but you can work on it through meet-ups and coffee dates with generous people. My number-one rule in life is “follow up.” If you talk to somebody who has something you want, follow up.Postings for data scientists can be pretty intimidating because most of them read like a data science glossary. The truth is that the technology changes so quickly that no one possesses experience of everything liable to be written on a posting. When you look at that, it can be overwhelming, and you might feel like, “This isn’t for me. I don’t have any of these skills and I have nothing to contribute.” I would encourage against that mindset as long as you’re okay with change and learning new things all the time.Ultimately, what companies want is a person who can rigorously define problems and design paths to a solution. They also want people who are good at learning. I think those are the core skills.”“To someone coming from math or the physical sciences, I’d suggest investing in learning software skills — especially Hadoop and R, which are the most widely used tools. Someone coming from software engineering should take a class in machine learning and work on a project with real data, lots of which is available for free. As many people have said, the best way to become a data scientist is to do data science. The data is out there and the science isn’t that hard to learn, especially for someone trained in math, science, or engineering.Read “The Unreasonable Effectiveness of Data” — a classic essay by Google researchers Alon Halevy, Peter Norvig, and Fernando Pereira. The essay is usually summarized as “more data beats better algorithms.” It is worth reading the whole essay, as it gives a survey of recent successes in using web-scale data to improve speech recognition and machine translation. Then for good measure, listen to what Monica Rogati has to say about how better data beats more data. Understand and internalize these two insights, and you’re well on your way to becoming a data scientist.”“I find it tough to find and hire the right people. It’s actually a really hard thing to do, because when we think about the university system as it is, whether undergrad or grad school, you focus in on only one thing. You specialize. But data scientists are kind of like the new Renaissance folks, because data science is inherently multidisciplinary.This is what leads to the big joke of how a data scientist is someone who knows more stats than a computer programmer and can program better than a statistician. What is this joke saying? It’s saying that a data scientist is someone who knows a little bit about two things. But I’d say they know about more than just two things. They also have to know to communicate. They also need to know more than just basic statistics; they’ve got to know probability, combinatorics, calculus, etc. Some visualization chops wouldn’t hurt. They also need to know how to push around data, use databases, and maybe even a little OR. There are a lot of things they need to know. And so it becomes really hard to find these people because they have to have touched a lot of disciplines and they have to be able to speak about their experience intelligently. It’s a tall order for any applicant.It takes a long time to hire somebody, which is why I think people keep talking about how there is not enough talent out there for data science right now. I think that’s true to a degree. I think that some of the degree programs that are starting up are going to help. But even still, coming out of those degree programs, for MailChimp we would look at how you articulate and communicate to us how you’ve used the data science chops across many disciplines that this particular program taught you. That’s something that’s going to weed out so many people. I wish more programs would focus on the communication and collaboration aspect of being a data scientist in the workplace.”I think the areas where the biggest opportunities are also have the most challenges. Healthcare data obviously has some of the biggest issues with PII and privacy concerns. Added to that, you’ve also got sclerotic bureaucracies, fossilized infrastructures, and data silos that make it very hard to solve hard problems requiring integration across multiple data sets. It will happen, and I think a lot the technologies we’ve talked about here are directly relevant to making health care better, more affordable, and more distributed. I see this representing a generational opportunity.Another huge area in its early days is risk management — whether it’s in finance, trading, or insurance. It’s a really hard problem when you’re talking about incorporating new data sets into risk assessment — especially when applying these technologies to an industry like insurance, which, like health care, has lots of privacy issues and data trapped within large bureaucracies. At the same time, these old fossilized companies are just now starting to open up and figure out how to best interact with the startup community in order to leverage new technologies. This is another area that I find incredibly exciting.The third area I’m passionate about is reshaping manufacturing and making it more efficient. There has been a trend towards manufacturing moving back onshore. A stronger manufacturing sector could be a bridge to recreating a vibrant middle class in the US. I think technology can help hasten this beneficial trend.“I think, ultimately, learning how to do data science is like learning to ski. You have to do it. You can only listen to so many videos and watch it happen. At the end of the day, you have to get on your damn skis and go down that hill. You will crash a few times on the way and that is fine. That is the learning experience you need. I actually much prefer to ask interviewees about things that did not go well rather than what did work, because that tells me what they learned in the process.Whenever people come to me and ask, “What should I do?” I say, “Yeah, sure, take online courses on machine learning techniques. There is no doubt that this is useful. You clearly have to be able to program, at least somewhat. You do not have to be a Java programmer, but you must get something done somehow. I do not care how.”Ultimately, whether it is volunteering at DataKind to spend your time at NGOs to help them, or going to the Kaggle website and participating in some of their data mining competitions — just get your hands and feet wet. Especially on Kaggle, read the discussion forums of what other people tell you about the problem, because that is where you learn what people do, what worked for them, and what did not work for them. So anything that gets you actually involved in doing something with data, even if you are not paid being for it, is a great thing.Remember, you have to ski down that hill. There is no way around it. You cannot learn any other way. So volunteer your time, get your hands dirty in any which way you can think, and if you have a chance to do internships — perfect. Otherwise, there are many opportunities where you can just get started. So just do it.”“First and foremost, it is very important to be self-critical: always question your assumptions and be paranoid about your outputs. That is the easy part. In terms of skills that people should have if they really want to succeed in the data science field, it is essential to have good software engineering skills. So even though we may hire people who come in with very little programming experience, we work very hard to instill in them very quickly the importance of engineering, engineering practices, and a lot of good agile programming practices. This is helpful to them and us, as these can all be applied almost one-to-one to data science right now.If you look at dev ops right now, they have things such as continuous integration, continuous build, automated testing, and test harnesses — all of which map very well from the dev ops world to the data ops (a phrase I stole from Red Monk) world very easily. I think this is a very powerful notion. It is important to have testing frameworks for all of your data, so that if you make a code change, you can go back and test all of your data. Having an engineering mindset is essential to moving with high velocity in the data science world. Reading Code Complete and The Pragmatic Programmer is going to get you much further than reading machine learning books — although you do, of course, have to read the machine learning books, too.”“If someone is just starting out in data science, the most important thing to understand is that it’s okay to ask people questions. I also think humility is very important. You’ve got to make sure that you’re not tied up in what you’re doing. You can always make changes and start over. Being able to scrap code, I think, is really hard when you’re starting out, but the most important thing is to just do something.Even if you don’t have a job in data science, you can still explore data sets in your downtime and can come up with questions to ask the data. In my personal time, I’ve played around with Reddit data. I asked myself, “What can I explore about Reddit with the tools that I have or don’t have?” This is great because once you’ve started, you can see how other people have approached the same problem. Just use your gut and start reading other people’s articles and be like, “I can use this technique in my approach.” Start out very slowly and move slowly. I tried reading a lot when I started, but I think that’s not as helpful until you’ve actually played around with code and with data to understand how it actually works, how it moves. When people present it in books, it’s all nice and pretty. In real life, it’s really not.I think trying a lot of different things is also very important. I don’t think I’d ever thought that I would be here. I also have no idea where I’ll be in five years. But maybe that’s how I learn, by doing a bit of everything across many different disciplines to try to understand what fits me best.”“Though somewhat generic advice, I believe you should trust yourself and follow your passion. I think it’s easy to get distracted by the news in the media and the expectations presented by the media and choose a direction that you didn’t want to go. So when it comes to data science, you should look at it as a starting point for your career. Having this background will be beneficial in anything you do. Having an ability to create software and the ability to work with statistics will enable you to make smarter decisions in any field you choose. For example, we can read about how an athlete’s performance is improved through data, like someone becoming the gold medalist in the long jump because they optimized and practiced the angle at which they should jump. This is all led by a data-driven approach to sports.If I were to go into more specific technical advice, then it depends on the ambitions of the person who is receiving the advice. If the person wants to create new methods and tools, then that advice would be very different. You need to persist and keep going in your direction, and you will succeed. But if your intent is to be diverse and flexible in many situations, then you want to have a big toolbox of different methods.I think the best advice given to me was given by a Stanford professor whose course I attended a while ago. He recommended having a T-shaped profile of competence but with a small second competence next to the core competence, so that you have an alternative route in life if you need it or want it. In addition to the vertical stem of single-field expertise, he recommended that you have the horizontal bar of backgrounds broad enough so that you can work with many different people in many different situations. So the while you are in a university, building a T shape with another small competence in it is probably the best thing to do.Maybe the most important thing is to surround yourself with people greater than you are and to learn from them. That’s the best advice. If you’re in a university, that’s the best environment to see how diverse the capabilities of people are. If you manage to work with the best people, then you will succeed at anything.”“I think perhaps they would need to start by looking at themselves and figuring out what it is they really care about. What is it they want to do? Right now, data science is a bit of a hot topic, and so I think there are a lot of people who think that if they can have the “data science” label, then magic, happiness, and money will come to them. So I really suggest figuring out what bits of data science you actually care about. That is the first question you should ask yourself. And then you want to figure out how to get good at that. You also want to start thinking about what kinds of jobs are out there that really play to what you are interested in.One strategy is to go really deep into one part of what you need to know. We have people on our team who have done PhDs in natural language processing or who got PhDs in physics, where they’ve used a lot of different analytical methods. So you can go really deep into an area and then find people for whom that kind of problem is important or similar problems that you can use the same kind of thinking to solve. So that’s one approach.Another approach is to just try stuff out. There are a lot of data sets out there. If you’re in one job and you’re trying to change jobs, try to think whether there’s data you could use in your current role that you could go and get and crunch in interesting ways. Find an excuse to get to try something out and see if that’s really what you want to do. Or just from home there’s open data you can pull. Just poke around and see what you can find and then start playing with that. I think that’s a great way to start. There are a lot of different roles that are going under the name “data science” right now, and there are also a lot of roles that are probably what you would think of data science but don’t have a label yet because people aren’t necessarily using it. Think about what it is that you really want.”“First is that you definitely have to tell a story. At the end of the day, what you are doing is really digging into the fundamentals of how a system or an organization or an industry works. But for it be useful and understandable to people, you have to tell a story.Being able to write about what you do and being able to speak about your work is very critical. Also worth understanding is that you should maybe worry less about what algorithm you are using. More data or better data beats a better algorithm, so if you can set up a way for you to analyze and get a lot of good, clean, useful data — great!”“Find a problem you’re excited about. For me, every time I started something new, it’s really boring to just study without a having a problem I’m trying to solve. Start reading material and as soon as you can, start working with it and your problem. You’ll start to see problems as you go. This will lead you to other learning resources, whether they are books, papers, or people. So spend time with the problem and people, and you’ll be fine.Understand the basics really deeply. Understand some basic data structures and computer science. Understand the basis of the tools you use and understand the math behind them, not just how to use them. Understand the inputs and the outputs and what is actually going on inside, because otherwise you won’t know when to apply it. Also, it depends on the problem you’re tackling. There are many different tools for so many different problems. You’ve got to know what each tool can do and you’ve got to know the problem that you’re doing really well to know which tools and techniques to apply.”“They should understand probability theory forwards and backwards. I’m at the point now where everything else I learn, I then map back into probability theory. It’s great because it provides this amazing, deep, rich basis set along which I can project everything else out there. There’s a book by E. T. Jaynes called Probability Theory: The Logic of Science, and it’s our bible. We really buy it in some sense. The reason I like the probabilistic generative approach is you have these two orthogonal axes — the modeling axis and the inference axis. Which basically translates into how do I express my problem and how do I compute the probability of my hypothesis given the data? The nice thing I like from this Bayesian perspective is that you can engineer along each of these axes independently. Of course, they’re not perfectly independent, but they can be close enough to independent that you can treat them that way.When I look at things like deep learning or any kind of LASSO-based linear regression systems, which is so much of what counts as machine learning these days, they’re engineering along either one axis or the other. They’ve kind of collapsed that down. Using these LASSO-based techniques as an engineer, it becomes very hard for me to think about: “If I change this parameter slightly, what does that really mean?” Linear regression as a model has a very clear linear additive Gaussian model baked into it. Well, what if I want things to look different? Suddenly all of these regularized least squares things fall apart. The inference technology just doesn’t even accept that as a thing you’d want to do.”“I think a strong statistical background is a prerequisite, because you need to know what you’re doing, and understand the guts of the model you build. Additionally, my statistics program also taught a lot about ethics, which is something that we think a lot about at DataKind. You always want to think about how your work is going to be applied. You can give anybody an algorithm. You can give someone a model for using stop-and-frisk data, where the police are going to make arrests, but why and to what end? It’s really like building any new technology. You’ve got to think about the risks as well as the benefits and really weigh that because you are responsible for what you create.No matter where you come from, as long as you understand the tools that you’re using to draw conclusions, that is the best thing you can do. We are all scientists now, and I’m not just talking about designing products. We are all drawing conclusions about the world we live in. That’s what statistics is — collecting data to prove a hypothesis or to create a model of the way the world works. If you just trust the results of that model blindly, that’s dangerous because that’s your interpretation of the world, and as flawed as it is, your understanding is how flawed the result is going to be.In short, learn statistics and be thoughtful.”I would highly recommend you to read Data Scientists at Work. The book displays how some of the world’s top data scientists work across a dizzyingly wide variety of industries and applications — each leveraging her own blend of domain expertise, statistics, and computer science to create tremendous value and impact.Data is being generated exponentially and those who can understand that data and extract value from it are needed now more than ever. The hard-earned lessons and joy about data and models from these thoughtful practitioners would be tremendously useful if you aspire to join the next generation of data scientists.— —If you enjoyed this piece, I’d love it if you hit the clap button 👏 so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn. Sign up for my newsletter to receive my latest thoughts on data science, machine learning, and artificial intelligence right at your inbox!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3658,Machine Learning in the Airline Industry: The Next Step,17,2019-08-31,https://towardsdatascience.com/machine-learning-in-the-airline-industry-the-next-step-f6e64080abf2?source=collection_archive---------11-----------------------,2019,"This blog post is one of three technical deep-dives about a specific type of architecture for serverless batch jobs on AWS. If you want to know more about the background of this cloud setup, go here for more details. The overall picture of the implemented service looks like this:In short, the architecture consists of one block of components for development and another one for executing a batch job. For this blog post, we focus only on the roles and policies of this architecture. There is also a public code repository that allows you to reproduce the full service and all its parts based on Cloudformation scripts.When we talk about roles and policies, we talk about Identity and Access Management (IAM). It constitutes the main security block of every AWS cloud architecture. IAM restricts which entity can do what within a given cloud environment. For this blog post, you need to understand two concepts: roles and policies.When users or services want to do something in the cloud, they assume roles. When you specify a role, you can limit the type of entity that can use it. For instance, you can define a role for human users that allows them to see everything but change nothing. If an AWS service tries to use that role, it fails. Think about roles as an organizational umbrella for what a user or service can do within your cloud environment.Policies, in contrast, list the specific actions a role can perform. In the same way that roles restrict which entity can assume them, policies restrict which resources they can use.When you design your roles and policies, stick to the principle of least privilege. That is, only allow access and actions that services need. Any extension beyond that is a potential security issue. For example, if a service needs to write data to an S3 bucket, you should not implement a policy that allows all S3 actions.Before we move on to the big picture, let’s start with a small primer to CloudFormation scripts. If you already have experience with CloudFormation — or similar concepts — you can skim or skip the next three paragraphs.CloudFormation is the AWS service for Infrastructure as Code. That is, you define a target infrastructure, push it to AWS, and AWS provisions it for you. You organize your resources in so-called stacks, which make it very easy to adjust, monitor, or delete them.You can write CloudFormation scripts in JSON or YAML files. All examples here and in the code repository are YAML files, but you could do the same thing in JSON.There is also a set of CloudFormation specific commands available to you. In the upcoming examples, I use three of them:From a role and policy perspective, we have to consider both component blocks of the service. For each part, we need to identify which services need to interact with other services. Services without outgoing interactions, such as S3 or a code repository, can be left as such. Services that require engagement with other components need policies that allow them to do that.For the development components, we need to make sure that we can build a container image and push it to the registry. For the batch job components, we need to make sure that the trigger can run the most recent container image from the registry.To avoid abstract ruminations, let’s dive into the implementation details. I provide an overview figure for each building block so that you can follow along visually.As mentioned before, roles are the organizational umbrella for policies. For the sake of simplicity, each role in this example relates to precisely one policy. Before we look at each policy, let me walk you through the two main properties of roles:Here is what it looks like in CloudFormation with the CodeBuild role as an example:It is best practice to define one role for each component of your architecture. Technically, you can embed policies into your role definition. However, it is easier to read and to maintain if you create a separate policy resource.There are two services involved in the development workflow: CodePipeline and CodeBuild. CodePipeline is an orchestration tool that triggers the CodeBuild project whenever developers push a new version of the master branch to the code repository.First, let’s look at the CodePipeline. In our scenario, the service needs to interact with three services:Here’s how CodePipeline’s role and the associated policy look like in CloudFormation:Note: For simplicity, I did not restrict the resources in this and the following examples. In a real-world implementation, the principle of least-privilege demands that you minimize the accessed resources, too!Second, there is CodeBuild. It involves four different services.The last point is optional, but I promise that you regret it if you leave it out. The corresponding CloudFormation scripts look like this:The two roles for the development components should be pretty self-explanatory. Well, things get a little bit more complicated now.We are now left to define roles and policies for the batch job. That is, for the Fargate task and the CloudWatch rule that triggers it. Let’s start with the slightly confusing one first: the Fargate task.Although one box represents the Fargat task in the architectural diagram, it requires two roles. Think about your relationship with your boss. Your boss has the authority to assign a task to you. You need the tools to carry out the assigned job. That’s what you need for Fargate, too. One execution role, aka a “boss role” and a task role, aka an “employee role.”The execution role comprises access to two services:Here’s how the execution role and its policy look like in CloudFormation:The vital policies for the task role depend on what developers put into the container image. That is, without knowing what the developers implemented, you can not decide on the correct set of policies. Let’s assume that the task loads some data from S3, transforms it, and writes it back to another bucket. In this simple case, you need to configure two services for the task role:Have a look at the CloudFormation specification for details:Again, the specific resources and actions depend entirely on the implemented business logic within the container image.Finally, there is the CloudWatch rule, which triggers the batch job. Its role and policy is straightforward again and concerns two services:Here’s the CloudFormation specification:A natural question to ask at this point: why are there several places, e.g., to allow access to CloudWatch logs? Why not define one policy for logging and attach it to all roles that need it?There is one primary reason to avoid that: shared policies add dependencies between resources. These dependencies can come back to bite you later on. If you change a policy that is assumed by several roles, you might break stuff without even realizing it.I hope this post helped you to understand better how to think about and build IAM roles and policies. Again, if you want to know more about the architecture, refer to the conceptual post. If you’re going to rebuild it, have a look at the public code repository. Also, there will be one more post shortly that extends this discussion to the specific building blocks for the whole service. If you are more interested in the network aspects of such a service, I already wrote a blog post about that.Let me know your thoughts and experiences in the comments. I am also happy to connect on Twitter and LinkedIn. Thank you for reading!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2110,Can You Find Waldo Faster Than A Computer? Spoiler: You Can’t.,307,2019-02-13,https://towardsdatascience.com/can-you-find-waldo-faster-than-a-computer-spoiler-you-cant-ff382e601c31?source=collection_archive---------7-----------------------,2019,"Machine learning is a powerful framework that from the outside may look complex and intimidating. However, once we break down a problem into its component steps, we see that machine learning is really only a sequence of understandable processes, each one simple by itself.In the first half of this series, we saw how we could implement a solution to a “data science for good” machine learning problem, leaving off after we had selected the Gradient Boosting Machine as our model of choice.In this article, we’ll continue with our pipeline for predicting poverty in Costa Rica, performing model optimizing, interpreting the model, and trying out some experimental techniques.The full code is available as a Jupyter Notebook both on Kaggle (where it can be run in the browser with no downloads required) and on GitHub. This is an active Kaggle competition and a great project to get started with machine learning or to work on some new skills.Model optimization means searching for the model hyperparameters that yield the best performance — measured in cross-validation — for a given dataset. Because the optimal hyperparameters vary depending on the data, we have to optimize — also known as tuning — the model for our data. I like to think of tuning as finding the best settings for a machine learning model.There are 4 main methods for tuning, ranked from least efficient (manual) to most efficient (automated).Naturally, we’ll skip the first three methods and move right to the most efficient: automated hyperparameter tuning. For this implementation, we can use the Hyperopt library, which does optimization using a version of Bayesian Optimization with the Tree Parzen Estimator. You don’t need to understand these terms to use the model, although I did write a conceptual explanation here. (I also wrote an article for using Hyperopt for model tuning here.)The details are a little protracted (see the notebook), but we need 4 parts for implementing Bayesian Optimization in HyperoptThe basic idea of Bayesian Optimization (BO) is that the algorithm reasons from the past results — how well previous hyperparameters have scored — and then chooses the next combination of values it thinks will do best. Grid or random search are uninformed methods that don’t use past results and the idea is that by reasoning, BO can find better values in fewer search iterations.See the notebook for the complete implementation, but below are the optimization scores plotted over 100 search iterations.Unlike in random search where the scores are, well random over time, in Bayesian Optimization, the scores tend to improve over time as the algorithm learns a probability model of the best hyperparameters. The idea of Bayesian Optimization is that we can optimize our model (or any function) quicker by focusing the search on promising settings. Once the optimization has finished running, we can use the best hyperparameters to cross validate the model.Optimizing the model will not always improve our test score because we are optimizing for the training data. However, sometimes it can deliver a large benefit compared to the default hyperparameters. In this case, the final cross validation results are shown below in dataframe form:The optimized model (denoted by OPT and using 10 cross validation folds with the features after selection) places right in the middle of the non-optimized variations of the Gradient Boosting Machine (which used hyperparameters I had found worked well for previous problems.) This indicates we haven’t found the optimal hyperparameters yet, or there could be multiple sets of hyperparameters that performly roughly the same.We can continue optimization to try and find even better hyperparameters, but usually the return to hyperparameter tuning is much less than the return to feature engineering. At this point we have a relatively high-performing model and we can use this model to make predictions on the test data. Then, since this is a Kaggle competition, we can submit the predictions to the leaderboard. Doing this gets us into the top 50 (at the moment) which is a nice vindication of all our hard work!At this point, we have implemented a complete solution to this machine learning problem. Our model can make reasonably accurate predictions of poverty in Costa Rican households (the F1 score is relatively low, but this is a difficult problem). Now, we can move on to interpreting our predictions and see if our model can teach us anything about the problem. Even though we have a solution, we don’t want to lose sight of why our solution matters.The very nature of machine learning competitions can encourage bad practices, such as the mistake of optimizing for the leaderboard score at the cost of all other considerations. Generally this leads to using ever more complex models to eke out a tiny performance gain.In the real-world, above a certain threshold — which depends on the application — accuracy becomes secondary to explainability, and you’re better off with a slightly less performant model if it is simpler.A simple model that is put in use is better than a complex model which can never be deployed. Moreover, those at the top of the leaderboard are probably overfitting to the testing data and do not have a robust model.A good strategy for getting the most out of Kaggle is to work at a problem until you have a reasonably good solution — say 90% of the top leaderboard scores — and then not stress about getting to the very top. Competing is fun, but learning is the most valuable aspect of taking on these projects.In the midst of writing all the machine learning code, it can be easy to lose sight of the important questions: what are we making this model for? What will be the impact of our predictions? Thankfully, our answer this time isn’t “increasing ad revenue” but, instead, effectively predicting which households are most at risk for poverty in Costa Rica so they can receive needed help.To try and get a sense of our model’s output, we can examine the prediction of poverty levels on a household basis for the test data. For the test data, we don’t know the true answers, but we can compare the relative frequency of each predicted class with that in the training labels. The image below shows the training distribution of poverty on the left, and the predicted distribution for the testing data on the right:Intriguingly, even though the label “not vulnerable” is most prevalent in the training data, it is represented less often on a relative basis for the predictions. Our model predicts a higher proportion of the other 3 classes, which means that it thinks there is more severe poverty in the testing data. If we convert these fractions to numbers, we have 3929 households in the “non vulnerable” category and 771 households in the “extreme” category.Another way to look at the predictions is by the confidence of the model. For each prediction on the test data, we can see not only the label, but also the probability given to it by the model. Let’s take a look at the confidence by the value of the label in a boxplot.These results are fairly intuitive — our model is most confident in the most extreme predictions — and less confident in the moderate ones. Theoretically, there should be more separation between the most extreme labels and the targets in the middle should be more difficult to tease apart.Another point to draw from this graph is that overall, our model is not very sure of the predictions. A guess with no data would place 0.25 probability on each class, and we can see that even for the least extreme poverty, our model rarely has more than 40% confidence. What this tells us is this is a tough problem — there is not much to separate the classes in the available data.Ideally, these predictions, or those from the winning model in the competition, will be used to determine which families are most likely to need assistance. However, just the predictions alone do not tell us what may lead to the poverty or how our model “thinks”. While we can’t completely solve this problem yet, we can try to peer into the black box of machine learning.In a tree-based model — such as the Gradient Boosting Machine — the feature importances represent the sum total reduction in gini impurity for nodes split on a feature. I never find the absolute values very helpful, but instead normalize the numbers and look at them on a relative basis. For example, below are the 10 most important features from the optimized GBM model.Here we can see education and ages of family members making up the bulk of the most important features. Looking further into the importances, we also see the size of the family. This echoes findings by poverty researchers: family size is correlated to more extreme poverty, and education level is inversely correlated with poverty. In both cases, we don’t necessarily know which causes which, but we can use this information to highlight which factors should be further studied. Hopefully, this data can then be used to further reduce poverty (which has been decreasing steadily for the last 25 years).In addition to potentially helping researchers, we can use the feature importances for further feature engineering by trying to build more features on top of these. An example using the above results would be taking the meaneduc and dividing by the dependency to create a new feature. While this may not be intuitive, it’s hard to tell ahead of time what will work for a model.An alternative method to using the testing data to examine our model is to split the training data into a smaller training set and a validation set. Because we have the labels for all the training data, we can compare our predictions on the holdout validation data to the true values. For example, using 1000 observations for validation, we get the following confusion matrix:The values on the diagonal are those the model predicted correctly because the predicted label is the same as the true label. Anything off the diagonal the model predicted incorrectly. We can see that our model is the best at identifying the non-vulnerable households, but is not very good at discerning the other labels.As one example, our model incorrectly classifies 18 households as non-vulnerable which are in fact in extreme poverty. Predictions like these have real-world consequences because those might be families that as a result of this model, would not receive help. (For more on the consequences of incorrect algorithms, see Weapons of Math Destruction.)Overall, this mediocre performance — the model accuracy is about 60% which is much better than random guessing but not exceptional — suggests this problem may be difficult. It could be there is not enough information to separate the classes within the available data.One recommendation for the host organization — the Inter-American Development Bank — is that we need more data to better solve this problem. That could come either in the form of more features — so more questions on the survey — or more observations — a greater number of households surveyed. Either of these would require a significant effort, but the best return to time invested in a data science project is generally by gathering greater quantities of high-quality labeled data.There are other methods we can use for model understanding, such as Local Interpretable Model-agnostic Explainer (LIME), which uses a simpler linear model to approximate the model around a prediction. We can also look at individual decision trees in a forest which are typically straightforward to parse because they essentially mimic a human decision making process.Overall, machine learning still suffers from an explainability gap, which hinders its applicability: people want not only accurate predictions, but an understanding of how those predictions were generated.We’ve already solved the machine learning problem with a standard toolbox, so why go further into exploratory techniques? Well, if you’re like me, then you enjoy learning new things just for the sake of learning. What’s more, the exploratory techniques of today will be the standard tools of tomorrow.For this project, I decided to try out two new (to me) techniques:Recursive feature elimination is a method for feature selection that uses a model’s feature importances — a random forest for this application — to select features. The process is a repeated method: at each iteration, the least important features are removed. The optimal number of features to keep is determined by cross validation on the training data.Recursive feature elimination is simple to use with Scikit-Learn’s RFECV method. This method builds on an estimator (a model) and then is fit like any other Scikit-Learn method. The scorer part is required in order to make a custom scoring metric using the Macro F1 score.While I’ve used feature importances for selection before, I’d never implemented the Recursive Feature Elimination method, and as usual, was pleasantly surprised at how easy this was to do in Python. The RFECV method selected 58 out of around 190 features based on the cross validation scores:The selected set of features were then tried out to compare the cross validation performance with the original set of features. (The final results are presented after the next section). Given the ease of using this method, I think it’s a good tool to have in your skill set for modeling. Like any other Scikit-Learn operation, it can fit into a Pipeline, allowing you to quickly execute a complete series of preprocessing and modeling operations.There are a number of unsupervised methods in machine learning for dimension reduction. These fall into two general categories:Typically, PCA (Principal Components Analysis) and ICA (Independent Components Analysis) are used both for visualization and as a preprocessing step for machine learning, while manifold methods like t-SNE (t-Distributed Stochastic Neighbors Embedding) are used only for visualization because they are highly dependent on hyperparameters and do not preserve distances within the data. (In Scikit-Learn, the t-SNE implementation does not have a transform method which means we can’t use it for modeling).A new entry on the dimension reduction scene is UMAP: Uniform Manifold Approximation and Projection. It aims to map the data to a low-dimensional manifold — so it’s an embedding technique, while simultaneously preserving global structure in the data. Although the math behind it is rigorous, it can be used like an Scikit-Learn method with a fit and transform call.I wanted to try these methods for both dimension reduction for visualization, and to add the reduced components as additional features. While this use case might not be typical, there’s no harm in experimenting! Below shows the code for using UMAP to create embeddings of both the train and testing data.The application of the other three methods is exactly the same (except TSNE which cannot be used to transform the testing data). After completing the transformations, we can visualize the reduced training features in 3 dimensions, with the points colored by the value of the target:None of the methods cleanly separates the data based on the label which follows the findings of other data scientists. As we discovered earlier, it may be that this problem is difficult considering the data to which we have access. Although these graphs cannot be used to say whether or not we can solve a problem, if there is a clean separation, then it indicates that there is something in the data that would allow a model to easily discern each class.As a final step, we can add the reduced features to the set of features after applying feature selection to see if they are useful for modeling. (Usually dimension reduction is applied and then the model is trained on just the reduced dimensions). The performance of every single model is shown below:The model using the dimension reduction features has the suffix DR while the number of folds following the GBM refers to the number of cross validation folds. Overall, we can see that the selected set of features (SEL) does slightly better, and adding in the dimension reduction features hurts the model performance! It’s difficult to conclude too much from these results given the large standard deviations, but we can say that the Gradient Boosting Machine significantly outperforms all other models and the feature selection process improves the cross validation performance.The experimental part of this notebook was probably the most enjoyable for me. It’s not only important to always be learning to stay ahead in the data science field, but it’s also enjoyable for the sake of learning something new.The drive to constantly be improving and gaining new knowledge is a critical skill for a data scientist.Despite this exhaustive coverage of machine learning tools, we have not yet reached the end of methods to apply to this problem!Some additional steps we could take are:The great part about a Kaggle competition is you can read about many of these cutting-edge techniques in other data scientists’ notebooks. Moreover, these contests give us realistic datasets in a non-mission-critical setting, which is a perfect environment for experimentation.The best contests can lead to new advances by encouraging friendly competition, open sharing of work, and rewarding innovative approaches.As one example of the ability of competitions to better machine learning methods, the ImageNet Large Scale Visual Recognition Challenge led to significant improvements in convolutional neural networks.Data science and machine learning are not incomprehensible methods: instead, they are sequences of straightforward steps that combine into a powerful solution. By walking through a problem one step at a time, we can learn how to build the entire framework. How we use this framework is ultimately up to us. We don’t have to dedicate our lives to helping others, but it is rewarding to take on a challenge with a deeper meaning.In this article, we saw how we could apply a complete machine learning solution to a data science for good problem, building a machine learning model to predict poverty levels in Costa Rica.Our approach followed a sequence of processes (1–4 were in part one):Finally, if after all that you still haven’t got your fill of data science, you can move on to exploratory techniques and learn something new!As with any process, you’ll only improve as you practice. Competitions are valuable for the opportunities they provide us to employ and develop skills. Moveover, they encourage discussion, innovation, and collaboration, leading both to more capable individual data scientists and a better community. Through this data science project, we not only improve our skills, but also make an effort to improve outcomes for our fellow humans.As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Twitter @koehrsen_will.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1477,Deploy a machine learning model using flask,1700,2018-11-30,https://towardsdatascience.com/deploy-a-machine-learning-model-using-flask-da580f84e60c?source=collection_archive---------4-----------------------,2018,"Data visualisation is an important skill when searching and presenting important insights. There are many visuals that can be used to present your data. One of the most interesting data visual is the cloropleth map.What’s a cloropleth map?A choropleth map (from Greek χῶρος “area/region” and πλῆθος “multitude”) is a thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical variable being displayed on the map, such as population density or per-capita income.Source: https://en.wikipedia.org/wiki/Choropleth_mapWhy is this such an interesting visual? a) it is pretty, b) it tells us the data that we are interested in exactly the location of where it is associated in, and c) it is pretty!With the introductions done, let’s get down to the code (and the preparations for it).Step 1: Install required Python librariesLet’s install several packages that we’ll need for this exercise. GeoPandas is an amazing package that takes pandas's DataFrame to the next level by allowing it to parse geospatial data. It will use Descartes to generate a Matplotlib plot.Step 2: Get the dataThere are two kinds of data that we will need for this exercise:Step 3: Begin to codeNow, let’s jump right into the code2. Load and view the shapefile dataOk, so as you can see we have several data fields in the downloaded shapefile. The ones that we are interested in are the column NAME_1 (province name) and geometry (the shape of the province). And as you can see as well, the shapefile stores the location information in the form of polygons. Let’s plot it, shall weSo we have the map of Indonesia, but it looks too small, let’s resize itMuch better,3. Load the province dataAs you can see, we have the provinces, 2015 population, number of cities, and several other interesting numbers. All we have to do now is to merge the data with the shapefile and we can begin visualizing these numbers4. Merge and show the mapCool, we have the data in the most clean format, let’s make the plotPretty good, right? But it can be better. For instance, we know which locations have high number of cities per region and which have low number of cities per region. To make the plot clearer, let’s add the province labels to it. Add the following code at the bottom of the code above.Ok, that is better. If we take a closer look, we can see that, according to Wikipedia, Jawa Tengah province has a very high number of cities per regions compared to other provinces.Another tweak we can do is to change the orientation of the color map legend to be horizontal, in case you want the upper space to focus on the map.Just change this codeTo this5. Save it!Now that we have made the cloropleth map, the last thing we’ll need to do is to save it in a friendly popular format, like .pngThat’s all for now, hope this post is useful.You can download the Notebook file for this excercise along with the cleaned province data and Indonesian shapefiles in my Github repo here.This post is inspired by Benjamin Cooley’s post.I was interested in learning and doing something further and loved the results, hence I am sharing it in my own post here. :)Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2714,Automated Lip Reading : Simplified,43,2019-06-03,https://towardsdatascience.com/automated-lip-reading-simplified-c01789469dd8?source=collection_archive---------18-----------------------,2019,"I recently read through the book R for Data Science, and I found “ggplot2” in R is more powerful than I thought.So, I played with the tool by applying it to the NBA player stats in the 2019–2020 Season. I am going to share my exploration step by step in this article and I hope “ggplot2” in R can be another useful instrument in your data-science tool case.I should clarify that the following materials are driven by exploring the functions in ggplot2 instead of understanding the data.R in Data Science describes the equation to generate a plot in ggplot2 as shown below:where DATA is your data frame, GEOM_FUNCTION is the type of plot you want to use (eg. barplot), MAPPINGS is the variables to plot (eg. x and y), STAT is the equation applied to the raw data in your data frame (eg. calculate the count of observation), COORDINATE_FUNCTION is to adjust how to place the generated plot, and FACET_FUNCTION is used to place multiple plots in an organized order.Players’ stats per game in the 2019–2020 NBA season.The dataset has already been cleaned by removing “NA”s and correcting stats of players traded during the season.The complex figures generated by ggplot2 usually include multiple layers, which are basically the overlapping of plots generated by a sequence of GEOM_FUNCTIONs. Therefore, if no GEOM_FUNCTION specified, the plot will be empty.Let’s start by putting one layer on the plot, which is a scatter plot with x as the minutes played (MP) and y as the points made (PTS).Here, I put MAPPINGS in the ggplot() function instead of geom_point() function to set it as a global variable, which will be used by all the following layers. However, it can also be put into geom_point(), which will serve as a local variable instead.It seems that there is a positive correlation between the players’ playing time and points made per game, however, it is not strictly linear, which suggests that average players cannot get that many points as the superstars even given the same amount of playing time.Too simple?Yeah, you can add color or shape by specifying the corresponding column names in the data.Here, I used variable Pos in the dataset to separate the players into five groups and highlighted the points by different colors and shapes.The color can also be assigned to a continuous variable instead of a categorical one, which is shown in the following example.The amazing part of ggplot2 is that you can even specify a condition in the arguments. For example, we want to separate LA Lakers from the other teams.Here, Tm stands for Team in the dataset and “LAL” is the Lakers.Actually, the scatter plots are very messy with everything overlapping with each other, which is hard to draw any conclusion with.Let’s next try splitting the data into different groups based on the categorical variables in the dataset.For example, I want to split the data by the teams. I would use the same ggplot equation as before but adjust the output in the facets using FACET_FUNCTION.You can also do it with multiple categorical variables in the facet function by replacing facet_wrap() by facet_grid().Pretty messy, right? This is one of the visualizations we need to avoid.Next, I am going to add another layer to the plot.The plot now has another layer of line chart generated by geom_smooth(). Since we specified the argument ‘color’ in the global variable in ggplot(), both the scatter plot and the line chart split the players by positions with the same color setting.To make the plot look less messy, I decided to add the facet function to better organize them.Much better, right?Seems the trend is similar for all five positions. However, I did observe a slightly smaller variation at the high MP region for SG and SF comparing to the other three positions. These results remind me of the common positions of those superstars.Barplot is another popular plot type in data visualization. However, it’s a little special in ggplot2, because its default function is not plotting the raw values in the data.The reason is related to the STAT function in ggplot2. The default STAT function of a barplot (geom_bar) is stat_count(), which calculated the number of observations in each category defined by argument x.This plot shows the number of players in each category on the Y-axis, which is the default behavior of geom_bar() function in ggplot2.In addition, the color of the bars needs to be defined via ‘fill’ instead of ‘color’, which is another difference with the scatter plot.However, most of the time a barplot of the raw values in the dataset is more important than that of the count, so ‘stat= “identity”’ is one argument to be used instead.The y-axis now in the plot above is changed from count to the sum of points.The last part of the code is to show you can present the generated figure in a different coordinate type. My favorite one is called the Coxcomb chart as shown below.Isn’t it cool?We can interpret from the data that the shooting guards contribute the most in total points per game in the season. However, this may result from the largest number of players in this position.Even though NBA suspends season after player tested positive for the coronavirus, I hope all the players, fans and people in the world stay healthy and win the war against the virus.I hope ggplot2 will be one of your visualization weapons as a data scientist.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2754,"<strong class=""markup--strong markup--h3-strong"">Computer Vision — An Introduction</strong>",80,2019-06-08,https://towardsdatascience.com/computer-vision-an-introduction-bbc81743a2f7?source=collection_archive---------4-----------------------,2019,"Disclaimer: This piece of writing isn’t meant to cause panic. I’m in no way an expert, in neither epidemiology nor mathematical modeling. The approach undertaken below is laden with assumptions, both known and latent in nature. This is meant to drive the case for social distancing and self-quarantines. This is an opinion piece, and not an advisory on public policy.November, 2019: People in the wet market in the city of Wuhan, an oft-forgotten city in the Chinese province of Hubei, go about their business for the day, knowing little of the havoc that’ll shortly be unleashed on the world.Fast forward to March, 2020.The new year has come and gone, and with it the first stories from China of a new pathogen the likes of which the world has never before seen: Covid-19.Sweeping through the continental mass of China, it has now spread all over the planet, almost as if taken from a dystopian novel.As it reaches the shores of India, the country of my residence, I cannot help but feel a little scared and worried. The stories that come out of affected countries are dire, dreary and desperate. Italy faced one of its worst days, just yesterday (20th March, 2020), when close to 700 people succumbed to the vile virus in a single day.I must admit, I did not take the threat seriously, much like so many other people in my age group. The argument below was commonly heard at the lunches, in the corridor conversations, and on the Whatsapp groups.“Oh, so what if it infects a lot of people… only like 2% die. That’s so much lower than MERS or H1N1 or even the common flu. Why are we worrying about this?”In hindsight, what was commonly overlooked in the argument was that this was heavily confounded by the age of victims.Statistical theory tells you to be cautious with the numbers. Be prudent with the numbers. Be patient with the numbers.You always need to coax the numbers to give insights.The older one gets, the higher the mortality rates. And significantly higher at ages more than 65.Covid-19 is a boomer-killer.Much like the average 20-something Indian, I used to spend most of my childhood summers at my grandparents’ place, and I have a lot of cherished memories of those summers, and of them. For the longest time, because of their constant presence, I was convinced that they’re invincible. That they will always be there. And now, for the first time, I’m afraid.I’m very afraid.So much so, that for the past week or so, I’ve been brushing up on the theory of epidemiology. From May’s 1982 paper where the fundamental, basic reproduction number, Rₒ, was introduced to the more recent work by Diekmann, Odo, Hans Heesterbeek, and Tom Britton¹ on formalizing the definition and properties of Rₒ.This piece of writing looks at adopting a disease model that gives insight into how social distancing and quarantining helps.And it does, quite considerably.The following section can get quite dry because of the mathematical nature of the system. I’ll try my best and keep the content as concise as I can. However, not all of the math can be filtered, so please bear with me.What we’ll do in this section is to try and fit the model for the data shared for Germany. The same analysis can be replicated for other countries, provided the data is available publicly, and without concerns of sampling bias and representation bias.At this juncture, it’s vital to re-iterate the fact that this analysis and the findings are in no way an accurate picture of reality simply because of the various assumptions that are rendered void in real life.At the end of the day, this analysis should urge the reader to stay at home. And be proud, and satisfied with taking such a decision.In modeling Covid-19, we aren’t modeling individual likelihoods. We don’t have sufficient publicly available data for that. What we are attempting is to model the population under the behavior of the contagion.While there is an entire spectrum of models out there (R: earlyR, EpiEstim), the model we’ll implement is the simplest of them all. The more complex the model, the more assumptions are baked into the interior workings; in the current case of a rampant, life-threatening pandemic I am not making any assumptions that I don’t fully understand.The model we adopt is called the SIR model². First discussed in the Royal Statistical Journal in 1927, it is a dynamic model that compartmentalizes the population/junta into three broad groups:A key assumption in this model is that infection guarantees future immunity. So something like a flu cannot be modeled using the SIR framework, while chickenpox, and measles can. As of now, Covid-19 hasn’t shown any behavior where recovered patients have relapsed.Further, we don’t make any assumptions on the population vitals. These are the birth rates, and death rates due to causes other than Covid-19.The movements from one group to another are governed by a set of differential equations.These equations are important because of the parameters they solve for.β, γ are the two most important elements in this model.In the recent public policy terms,If there’s anything you should remember at the end of this, it’s this.Lower your beta, and increase gamma. Period.You solve those equations, and you get to understand how the disease will develop.Now, I can hear you saying “enough of the theory… Where’s the actual model dude?”We extract the data for Germany, because their’s seemed to be the best maintained (by the Robert Koch Institute), by running a simple python script that extracts it from their Wikipedia page. Other countries considered were Italy, and the US.Note: The data for India is incomplete, and there are concerns regarding sufficient testing in the country. Spain has incomplete data for certain days.Ditching the time-series index, we adopt the notation of days since the first day. Let’s plot the data.Using Python’s scipy module, we solve for the ODEs and get estimates for β and γ.Let’s evaluate model fit after making predictions for the next ~50 days (amounting to 80 days in total).We get an RMSE of 625, and MAPE (mean average percentage error) of 23%.Considering that the model fit is not too bad, what can we tell?This is slightly off compared to WHO estimates of 2 to 2.5. Germany’s federal public health organization that monitors infectious diseases, the Robert Koch Institute, estimates that the basic reproduction number of Covid-19 is between 2.4 and 3.3.Conventional disease study says that if the Rₒ > 1, then the disease should be considered an epidemic. (Duh)So what now?If you recall what was mentioned in the previous section, there are two ways to cut down on Rₒ:But before going into why we must reduce Rₒ, let’s plot the all-too-familiar curve.Yes, the infamous Everest from the now-common “flatten the curve” discussions.Let’s evaluate what would happen if we adopt a simple strategy of distancing ourselves. Say, some social distancing intervention was done in Germany today such that the contact rate was reduced by 50% for the next n days, following which, it lapses to a more cautious reduction of 20%. In other wordsNow, we can change n for various configurations such as: 7 days, 14 days.When I ran this simulation, the way the model behaved blew my mind.The curve flattens!Nowadays, one of the most hotly debated components of these interventions is the timing. In the above simulation, we are enabling a 50% reduction in contact rate from the current day, i.e., 21st March, 2020. On the graph, this is day 26.What would happen if this were to take place in the peak of the epidemic, say day 40, when close to 2% of the population is infected?Voila!2% might not seem much, but this is against the entire population of Germany. A meager 2% amounts to 1.6 million people.Hence, early interventions are good when it comes to Covid-19.One thing to note from the visualizations above is that as the intervention takes place and the number of infections drop, the number of susceptibles remains large. This would mean exercising greater caution once the intervention is over. A slight lapse on the β, as simulated above, can lead to a significant growth in infections, again.This is a fairly simple strategy, dependent only on time. If data for different ages were available, strategies for age groups and demographic segments could be formed and tested in the form of simulations.In summary,It is just not worth the risk to mingle with people until the situation comes under control.As a final note, it’s important to call out the fact that this analysis is purely illustrative. Sure, we used actual data, and fit the model to it. But disease modeling has so many more nuances that I don’t have the expertise on. Moreover, in view of this outbreak, there have been questions on the quality of the data considering the frequency of the tests.These are biasing factors that affect the sample, and one must be cognizant of them.The simple SIR model used in this post suffers from a number of limitations. However, at the large scale of the outbreak we are now talking about, this simplification appears acceptable.Further, it assumes homogeneous mixing between individuals, which is waaay too simple. One could, for instance, divide the population into age groups, along with their locations and model the interactions between them. That might give a more realistic reflection of how the population is shaped.Again, for the purpose of the visualization of the flatten-the-curve effect, I think a simple model is alright.The insight from here isn’t that the model has predicted it all and we’re all doomed. No model is ever correct.Instead, focus on how critical social distancing is. Focus on how self-quarantining can help. Flatten the curve.Stay home, stay safe.Note from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2807,Play with QuickDraw: A Real-time Application,5,2019-06-13,https://towardsdatascience.com/play-with-quickdraw-a-real-time-application-137e66ea9b60?source=collection_archive---------20-----------------------,2019,"Premise: This article is intended as a ‘live’ document. This means I will add reviewing follow ups when we have updates from the Supervisory Authority (and I have time…).22.09.20 Update: A few weeks ago, the Supervisory Authority issued its final guidance on the topic. Apart from some minor adjustments (especially with regard to the potential ‘bias’ of AI, security, synthetic data and transparency) and the addional Glossary, the document is almost identical to the previous draft.When working in an AI company, there are basically two potential types of team. These could be summarised as:1) “We have all the security measures in place to protect our machine learning system and its data”, and2) “With all the new guidelines and procedures, we need to do something but we don’t know exactly what or how to do it”.Both approaches are understandable nowadays. With the different papers we are seeing about AI and data protection, it can be hard to judge what the best thing to do is.Should we wait for more defined rules, or move forward anyway?[update] Today, I am going to summarise the latest guidance of the ICO (the UK data protection authority) and its practical advice on managing the training and the deployment of AI, with some further advice on the related risks.I’ll try to avoid any form of legalese, I promise, but please bear in mind that a basic knowledge of the data protection field would help the content to sink in.If you want to immediately skip to the section dedicated to the actionable task, you can jump to the latest section (‘[+1]. So, what do we do?’).Ok, let’s begin.The ICO is famous for putting particular attention on the implementation of the Data Protection Impact Assessment (DPIA), which is required when AI systems process personal data.The ICO specifies that it is ‘unrealistic’ to reduce all the risks of the data subjects involved to zero (and indeed the law doesn’t require this). Instead, the main point is to identify and mitigate the risks to the greatest extent workable.Right now, we have several tools that make this possible.In relation to the AI field, implementation of the DPIA should specifically address and provide:Having said that, the ICO points out that it could be useful to describe all the different aspects in two separate assessments: a thorough technical description for ML/IT specialists and engineers, and a more generic and easily readable overview with all the key information around the logic of inputs, outputs and the personal data involved.Imagine you work with a company that provides a dedicated cloud computing environment with processing and storage, and a suite of common tools for machine learning. You probably define what data and models you want to use, the key model parameters, and the processes for evaluating, testing and updating those models.In this case it is easy to see that your supplier is a ‘Processor’ (they act on your behalf, under your instructions and decide only non-essential details) and you are a ‘Controller’ (you have full control of the final outcomes of the project, and you are ultimately in charge of the processing).However, what happens if your supplier uses this data for other purposes (for example, to improve their training system)?In this case, they immediately become a Controller for such processing, which requires another specific agreement with you for the disclosure and further processing of your data.The framework could become even more unclear when considering some companies provide live AI prediction and classification services to their customers.Are they a Controller, Processor or Joint Controller? It depends on each phase of the deployment. I thought an infographic would help in this scenario.In AI, the measure of ‘accuracy’ highlights how often an AI system guesses the correct answer. These answers often include personal data as possible predictions or inferences. Simple and clean.That said, the ICO adds that the more statistically accurate the system is, the more likely it is that the processing will be fair for the data subjects involved.This is why the statistical accuracy of a system should be assessed on an ongoing basis, focusing on precision (meaning the percentage of cases identified as positive that are in fact positive) and recall (the percentage of all cases that are in fact positive that are identified as such).Another aspect to consider is the proper legal basis for collecting and processing personal data for an AI project. Leaving aside consent, legal obligation, vital interest and performing a public task, which appears difficult to apply for in most of the activities in place, can we rely on the performance of a contract and legitimate interest of a business?The answer is yes.The guidance clarifies that AI could be necessary to deliver a contractual service to the relevant individual, or support in a pre-contractual phase (e.g. an AI-derived quote for a service).In this case, ‘performance of a contract’ is the perfect grounds for the use of the system (delivery phase), but it may not be the best choice for the prior phase: the training/development of the AI system.Therefore, ‘legitimate interest’ seems the most appropriate legal basis to train your AI model, but don’t forget to put in place your legitimate impact assessment to balance all the variables involved.Remember: even if you have a proper legal basis for processing personal data, the mere possibility that some data might be useful is not by itself enough for the organisation to prove that processing this data is necessary for building the model (more details on data minimisation later on).The most frequent disincentive for the implementation of AI is the form of discrimination that it could generate, in particular when we have imbalanced training data or just the glare of past discrimination. Some measures suggested by the guidance are:I won’t go in-depth on these aspects owing to the fact that we could talk for days about the difficulties related to each one. A fantastic paper on this topic could be found here.[update] The guidance also highlights that each measure should also be balanced, because they can have an impact on the statistical accuracy of the AI system’s performance.(Note: this small section is quite technical. I tried to simplify as much as I can, but for most of these aspects you need a strong technical knowledge related to AI.)In AI, security is worth the majority of business’ attention.If you think that usually an engineering team‘s use of third-party frameworks and code libraries is to create, maintain and develop an AI system, you’ll have in mind the potential risks that every AI company could have. Luckily, the community is already working on reducing the potential vulnerabilities behind these algorithms and their open-source packages, but we are still in the preliminary phase of this kind of approach. (More technical details here).On this topic, the ICO describe a few specific risks each company could face. In particular:Leaving aside the standard requirements for maintaining code and managing security risks that apply to the development work, the ICO basically suggests that, ‘the less data you have in the system, the more secure you are’ (data minimisation principle).With the aim of achieving that, the guide explores different measures that could be implemented, including:User’ requests (‘data subject requests’ or ‘DSRs’) appears as one of the most intricate aspects of the new technology for several reasons.For example:In this scenario, the simple access to their personal data could be burdensome, and sometimes, you will not be able to identify the individual in the training data either.Also, a portability request could be the trickiest one to address, particularly if the data was heavily transformed in a machine-readable form. The original data can be provided, however, the ICO points out that where this transformation is significant, the resulting data may no longer count as ‘provided’ by the data subjects and therefore no longer subject to the portability request(although it is still personal data, so can be the focus of different requests, e.g. the right of access)On the other hand, a possible request to erasure or rectification could be manageable and also should have a minor impact on the model built.Then, the guidance emphasises that, in the event of automated-decision making, the process for individuals to ask for human intervention should be simple and user-friendly (e.g. a webpage with a link or clear information allowing the individual to contact staff who can weigh in).The key point here is transparency: have in place a privacy policy that explains with common and understandable terminology the overall structure of the AI system, its purpose and the decisions made from it. This could drastically help in building trust of your users, and (hopefully) reduce the number of requests received.Someone said ‘blackbox’?[update] Yes, it’s true, for very complex systems it may be hard to follow the logic of a system and difficult to explain how they work. Also, some methods could accidentally reveal sensitive proprietary information about the AI model and facilitate the exposure of privacy attacks from external actors. The explAIn guidance could help you with that.If you have followed me throughout this article and you share my point of view, you may be wondering what actions you can take now.To provide you with an answer, I’ve created an AI Data Protection Checklist, which traces the ICO official paper in its entirety and could be your roadmap to identifying and controlling risks within your AI team.It’s teamwork, not a single man’s project!Views from your colleagues on every single step described above could be key to streamlining the process, and crucial for the end result. A supportive organisational culture is fundamental.[update] The guidance also clarified that it will constantly work to provide more support in this emerging field, with a proper ‘toolkit’ and a new ‘Cloud competing guidance’, which it will release in the coming years.Saying that, the document already provides relevant pieces of advice from the ICO and illustrates the key steps necessary to mitigate the risks one could face from AI, as well as steps that could be adopted immediately.Do you want to read more? Below you can find further respected papers on this topic:European Commission - White Paper on Artificial IntelligenceFuture of Privacy Forum - Warning SignsAEPD - RGPD compliance of processings that embed Artificial Intelligence | An IntroductionICO - Big data, artificial intelligence, machine learning and data protectionICO and The Alan Turing Institute - Project ExplAInWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2428,Intuitive Understanding of Attention Mechanism in Deep Learning,1920,2019-03-20,https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f?source=collection_archive---------0-----------------------,2019,"The pursuit of astronomy has been a gradual process of uncovering the insignificance of humanity. We started out in the center of the universe with the cosmos literally revolving around us. Then we were rudely relegated to one of 8 planets orbiting the sun, a sun which subsequently was revealed to be just one of billions of stars (and not even a large one) in our galaxy.This galaxy, the majestic Milky Way, seemed pretty impressive until Hubble discovered that all those fuzzy objects in the sky are billions of other galaxies, each of which has billions of stars (potentially with their own intelligent life). The demotion has only continued in the 21st century, as mathematicians and physicists have concluded the universe is one of an infinity of universes collectively called the multiverse.On top of the relegation to a smaller and smaller part of the cosmos, now some thinkers are claiming we live in a simulation, and soon will create our own simulated worlds. All of this is a long way to say we are not special. The idea that the Earth, and by extension humanity, does not occupy a privileged place in the universe is known as the Copernican Principle.While the Copernican Principle was first used with respect to our physical location — x, y, and z coordinates — in 1993, J Richard Gott applied the concept that we aren’t special observers to our universe’s fourth dimension, time. In “Implications of the Copernican principle for our future prospects” ($200 here or free through the questionably legal SciHub here), Gott explained that if we assume we don’t occupy a unique moment in history, we can use a basic equation to predict the lifetime of any phenomenon.The equation, in its simple brilliance (derivation at the end of article) is:Where t_current is the amount of time something has already been around, t_future is the expected amount of time it will last from now, and confidence interval expresses how certain we are in the estimate. This equation is based on a simple idea: we don’t exist at a unique moment in time and therefore, when we observe an event, we are most likely watching the middle and not the beginning or the conclusion.As with any equation, the best way to figure out how it works is to input some numbers. Let's apply this to something simple, say the lifetime of the human species. We’ll use a 95% confidence interval and assume modern humans have been around for 200,000 years. Plugging in the numbers, we get:The answer to the classic dinner-party question (okay, only the dinner parties I go to) of how long humans will be around is 5130 to 7.8 million years with 95% confidence. This is in close agreement with actual evidence that shows the mean duration of a mammal species is about 2 million years with the Neanderthals making it 300,000 years and Homo erectus 1.6 million years.The neat part about this equation is it can be applied to anything while relying only on statistics instead of trying to untangle a complex underlying web of causes. How long a television show runs for, the lifetime of a technology, or the length of time a company exists are all subject to numerous factors that are impossible to tease apart. Rather than digging through all the causes, we can take advantage of the temporal (a fancy word for time) Copernican Principle and arrive at a decent estimate for the lifetime of any phenomenon.To apply the equation to something closer to home, data science, we first need to find the current lifetime of the field, which we’ll put at 6 years based on when the Harvard Business Review released the article “Data Scientist: The Sexiest Job of the 21st Century”. Then, we use the equation to find we can expect, with 95% confidence, data science will be around for at least another 8 weeks, and at most, 234 years.If we want a narrower estimate, we reduce our confidence interval: at 50%, we get from 2 to 18 years.This illustrates an important point in statistics: if we want to increase the precision, we have to sacrifice accuracy. A smaller confidence interval is less likely to be correct, but it gives us a narrower range for our answer.If you want to play around with the numbers, here’s a Jupyter Notebook.You might object the answers from this equation are ridiculously wide, a point I’ll concede. However, the objective is not to get a single number — there are almost no situations, even when using the best algorithm, that we can find the one number guaranteed to be spot on — but to find a plausible range.I like to think of the Copernican Lifetime Equation as a Fermi estimate, a back of the envelope style calculation named for the physicist Enrico Fermi. In 1945, with nothing more than some scraps of paper, Fermi estimated the yield of the Trinity atomic bomb test to within a factor of 2! Likewise, we can use the equation to get a reasonable estimate for the lifetime of a phenomenon.There are two important lessons, one technical, one philosophical, from using the Copernican Principle to find out how long something will be around:With regards to the first point, if you want to figure out how long a Broadway show will run, where do you even start gathering data? You could look at reviews, actors’ reputations, or even the dialogue in the script to determine the appeal and figure out how much longer the show will go on. Or, you could do as Gott did, apply his simple equation, and correctly predict the runtimes of 42 out of 44 shows on Broadway.When we think about the individual data points, it’s easy to get lost in the details and mis-interpret some aspect of human behavior. Sometimes, we need to take a step back, abstract away all the details, and apply basic statistics instead of trying to figure out human psychology.On the latter point, as Nassim Taleb points out in his book Antifragile, the easiest way to figure out how long a non-perishable item — such as an idea or a work of art — will be around is to look at its current lifetime. In other words, the future lifetime of a technology is proportional to its past lifetime.This is known as the Lindy Effect and makes sense with a little thought: a concept that has been around for a long time — books as a medium for exchanging information — must have a reason for surviving so long, and we can expect it to persist far into the future. On the other hand, a new idea— Google Glass — is statistically unlikely to survive because of the vast number of new concepts arising every single day.Moreover, companies that have been around for 100 years — Caterpillar — must be doing something right and we can expect them to be around longer than startups — Theranos — which have not demonstrated they fulfill a need.For one more telling example of the Copernican Lifetime Equation, consider the brilliant tweet you sent an hour ago. Statistics tell us this will be relevant for between 90 more seconds to a little less than 2 days. On the other hand, the oldest English story, Beowulf, will still be read by bored students at least 26 years from now and up to 39,000 years in the future. What’s more, this story won’t be experienced in virtual reality — consumer virtual reality having between 73 days and 311 more years — but on the most durable form of media, books, which have 29.5 to 45000 years of dominance left.Some people may view the Copernican Principle — both temporal and spatial — as a tragedy, but I find it exciting. Much as we realized the stunning grandeur of the universe only after we discarded the geocentric model, once we let go of the myth that our time is special and that we exist at the pinnacle of humanity, the possibilities are vast. Yes, we may be insignificant on a cosmic scale now, but 5000 years from now our ancestors — or possibly us — will expand throughout and even fundamentally alter the Milky Way.As David Deutsch points out in his book The Fabric of Reality, anything not prohibited by the laws of physics will be achieved by humans given enough time. Instead of worrying that the job you’re supposed to be doing now is meaningless, think of it as contributing to the great endeavor upon which humanity has embarked. We are currently subject to the Copernican Principle, but maybe humans really are different: after all, we are starstuff that has evolved the ability to contemplate our place in the universe.The derivation of the Copernican Lifetime Equation is as follows. The total lifetime of anything is the current lifetime plus the future lifetime:If we don’t believe our temporal location is privileged, then our observation of a phenomenon occurs neither at the beginning nor the end:Make the following substitution for z:Insert the definition of the total lifetime to get:Then solve for the future lifetime of any phenomenon:With a confidence interval of 95%, we get the multiplicative factors 1/39 and 39; with a confidence interval of 50%, the factors are 1/3 and 3; for 99% confidence, our factors become 1/199 and 199.You can play around with the equations in this Jupyter Notebook. Also, take a look at the original paper by Gott for more details.As always, I welcome constructive criticism and feedback. I can be reached on Twitter @koehrsen_will.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1390,All Warm Encoding,121,2018-11-20,https://towardsdatascience.com/all-warm-encoding-736c9c6799bb?source=collection_archive---------17-----------------------,2018,"Convolutional Neural Nets (CNNs), a concept that has achieved the greatest performance for image classification, was inspired by the mammalian visual cortex system. In spite of the extraordinary progress in automated computer vision systems, most of the success of image classification architectures comes from labeled data. The problem is that most of the real world data is not labeled.According to Yann LeCun, father of CNNs and professor at NYU, the next ‘big thing’ in artificial intelligence is semi-supervised learning — a type of machine learning task that makes use of unlabeled data for training — typically a small amount of labeled data with a large amount of unlabeled data. That is why recently a large research effort has been focused on unsupervised learning without leveraging a large amount of expensive supervision.“ The revolution will not be supervised” — Alyosha EfrosInspired on this concept, our class at NYU hosted a competition to design a semi-supervised algorithm from 512k unlabelled images and 64k labelled images from ImageNet.Semi-supervised learning is in fact the learning method mostly used by infants. When we are born, we don’t know how the world works: we don’t distinguish gravity, we don’t understand depth, or much less do we recognize human expressions. The human brain absorbs data mostly in an unsupervised or semi-supervised manner.For the competition, the method employed by our algorithm recognizes the 2d rotation that is applied to the input image [2]. In a way, this assimilates to how babies learn to see the world through experience. For example, through time babies get used to how objects stand and how mountains lie below the sky. As it turns out, even this simple task of image rotation allows the network to learn features that are relevant for supervised tasks.We used ResNet architecture (more specifically ResNet-50) for our final submission. ResNet models achieve state-of-the-art performance on many classification data sets. Moreover, ResNet residual units are invertible under certain conditions, which might help to preserve information from the early layers of the network obtained as a result of pre-training. We utilised a large set of unlabeled images to pre-train a ResNet-50 model on the rotation pre-task. The goal of the model is to predict one of the four rotation angles (0, 90, 180, 270) for each input image. As a result of training the model on this auxiliary task, we expected it to learn features that would be helpful in the main classification task.After pre-training, we trained a linear classifier on top of this feature extractor for the main task, and fine-tuned the whole network by gradually unfreezing the layers starting from the top. We also used data augmentation. This strategy led to a significant boost in accuracy as compared to the previous model.The figure below shows the training and validation loss curves, and accuracy for the final model over the course of training. Red line shows the moment of switching from training only linear classifier to fine-tuning the whole network.The figure below visualizes nearest neighbors for 4 random images in the validation dataset (first column). As it can be seen, semi-supervised rotation model clearly managed to learn meaningful representations of the images.The final top-1 validation accuracy was 32.4%. Overall, we can conclude that semi-supervised pretraining on the unlabeled data helped to improve accuracy on the main classification task. However, it is not clear how much more beneficial semi-supervised training is as compared to pre-training on existing large labeled datasets such as ImageNet.For more information, you may refer to our paper.This project was developed by Evgenii Nikitin, @sunor.yavuz, and ManriqueReferences[1] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In International Conference on Computer Vision (ICCV), 2015.[2] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. CoRR, abs/1803.07728, 2018.[3] Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible residual network: Backpropagation without storing activations. CoRR, abs/1707.04585, 2017.[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR, abs/1603.05027, 2016.[6] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. CoRR, abs/1901.09005, 2019.[7] Sebastian Ruder. An overview of gradient descent optimization algorithms. CoRR, abs/1609.04747, 2016.[8] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2087,"<strong class=""markup--strong markup--h3-strong"">Inference using EM algorithm</strong>",409,2019-02-11,https://towardsdatascience.com/inference-using-em-algorithm-d71cccb647bc?source=collection_archive---------10-----------------------,2019,"Introduction:Every couple of years there is an artist who seems to take the world by storm. In the past, this has been The Beatles and Michael Jackson, among others. These artists have the intrinsic ability to influence millions with their creative genius. It seems that when we started the second decade of the 21st century, a multitude of artists were jockeying to be number one. However, perhaps unexpectedly, a Toronto native by the name of Aubrey Graham ascended to the top under the stage name “Drake.”Drake’s original claim to fame was from his role on the popular teen sitcom “Degrassi: The Next Generation” in the early 2000s. However, Drake left the show when he figured he wanted to become a rapper. Lil Wayne, one of the most influential rappers at that time, made the Toronto Native his protege. After signing with Wayne’s record, Young Money Entertainment, Drake released his first Studio Album, So Far Gone. It was certified Platinum and expedited Drake’s rapid ascent to the top of the hip hop world. Over the course of the next eight years he dropped four additional studio albums, a mixtape, and a playlist, with Scorpion being his most recent release (source).We know for a fact that Drake’s work is popular but why are the majority of his songs such a hit? Is it the production? Is it the marketing? It is probably a combination of factors. However, the aspect I will be focusing on is his lyrics. Drake’s work is expansive and well-documented, so getting text data was not a difficult task. However, figuring out how to analyze it was. But thanks to recent improvements in NLP (Natural Language Processing), analyzing text data is now easier than ever.According to Wikipedia, Natural Language Processing (NLP) “ is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.” NLP is the most interesting field of machine learning in my opinion. Text is produced in so many different forms, its gives us so much data to work with.In the past 5–10 years, NLP has seen rapid development, coinciding directly with the rise of deep learning. Neural Nets have become a common framework for a myriad of NLP algorithms. Today, there are a diverse set of tools available so practitioners can solve a plethora of NLP problems. These tools have allowed me to examine Drake’s Lyrics.Sourcing the Lyrics:Before jumping into actual analysis, I had to get my hands on Drake’s Lyrics. Although there are several online lyric resources, I decided to use Genius.com. For those who are unaware, Genius is a website that annotates song lyrics. Genius has a wonderful API that is quite easy to use.Part IWhich Drake Song has the most unique words?One of the things that Drake often gets criticized for is his creativity -or lack thereof. In the past he has been accused of stealing other rapper’s flows and having ghost writers . I set out to see out to see if his critic’s complaints were warranted.The inspiration to use number of unique words per song was taken from thisbeautiful article that visualized the largest vocabularies in rap. In my opinion, total words is a subpar measure of creativity due to the repetitiveness of today’s artists.Once I finished cleaning the text data, I then began analyzing the unique number of lyrics in every song. Below is a histogram of the distribution of unique lyrics in all of Drake’s songs. It seems that the majority of his songs have between 100 and 200 unique words. Without a reference to the distribution of other artists songs, this histogram does not tell me much about Drake’s creativity.A better way to plot my findings was to look at his creativity by album. The plot, pictured below, was created in Tableau. On the x-axis is the name of the work. The y- axis represents the number of unique words. Each Bubble represents a song. No album seems to be substantially more creative (in terms of unique lyrics). However, it seems that every work has at least one outlier in terms of number of unique lyrics (minus Scorpion). It is kind of fascinating to see that songs on Scorpion, his most recent release, have little variation in number of unique lyrics despite being such a massive album (25 songs).Now, to answer the question, which song has the most unique lyrics? The answer seems to be 6PM in New York. The rest of the top 10 is below.Part IINamed Entity RecognitionNamed Entity Recognition is “a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.” (Wikipedia). NER is a particularly tricky task. The complexity of the English Language makes it very difficult to create a NER algorithm that is accurate for all sources of text. An algorithm may perform very well on one corpus (the set of Drake songs in our case) of text, and then perform poorly on another. This inconsistency makes it necessary to try out several NER algorithms. As you will see, algorithms are not very inaccurate.The first one I implemented was the Named Entity Algorithm provided by NLTK. “Ne_chunk” uses a list of words with Part Of Speech tags (POS tagging) assigned to them to infer what words are Named Entities. As you can see from the results I found, NLTK’s algorithm does not do a very good job on its own.The second Named Entity Algorithm that I tried was the one produced by Stanford. Stanford’s Computational Linguistics Department is arguably the most prestigious in the world. One of the multitude of impressive tools developed out of this esteemed department was their NER tool .Compared to NLTK’s algorithm, this tool takes a much longer amount of time to run, but it also produces much more accurate results. Although it is not perfect, it is a massive improvement.Part IIITopic Modeling:One of the most interesting disciplines within NLP is topic modeling. A topic model is “is a type of statistical model for discovering the abstract “topics” that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body.”(source) There are several prominent algorithms for topic modeling. Among the most prominent are Explicit Semantic Analysis and Non-Negative Matrix Factorization. However, the one I chose to use for the purpose of this article was Latent Dirichlet Allocation (LDA). LDA is a generative statistical model developed by Andrew Ng, Michael I. Jordan and David Blei. Basically it works by first learning the representation of a fixed number of topics in a given corpus. Given this number of topics, LDA learns the topic distribution that each document in a corpus has.Topic Modeling all Drake LyricsOne of the first things I wanted to use LDA for was to learn the most prominent topics in all of Drake’s songs. In order to accomplish this, I put all songs into a list. Then, using SciKitLearn’s CountVectorizer, I created a Bag Of Words Representation of all these songs. Bag of Words is a simple way of representing words through a matrix representation ( link). Then, using SciKit learn’s version of LDA, I fit a model with the objective of finding 8 topics within the given text.Visualizing the TopicsI found be two viable paths to visualize the LDA model. The first was through a function I wrote. Basically it outputs the most prominent words from every topic.The results here are interesting but only provide me a moderate amount of information. Clearly Topic 7 is different than Topic 2 but not enough information is provided to tell me how different they are.The auxiliary topics do not provide enough information to differentiate one from another. For this reason, I formulated another way to display topics within the text.Within Python, there is a wonderful library called pyLDAvis. It is a specialized library that uses D3 to aid in the visualization of the topics created by the LDA model. D3 is arguably the best visualization tool out there. However, it is for Javascript users. Having this plugin is very useful for someone who does not know JavaScript very well. To visualize the data, the library uses dimensionality reduction. Dimensionality reduction compresses a data set with many variables into a smaller amount of features. Dimensionality Reduction techniques are extremely useful for visualizing data as it can compress data into two features. For my particular visualization, I decided it was best to use T-SNE (T-Distributed Stochastic Neighbor Embedding) for dimensionality reduction.It seems from the fitting of my model, the majority of Drake’s lyrics can be classified into one massive topic that occupies the majority of the graph. The rest of the topics are diminutive by comparison.What did the topics looks like for all of Drake’s major releases?In order to do this, I followed the same steps as before, except ran the LDA algorithm to find exactly one topic for each album. I then used a function I defined earlier to display the most prominent words for all his major works.Conclusion:Drake is arguably the most popular artist in the world. By the time he decides to retire, he’ll be one of the most accomplished rappers ever. As a result, whenever he releases a new song or album, a palpable amount of buzz is certain to follow. His work almost always ends at the top of popularity charts. As a result, his lyrics instantaneously become staples of Instagram and Facebook captions for weeks. His songs are memorable, and his lyrics are major reason.In terms of my first NLP project, I would deem this to be a success. I felt like through the work done here, I got a more concrete understanding of Drake’s lyrics. Although there are certainly a bevy of other NLP tasks I could play around with for future work, Topic Modeling and Named Entity Recognition are a good starting point.Thanks for reading this! If you have any suggestions for future work or critiques of my work, feel free to comment or reach out to me.Something to note:I removed the expletives from the song lyrics to make this article more family-friendly.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1337,Outlier Detection with One-Class SVMs,987,2018-11-08,https://towardsdatascience.com/outlier-detection-with-one-class-svms-5403a1a1878c?source=collection_archive---------1-----------------------,2018,"Towards the end of completing my masters in data science, I started picturing myself doing clever things with machine learning and automated trading. If like me, you have run into the “how do I get historical free tick data” connundrum, then this post is for you.I have structured my post in three sections:Feel free to jump to the parts that interest you the most.As I mentioned before, this all started when I became interested in applying some machine learning (ML) to the problem of predicting either the market price or even simpler, the market direction. That is to say that I want to use ML to to analyse some historical data for a given currency pair and then tell me what the price will be at some specific point in the future. I would then use this predicted price to make a decision about how to invest my money. Since I only need to know whether the value of a target currency will increase or decrease, I could simplify my algorithm to simply predict the market direction (positive or negative). This is of course a gross over simplification of what is needed to trade profitably, but predicting a future state of the market accurately (better than 50% of the time) is the critical first step.Before I can build a model of the market using ML, I first need some data about the market. This data comes in a variety of forms. More often than not, data is freely avialable in what is known as “candle data” or time based “bars”. Candle data comes in time based increments (frequencies). This could be 1 min, 5 min, 1 day, monthly etc. Below is an example of day level candle data from FXCM:The output will look something like the following:Each row represents a candle of data for a single day for a given currency pair (USD/JPY in this case). What it is telling you is what the price was at the start of the candle, what the price was at the end of the candle, what the highest price was and the lowest price was during the candle window. It does this for both sides of the market, Bid and Ask.Together these values represent the boundaries of time and price value within which all price changes (known as ticks) occured. In addition to the candle boundaries, the data also includes a tick quantity value. This value represents the number of price changes that occured within the candle boundaries. It does not however tell you when the changes occured or how big or small the changes were.The image below is a visual representation of what a candle:By looking at the above diagram, the space between the open and close boundaries are specified by the candle size. In the USD/JPY example, this space represents an entire day. It could however be 1 minute, 1 hour or whatever the candle data supplier can produce for you.Essentially a candle is a sample of the raw underlying tick data combined with some descriptive statistical information about the sample it self (tick count). In fact one could argue that candle data is actually limited meta-data about what happened rather than the actual record of events.So whats wrong with candle data? clearly everyone loves it.. well first of all candle data is low resolution and does not describe the underlying price changes that produced the candle ‘box’. Secondly, standard candles use time based sampling which obscures the underlying events even further. Marcos Lopez De Prado explains the issues in detail in his book “Advances in Financial Machine Learning”.A simple argument against time based candles is that market activity does not occur uniformly throughout the day. As such an influx of price changes that produce opportunities to trade are obscured when they exist inside a candle. This in turn makes it impossible for a ML algorithm to learn anything about these times of volitality and thus wont be able to identify trading opportunities since the indicators required are not present in the time candle data.De Prado does make a case for alternative types of candle sampling, but to create these types of candle data sets, you need to have the underlying tick data. An example of alternative sampling is to sample the tick data based on a regular number of ticks. In other words create candles based on fixed volumes which remain constant regardless of how active the market is.Ultimately having easy access to raw data affords me the ability to do my own feature engineering by using a variety of dataset construction concepts specifically aimed at getting better machine learning results. Further more, I am keen to try some deep learning techniques on this kind of problem which demands large volumes of training data.Enough background.. the point here is to tell you the technical story of how to get your fingers on a decent amount of free tick data. I will walk you through how I did this along with the challenges I ran into and how to overcome them.I will show you how I scraped 2 years (2017 and 2018) worth of tick data for 21 currency pairs and loaded it into a hierarchical data file (HDF5). The data comes from FXCM and initially makes use of their RestAPI. I eventually abandoned their api and took a different approach.First and foremost here are the libraries I used during my first attempt to do this:FXCM provides a function called fxcmpy_tick_data_reader (assigned also here as tdr), which will return information about which currency pairs are available but also allow you to download a target currency.The output will look as follows:Counting the number of symbols shows there are 24, however there is some duplication going on here, so lets create a new list without any duplicates:As you can see we now only have 21 currency pairs:To get some of the data, you specify the start and end date of the range and then call the function asking for a target currency pair as shown below:As it turns out, the data is provided in 1 week blocks at a time regardless of whether you only wanted 1 day. So when you look at the URL, we can see that the above code has returned a gzip file for week 1 of 2017. If you change the dates and run the code yourself, you will see that the URLs follow the same pattern by simply changing either the year or week number based on the date range you have requested. If you request data for more than a week, it simply returns more gzip files.To look at the data you need to call some methods on the downloaded data which will extract the file and convert the contained csv to a pandas dataframe. Depending on which method you use, it will also update the index from type object to type DataTimeIndex.As you can see below the get_data() method has converted the index from object (aka str) to DatetimeIndex:In the case of get_raw_data() the index remains of type object`.When calling get_data() you will notice it takes significantly longer than get_raw_data() to complete. This is due to the index conversion from object to DatetimeIndex. Both get_data() and get_raw_data() produce a Pandas DataFrame, however get_data() updates the index to use the DateTime column as the index and converts it to type DatetimeIndex. I later discovered that Pandas does not do well when converting to DatetimeIndex when the microsecond field is included without a srftime format directive. See ""Why is pandas.to_datetime slow for non standard time format such as '2014/12/31'"" on stackoverflow for a more info.The bottom line here is that a srftime directive is needed to speed things up (for index conversion). I will come back to this later.You might be asking why having a DatetimeIndex matters?.. well its important given we want to be able to slice the data in terms of time later on. To do that the data should be loaded into the HDF5 datastore correctly from the start. We dont want to have to convert the index later.In order to automate the download, we need to supply the date ranges that will trigger the weekly files to be downloaded. To prevent the download from timing out, I decided to download the files one month at a time. To do this I created short lists which i refer to as periods that contain the date for each monday within that period.The script below figures out the correct date for every monday in the year and organises them in lists of 4 in a list (a period) and puts all the lists together in one big list of lists. Later on I use this list to automate the download.If my description seems confusing, the below sample output shows 2 short lists where each contains 4 Monday dates in sequence and leading from the previous list.Next I created a function that does the following:I also included a few print statements to help me keep track of what has downloaded, extracted and uploaded.I then use the function inside a small program that opens the HDF5 file, uses the ETL function and finally closes the HDF5 file.So after running the script, it became clear pretty quickly that this was going to take a while. When I say a while.. I mean days!!.. literally DAYS!!Obviously this isn't good enough and because I didn't know why it was going so slowly I just assumed this was an internet problem caused space monkeys. The slowness of the process caused all sorts of frustrating problems for me.After doing a bit of poking around I realised that space monkeys were not to blame. Instead the index conversion mentioned earlier, was the reason the whole thing was taking so long. I also discovered that the files I was downloading were gzip files that were around 5Mb each. Once extracted they would expand to around 80Mb or so.This presented some opportunities to speed this whole process up..After several attempts trying to download all the available tick data and failing due to timeouts, file corruptions and having to restart.. I decided to take a different aproach.As mentioned before the gzip files are only a few megabytes each. I decided to first scrape all the files and store them in a directory that I would then extract later. This way I would avoid having to re-download the files each time something goes wrong.Libraries you’ll need for version 2:If its not clear in the code, what I’m doing is construct urls that follow the FXCM format and then I used the requests library to download each file at the constructed url. To ensure each file downloads properly, I decided to stream the file downloads and write the data to disk in chunks. Below is the scraping script:Even though the files are only about 5–10 mb in size each, it is worth noting that there are over 2000 of them and downloading them all did take a while. You might find yourself walking away to eat some toast or to catch a show on Netflix.When you get back to your computer, you might want to check you downloaded all the files for each available currency. You could painstakingly check the files manually or… you can use the code below to check them for you:Assuming you downloaded everything successfully, the above script output should look something like this:So now that you have all the files downloaded, the next step is to extract and load them into our database (HDF5 file). At this point I had not yet solved the slow index update issue yet. As such I created batch folders by currency pair so that I could supervise the extract, transformation and loading (ETL) of the data into the database without being stuck at my computer for days at a time. Each batch was taking about 3–4 hours to complete.You won’t need to create batches by the end of this post, but if for some reason you wanted to, you can use the following script.It creates a folder for each currency, then finds the files that belong to the folder by matching the first 6 characters to the folder name it belongs to, and then copies the file into the destination folder. By the end, you will have 21 folders containing 104 files each.The goal now is to unpack the gzip files, transform the data into the correct format and then load it into the HDF5 database. If you are comfortable reading code, simply skip to the end, but if you’re interested in the tale of how it all came together, keep reading..So now that I decided to abandon the FXCM api to unpack these files, I needed to extract them manually. I couldnt use the tick_data_reader() function anymore because it calls the web server looking for the file associated with the date I'm requesting. All the files are on my hard drive now so I needed to write my own extraction code.Extracting the data proved a bit more challenging than I expected due to codec issues. In the end, I looked up the FXCM api source code to find the function (see from line 181) that does the extraction. I adapted the code a little and I was off to the races.Next I ran into a more obscure issue. My script would run fine until it encoutered data where the Datetime milisecond field didnt contain the required 6 decimal places.Numbers that rounded to less than six places would not include the zeros, which in every other circumstance would make perfect sense. For example .123000 would instead appear as .123. Unfortunately when you specifiy the srftime directive (format=""%m/%d/%Y %H:%M:%S.%f""), this breaks your code.Fortunately there is a wonderful method dataframe.DateTime.str.pad() that allows you to pad strings with whatever character or number you want. So all I needed to do was to pad the “DateTime” column with zeros where the length of the string did not match the required 26 characters. Even better is that it only took a single line of code to achieve.Next was the rather painfully slow issue of converting the index from type object to type DatetimeIndex.After posing the question to some old classmates, one of them (thanks Alex) suggested it might have something to do with the fact that I was asking pd.to_datetime() to infer the format. In the Pandas documentation it specifically mentions that inferring the datetime format can potentially speed up the conversion by up to 10 times. Since I was dealing with about 2 billion rows of data, I was grateful for any speedup I could get.As it turns out though, inferring the datetime format does not actually play well when it includes microseconds. See ""Why is pandas.to_datetime slow for non standard time format such as '2014/12/31'"" on stackoverflow for more info (thanks for sharing the link Davide).So armed with this tip I tried specifying the format with a srftime format directive for example: format=""%m/%d/%Y %H:%M:%S.%f"". This made a massive difference to the execution time. Using the ""infer"" specification took minutes to convert just one file. By specifying the format, the conversion literally took just a few seconds!!.Unfortunately my troubles were not over yet. Halfway through converting and loading the “EURCHF” files, my code broke again. Turns out that not all files follow the same datetime formatting. That is to say that some files format their dates as Month/Day/Year while others might be Day/Month/Year or even Year/Month/Day.To deal with this issue I decided to sample the first and last records in the dataset, knowing that each file contains 5 business days worth of data. I then used a for loop and some nested if statements to figure out what format I was dealing with for each row in a given file, before converting the index using the correct srftime directive. This part of the script allowed me to extract and load all the files (2184) in less than 2 hours, whereas before it was 3–5 hours just to load 104.Here is the code for converting the index to Dtaetimeindex:Last but not least is to load the data into the database. Fortunately this is actually quite easy. PyTables makes this step super simple. All thats required is:Thourghout my script you will notice I am printing some summary information. You dont need to do this. I was using this as a means of keeping track of where the script was up to and to have a sense where in the data things broke.Last but not least since I was doing my ETL in batches. I wanted some kind of audible notification once a batch had completed, so I borrowed some code from realpython.com that builds a 440Hz sine wave from scratch and plays it for 2 seconds. I just used it as-is at the end of my script.So as promised here is all the code you need to:IMPORTANT: Remember to update the directory locations for your environment before you run the code.Part 1 — Scrape the data & check the downloadPart 2 — Extract, Transform and Load the files into a HDF5 data store.I Hope you found this post useful. Next time I’ll talk/demonstrate some of the fun that can be had with this much tick data.Thanks for reading.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
1232,Python PyPI stats in BigQuery: Reclustered,47,2018-11-02,https://towardsdatascience.com/python-pypi-stats-in-bigquery-reclustered-d80e583e1bfe?source=collection_archive---------27-----------------------,2018,"TL;DR: at the bottom.I have just returned from the International Conference on Learning Representations (ICLR) 2019 in New Orleans and what a fruitful year that was on GAN papers. In the first section, I discuss the themes featuring papers on image synthesis (BigGAN), audio (WaveGAN), feature selection (KnockoffGAN), 3D, text & tabular and many other! The second part of this article is then focused on more practical ML considerations.Before going to ICLR, I made a list of all the talks & workshops that had something that I wanted to learn. This meant a very busy Monday — where at one point four interesting workshops were running in parallel (more on the workshops in the “Applied ML” section). It also meant a busy Tuesday where the organizers put 37 GAN papers into the day. This meant starting the poster session early and finishing late. I have kept track of all this using a spreadsheet.I have included links to all the papers I mention and there are even links to the livestreamed workshops as well as the plenary session, which also heavily featured GANs.Here I want to explore the changes specifically just discussing Generative Adversarial Networks (GANs). As many have said, this is an exciting new technology that — unlike most of other ML — has only been around for less than 5 years. In the spirit of the previous ICML 2018 article, I talked to academics so you don’t have to, but given the volume of the content, it is no longer possible to go through every paper, so I will just pick some main themes.Ian Goodfellow frequently talks of how deep learning revolutions in 2012, enabled a “Cambrian explosion” of machine learning applications. This is because in any technical field the first order of business is to make a technology work reliably and that enables a whole wealth of downstream applications.This has somewhat happened with image synthesis. Now that BigGAN can reliably generate very diverse high-fidelity images, we can start thinking about applying it for other use-cases. One example is using BigGAN as a way to augment the existing training data (i.e. artificially increasing the number of data points by synthesizing new ones). Now even though there was another paper accepted at ICLR that showed the limitations of this technique. It seems that in this case of a balanced dataset, the GAN data augmentation has likely limited impact on the downstream task. But the sheer fact that this is a proposal that is seriously studied seems like a good sign and still leaves many data-augmentation avenues unexplored.Another downstream task that we may care about is image synthesis with fewer labels. In the original BigGAN, we are using all labels in ImageNet to synthesize the 1,000 types of objects. However in another ICLR paper, we can see equally high quality pictures with just 10% of the labels and even better results than BigGAN with just 20% by using self and semi-supervised learning.Furthermore, ICLR featured several papers that had interesting proposals to achieve more granular control over the generated images. So now that giraffe you always wanted in your photos instead of your ex can be just in the right spot.I am just amazed at how quickly the field is moving that in less than 5 years since the original paper, we have managed to produce 1000 classes of 512x512 images that are realistic enough to be used in downstream applications. In the words of Károly Zsolnai-Fehér, what a time to be alive!Another substantial theme in this year’s ICLR was the presence of more “exotic” data types and applications. I’ll just go through a couple of the more interesting ones. To me, this again seems somewhat indicative of the growing maturity of GANs as a field.As always, there were many papers dealing with some aspect of training (rejection sampling, relativistic GAN, variational discriminator bottleneck) or some theoretical property of generative models (e.g. latent space interpolations or invertibility of GANs).While academics tend to love this area, at ICML ’18, the results were somewhat mixed. I felt that many papers introduced a huge amount of extra complexity to derive some properties that I did not think are hugely interesting or do not expect them to become the de facto standard the same way e.g. Wasserstein GAN or gradient penalties are.Fortunately, at ICLR that was not the case. All three techniques from above plus averaging during training all look like simple, effective techniques that could easily become the standard pattern for the future state of the art.As someone who still frequently has to worry about how I am going to productionize the systems I am building. I was very pleasantly surprised that even the workshop organizers from ICLR thought this was important. So I was trying to capture all the interesting content from the following workshops:Overall, I am constantly amazed at the rate of progress of ML and academic conferences have their disadvantages, but if you plan & prepare accordingly, you will get much more of them than any other conference I have ever been to.TL;DR:Thanks Dr. Daniel Duma and Harpal Singh for their excellent feedback.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
450,Object Detection with Deep Learning on Aerial Imagery,117,2018-06-22,https://towardsdatascience.com/object-detection-with-deep-learning-on-aerial-imagery-c6aa7a554a59?source=collection_archive---------4-----------------------,2018,"Social Media Monitoring can be tough, especially when the number of channels, profiles, and posts to keep an eye on grows. In many companies, data collection is a job typically done either by interns or specialized external service providers. Small and Medium-sized Enterprises as well as private individuals, like bloggers and influencers, often need to do it manually by copy & pasting relevant data from social media to Excel sheets.This is boring.If you ever had to do some task like that manually, you know how exhausting and uninspiring it can be. That’s why we invented computers as the modern-day slaves.I am currently working on a academic paper, my Master thesis. For that I needed to create a database of roughly half a Million Instagram posts and 100,000 Instagram profiles. Doing that manually would have meant to tripple my total time at university (which is large enough anyways…). Obviously, that was not an option for plenty of reasons.Naturally, I thought about automating the task and wrote some R Scripts to do the work for me. And since there are many people out there with a need for automation, I decided to share my databaseBuilder.R Script as part of the growing InstaCrawlR package.The newest member of the InstaCrawlR family, databaseCreator.R, is an R script that automatically creates a database of Instagram posts and profiles. It takes a list of post URLs as an input and crawls Instagram for various Post Meta Data as well as Profile Meta Data.Post Meta Data:Based on any Instagram post, the script extracts the username and crawls data on the author’s Instagram Profile.Profile Meta Data:Finally, the script combines these data to one big table that can be exported as a csv file for further analyses in Excel or Google Spreadsheets.Once the database is built it can be used for all sorts of analyses. Here are some examples.Social Media Marketing: You have a list of a couple of influencers and you want to analyze their performance comparably, e.g., using the engagement-per-follower or follower-following ratio.Social PR Monitoring: What do Instagrammers say about your company or brand? Find out by using InstaCrawlR’s other scripts to crawl most recent posts that feature the hashtag of your brand (e.g., #nike) and then build a database using databaseCreator for these posts. What other brands are mentioned? Do users say positive things about your business? Is there a relationship between number of followers and post sentiment?Competitive Analyses: It doesn’t matter if you are an Influencer or working for a business — you want to know your performance and compare it to how your competitors are doing. Just take a couple of links to your posts and to those of competitors as an input and let databaseCreator do its job. Once the output is loaded in Excel, do your usually analyses and visualization.This is an important point, especially for researchers and small businesses. Since the script basically does what an intern would do manually (copy&pasting parts of HTML code), it is hard for Instagram to differentiate between whether it is dealing with a person or a computer.I optimized the code to minimize the number of server requests and built in a logic to wait a couple of seconds when Instagram Server returns an HTTP 429 (too many requests). The script will process roughly 80,000 — 100,000 posts in 24h, depending on how strict Instagram’s servers are calibrated.You don’t need to do anything once the script is started.I let mine run at nights or when I was out with friends. You just gotta love technology for that! However, I still recommend to take a look at it from time to time and to check the outputs, just to be sure.InstaCrawlR is a collection of R scripts that can be used to crawl public Instagram data without the need to have access to the official API. Its functionality is limited compared to what would be possible using the official API. However, it seems to be the only option for private individuals and small businesses to gather and analyze Instagram data.InstaCrawlR consist of five scripts — jsonReader, hashtagExtractor, graphCreator, g2gephi, and databaseCreator — which are described in the instruction PDF. InstaCrawlR can be used to download, analyze, and visualize the most recent posts for any specific hashtag that can be found on Instagram’s Explore page (instagram.com/explore/tags/HASHTAG/) as well as to build your own database of post and profile meta data.Here is an example: https://medium.com/@jonas.schroeder1991/social-network-analysis-of-related-hashtags-on-instagram-using-instacrawlr-46c397cb3dbeFurther scripts will may be included as part of the InstaCrawlR family in the future. You can find all current scripts and instructions on GitHub.Feel free to connect with me on LinkedIn, Twitter, ResearchGate, or Academia if you have any questions about the scripts, suggestions of any kind, or just want to talk about data science in marketing.Thanks for reading,Jonas SchröderUniversity of MannheimPS: If you’re not a programmer but still want to use the script, feel free to write me on LinkedIn to talk about a scalable server solution, custom scripts, and general consultancy.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
3677,Artificial Intelligence: Digging for Value,2,2019-09-02,https://towardsdatascience.com/a-i-primer-what-why-and-how-of-the-revolution-shaping-the-modern-economy-3e805e30ec46?source=collection_archive---------23-----------------------,2019,"I remember how excited I was the very first time I delved into the realm of Machine Learning. And the hype is understandable, what software engineering student wouldn’t want to jump into one of the most exciting and relevant technologies of today?But as my interest grew, and I began to educate myself on the subject, I was often scared off by how technical some of the articles I read were. Fantastic articles, no doubt, but written from too advanced a perspective, even the ones aimed at beginners. On the other hand, so many were overly-pragmatic, neglecting theoretical explanations in favour of getting new learner’s hands dirty as soon as possible. And while both are valuable, I feel that, as a fellow student, there is a gap I can fill here.My goal is to amalgamate all the information I wish I’d had at my disposal when I started, outline the theory behind this simple Machine Learning algorithm, then give a thoroughly-explained practical example, in an accessible but comprehensive way. One student to another.So, welcome to the article I wish I could have read when I built my first Linear Regression model.Regression analysis is a set of statistical processes whereby we estimate the relationship between a dependent variable (y) for one or more given independent variables (x). In the context of Machine Learning, it is a subfield of supervised learning.There are several types of Regression, each describing a different mathematical relationship between the independent and dependent variables. Some common examples include Polynomial, Logistic and, the topic of this article, Linear.But how do you choose one? What’s the difference? Well, as I said above, it depends on the data. Here is an example: Say, for instance, we wish to predict the progression of a disease as it sweeps through a population and gradually dies out. Naturally, as the number of days increases, so shall the number of cases — until they begin to fall, resulting in a parabolic shape. As you can see below, a straight line of best fit does not accurately predict the number of cases by day 100. A Polynomial Regression does though. But we’ll go into that in the next article.Conversely, when we have data that moves in a trend as shown below, a straight line fits relatively accurately. This is a Linear Regression:So, Linear Regression is used when the relationship between the dependent and independent variables can be modelled quite accurately as a straight line.This will be our line of best fit, and you may remember its equation from high school:Where:So, how do we find the equation of the line of best fit? By adjusting a set of parameters (W0 and W1) until we find their respective values that make the sum of the squared residuals (the difference between the actual and predicted values) of the model as small as possible.Let’s go over some critical terminology before moving on. It’s easy to get the terms confused, but understanding these metrics is crucial for determining a model’s reliability.Essentially, variance is a measure of how inaccurate our line of best fit is, and is quantified by the R² score. Our goal is to make the variance as small as possible, so the higher our R² score, the better.Sometimes called the Cost function, it is used to express variance as the coefficient of determination R² of the prediction, which ranges from 0 to 1, with 1 being a perfect fit.The average of the square of the errors (we square them so there are no negative values). The larger the number, the greater the error. Our goal is to minimise this.We will use the Ordinary Least Squares method, which is the simple, analytical and non-iterative solution. If we wanted to apply a more complex Machine Learning algorithm, say Support Vector Machine, then we’d need to use Gradient Descent, which would give us an approximation of the OLS solution, done iteratively. But that is a topic for another article.So, with the above functions, we train our model until it learns the optimal coefficients that minimise the sum of the squared residuals. Once we’ve trained our model on some data (say, the first 80% of the dataset), we’ll test it on the rest of the data (the other 20%).Let’s start at the very beginning, imports:Next, we load the dataset and create an object dx. The diabetes dataset comes with Scikit-Learn and consists of 10 physiological variables (age, sex, weight, blood pressure etc.) measured on 442 patients and an indication of the disease progression after one year. The goal is to predict disease progression from physiological variables.Now, the Scikit-Learn datasets return something called a Bunch which is similar to a dictionary. This Bunch has various attributes, one of which is data. This is the data matrix, which we wish to use. Another is target, which we will come to shortly. But we don’t need all the data, so we select the features we want, and use numpy.newaxis to increase the array dimensionality from 1 to 2. We have now turned our array into a column vector.If that step was a little confusing, that’s alright. The point is, we now have a 2D array containing the data, which is the necessary format. You really could achieve this with any dataset (custom lists or a .csv file) where you have data points with x and y values. So now ours looks something like this:Next, we split our dataset into training and testing sets — a fundamental part of Machine Learning. You will notice the .target attribute that I mentioned earlier. These are essentially the correct values, or response variables.At this point, a scatter plot would be helpful. Just by looking at it, we may be able to infer whether Linear Regression will provide an accurate model. I’ll add some styling with rcParams to make it look a little more appealing, but don’t worry about that.As you can probably tell, it does look as though a straight line could predict more-or-less where this trend is heading.Now comes the interesting part. We will create an object lr for the Linear Regression, and fit the data to it.All we have left to do is plot the line of best fit over the scatter plot:Congratulations! You have successfully trained and tested a Linear Regression model.But we can’t pat ourselves on the back just yet…At this stage, I feel we should go more in-depth. It is imperative we understand exactly what is going on under the hood.The LinearRegression() class is where the good stuff happens. This is where the linear model lr is fitted with coefficients that minimise the residual sum of squares between the predicted and target values, as I mentioned earlier.This class contains the .fit() function, which we can see being applied to the Linear Regression object lr. We pass the training data (x and y values) in as arguments and the function returns an instance of the object, now fitted with the data.Finally, we see .predict(), another function of the LinearRegression() class. This is the function that returns the predicted values, by calculating the equation for the line of best fit.The best way to understand these functions is to rewrite our program without them.This is where the Ordinary Least Squares algorithm begins. The first thing we’ll need to do is find the gradient m and the y-intercept c of our line of best fit. Here are their respective formulas:We use numpy.mean to find the mean μ. I’ve implemented these two formulas as a single function:Note that now we don’t have to change our array to be 2D like before, because we aren’t using the .fit() function anymore. So amend the line where we previously used numpy.newaxis to look like this:Now when we plot the line of best fit, instead of using the .predict() function, we actually input our equation for the line of best fit, mx + c, as the y value.Congratulations for real this time! You just wrote a Linear Regression algorithm from scratch. Hopefully you now have a thorough understanding of the algorithm and its relevant functions.As a bonus, let’s calculate the mean squared error and score (the coefficient of determination R², as defined earlier, of the prediction) of our model.Using the LinearRegression() class:Not using the class: The coefficient R² is defined as (1 — u/v), where u is the residual sum of squares ((y_true — y_pred) ** 2).sum() and v is the total sum of squares ((y_true — y_true.mean()) ** 2).sum():The answers come to mse = 2548.07 and R² = 0.47.There you have it, a thorough introduction to Machine Learning’s simplest algorithm, Linear Regression. I hope that, as a student myself, I was able to explain the concepts in a relatable and comprehensive manner.Just to recap what we covered:If you found the article helpful, I’d love to engage with you! Follow me on Instagram for more Machine Learning, Software Engineering and Entrepreneurial content.Happy coding!Scikit Learn linear_model.LinearRegression() documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predictScikit Learn Linear Regression example: https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.htmlScikit Learn load_diabetes documentation: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetesScikit Learn Introduction to Machine Learning: https://scikit-learn.org/stable/tutorial/basic/tutorial.htmlReal Python Linear Regression: https://realpython.com/linear-regression-in-python/#simple-linear-regressionStatisticsbyjim Interpreting R²: https://statisticsbyjim.com/regression/interpret-r-squared-regression/BMC Mean squared error & R²: https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
3388,Robustly optimized BERT Pretraining Approaches,6,2019-08-05,https://towardsdatascience.com/robustly-optimized-bert-pretraining-approaches-537dc66522dd?source=collection_archive---------26-----------------------,2019,"I co-host a popular AI Ethics Podcast entitled “The Radical AI Podcast.” In our first month and a half since launch, we received nearly 5000 unique downloads and trended quite highly on the ‘New and Noteworthy’ list for technology podcasts on iTunes. Because of the amazing support of the AI Ethics community on Twitter we were able to interview some of the most influential academics and industry leaders in the global AI Ethics space.We had begun as a small project with a simple mission “To create an engaging, professional, educational and accessible platform centering marginalized or otherwise radical voices in industry and the academy for dialogue, collaboration, and debate to co-create the field of Artificial Intelligence Ethics.” We expected to get only a few dozen listens and had prepared as such. I had employed a fellow Ph.D. student to create our first logo, expecting only a few folks to see it. The logo worked for a small project but my co-host and I quickly realized that we needed a new logo that represented our vision for the future of the project and the communities we were trying to uplift.We employed a graphic designer that my co-host had worked with before and set out to vision a new logo. What started as a quick project escalated soon into a deep learning experience for us about technology, industry, graphic design, and our values. In the spirit of vulnerability and education, I want to share with you all 3 lessons I learned through a month-long process of logo creation for my own popular podcast.The title of the podcast “Radical AI” first emerged in a bar in Boulder, Colorado. I had just met my soon-to-be co-host about two weeks prior in Barcelona during a conference about AI Fairness, Accountability, and Transparency (FAT*/FAccT 2020) and we had made a follow-up meeting to discuss our research and connect. As we sat down and grabbed a beer it was clear that not only did we get along quite well but we also had the same frustrations with how AI Ethics is highlighted in conferences and in the media; ie. that it is dominated by already well-known white male scholars.Based on our experiences we knew that the folks that have historically been highlighted in AI Ethics spaces, and especially in AI Ethics podcasts, represent only the tip of the iceberg of the incredible people in the academy and industry doing groundbreaking AI Ethics work. To put it crudely, we had a shared frustration that even in an industry with ethics in its name the stories of folks with historically marginalized identities (ie. women, black folks, people of color, and more) were still being buried so egregiously. I ended the conversation saying off-handed “well, I’m thinking about starting a podcast that is… for lack of a better word, more radical about AI Ethics.”About a week later we met up again to plan and “The Radical AI Podcast” was born. The problem was that we still were not entirely sure what the term ‘radical’ meant in the context we were working in. Coming from a community organizing and slam poetry background from when I lived in New York City I had images of what I meant by ‘radical’ but it was difficult to come up with the language. The project adapted to both be an interview space to lift up amazing scholars and also a space for us to collect a range of definitions for what ‘radical AI’ meant in the first place.As we began to interview folks we were contacted by a group in California that had already begun organizing under the banner “Radical AI.” We met with them to not only discuss possible partnership but also to see if we had similar definitions of what ‘radical AI’ could mean. Their definition is as follows: Radical AI begins with a shared understanding that society distributes power unevenly — pushing People of Color, Black, Indigenous, Womxn, Queer, Poor, Disabled, and many other communities to the margins. Growing from these roots, Radical AI examines how AI rearranges power and critically engages with the radical hope that our communities can dream up different human/AI systems that help put power back in the hands of the people.Using this definition as a starting point we began to plan our logo design. We wanted the design to be inclusive, representative of the people and values we wanted to lift up, and embody our call for power redistribution. Unfortunately for us, this starting point did not take into account all the sheer number of connotations ‘Radical’ might have and we were about to make a misstep that we would soon regret.We wanted a queer punk androgynous robot with a mohawk. You can’t make this stuff up. Jess and I are both white, straight, and able-bodied. And our first move in designing a logo was to tell our graphic designer to model off of a character in the as-of-yet still unreleased Cyberpunk 3030. Let’s say this right out of the gate: this was a huge blunder. We own that.Our hearts were surely in the right place. The thought process was ‘well, if we are trying to uplift identities outside of the status quo then lets do it boldly and center those identities in our logo.’ Unfortunately, as is the case with many roads to hell, this one was paved with good intentions. At first we could not see that not only were we narrowly defining the identities that might fit into a ‘radical’ bubble and thereby being reductionist, we were also creating branding off of identities that neither of us represented.Thankfully, before getting too close to launching that iteration of the logo we spoke with several of our friends and mentors in the AI Ethics space. We could tell very quickly that they were not pleased with the direction we had taken. They were helpful and honest in pointing out our blind spots. In particular they were helpful in separating the concept of ‘radical’ from any one group or set of identities to instead grounding it in our original definition rooted in power and shared values.Without meaning to we had let our hidden biases and assumptions about identity groupings get in the way of creating a logo that was more truly inclusive. And so we went back to the drawing board.We worked with our designer to return to our original definition of Radical AI and the values that underpinned it. From there we identified metaphors that represented what we see as our fundamental project as a podcast. We came up with phrases like ‘an offering,’ ‘planting seeds,’ and ‘uprooting abusive systems of power.’ We then came up with symbols that summarized those metaphors, finally landing on: a hand, soil, and a sapling. As we worked towards a final image our goals were threefold:1. For the image to help listeners to be able to have space to interpret and define ‘radical AI’ for themselves.2. For the logo to represent resilience and hope.3. For the logo to be simple, not prescriptive or overly complex.After a few weeks of iterations we came up with the following logo, which is now the logo that is live for Radical AI:What we like about this logo is that it is simple, represents our values, and creates a feeling of movement, resilience and hope. Though we remain humble and open to feedback from our community, thus far we have received positive feedback and are happy with where this project has ended up.Often times in the tech sector we forget these three lessons: that language matters, that identity representation matters, and that sometimes (read: often) the simple solution is that best solution to complex problems.Through trying, failing, and learning from mistakes in this logo design process I was reminded of just how vital it is that those of us who design and consume tech keep these lessons in mind.Ethics is, at its core, about how we live our values out into the world. This includes how we create and consume technology. For those of us in design positions or positions in which we are creating public brands it is imperative that we remain vigilant about the language we use, the biases we might bring into our work unconsciously, the downstream impacts of the images we share, and the level of complexity we bring to solutions that might be better served by simplicity.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
787,Why learn Object-Role Modelling?,10,2018-08-29,https://towardsdatascience.com/why-learn-object-role-modelling-4eebd93dcda2?source=collection_archive---------11-----------------------,2018,"Data Science lacks a proper definition of the field leading to buzzwords and weird mixed of demanding skills.In this post, we light the way to get the best Data Science profile for your business needs. Especially, the article seeks to help companies that can not afford to have a huge DS Team and want to optimize their investment.I am lucky enough to work with a great team of Data Scientists from different countries and different backgrounds. We are all Data Scientist though, with different mindset and specializations.Mathematicians, physicists, statisticians, economists and computer scientists. My team is composed of these different backgrounds, with different level academic degrees: bachelors, MSc and PhDs.Okay, so then… how do I find the right Data Scientist? Let’s first try to define them.Every Data Scientist should be analytical. However, the Pure Analytical Data Scientist is a numbers cruncher. Linear Algebra and Statistics are the core of its solutions.Typical backgrounds for this kind of Data Scientist are mathematics, statistics, physicists and even economics. Frequently, these profiles are proficient with numbers but less experienced with complex software and data pipelines. But, give them the data and the problem and they will decode it. Or even better, let them define the problem!When do I need a Pure Analytical Data Scientist? You may require this kind of profile when the core solution of your problem is based on statistics and maths. Commonly, the software requirements of your project should be either met already by another team or very light.Real-world examples:So what an Analytical Data Scientist deliver? Complex analysis in the form of reports, conclusions and suggestions to decisions.What tools do they normally use? R, SAS, Python, MATLAB, Excel, etc..When you should not look for this profile? Online end-to-end models, Big Data (when your data does not fit in RAM), your company does not need to solve challenging analytical solutions, etc.The Data Scientist that likes to work close to IT. These profiles are Unix users and do not like complex GUIs. They even know the basics of Docker!Typical backgrounds are Computer Science, Electrical Engineering, Information Systems, etc. So, what to expect from them? They are capable to be integrated into an IT team. Therefore, this means that they can develop end-to-end Machine Learning models or data systems along with the IT and/or BI team. You should not try to leave these guys aside IT.Real-world examples:So what a Hacker Data Scientist deliver? End-to-end prototype APIs, Machine Learning Models, Python libraries, etc.What tools do they normally use? Python (scikit-learn, pandas, Keras, seaborn, Flask, etc.), PySpark, SQL (basic), Unix systems, Docker, Cloud Computing (AWS/GCP/Azure), Git, etc.When you do not need this profile? When your company uses Windows, the candidate may immediately give up. When you do not deliver online systems, when technology and innovation is not a priority, etc.Finally, we could not forget the Research Data Scientist. These profiles are the ones working in laboratory environments and far from production systems. They are the ones building the new state of the art algorithms that will disrupt the industry. They publish papers in important conferences, spend time reading papers and probably hold a PhD or high academical degrees.Some examples of well-known companies with these kinds of profiles are Google DeepMind, Zalando AI, OpenAI, etc. In these cases, the job offers might not come by the name of Data Scientist but rather the specialization they are looking for. However, not only top companies with strong Research Teams can have these specialized roles, your company might want them too!Let’s put some examples where a company would be interested in this kind of profile:So what a Research Data Scientist deliver? State of the art algorithms and solutions, conference papers, research, etc.What tools do they normally use? It really depends on the case. However, most scientific libraries are in Python and MATLAB. Deep Learning low-level frameworks such as TensorFlow, PyTorch, etc. are well-known in the AI community.When you do not need this profile? When you want a solution that can be solved cheaper and easier with Amazon Rekognition, Google Vision API or other SaaS providers. And, obviously, when you do not want to invest in pure innovation and research.These Data Scientist Profiles are not rigid and disjoint definitions. In most of the cases, you can find Data Scientists that are some kind of mixture between these types. However, depending on how deep or wide your requirements are you may look for a more generalist profile or a more specialized profile. Also, one must take these real-world examples as just that, examples. A Data Scientist should be domain agnostic as long as you have a problem that can be formulated as a mathematical one.If what you would like to have is someone who is able to crack everything at once then I suggest you read my other article: https://medium.com/@joelqv8/the-data-scientist-unicorn-8c86cb712ddeThis last profile, the Data Scientist Unicorn, never fails to disappoint you.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
5226,How to create an interactive booklist with automatic Amazon affiliate links in R,2,2019-12-26,https://towardsdatascience.com/how-to-create-an-interactive-booklist-with-automatic-amazon-affiliate-links-in-r-7c1304dec43?source=collection_archive---------27-----------------------,2019,"Introduction:Python is a popular language that allows programmers to write elegant, easy-to-write and read code like plain English. The unique feature of Python is a different type of comprehensions.In Python, there are three types of comprehensions viz. List, Dictionary and Set.By the end of this blog, you’ll understand the full power of Python comprehensions and how to easily use its functionality.List: List is a collection of data surrounded by square brackets and each element are separated by a comma.List Comprehension: Is also surrounded by square brackets but instead of the list of elements inside it contain expression like for loop &-or followed by if-clauses.Example:a. Create a List of the square of numbers between 1 to 100.b. List comprehension with the condition.c. List comprehension — Mathematical Applications.Explanation:The above will produce the same result but when we use list comprehension the line of codes get reduced and the same operation is done with a single line of code.2. DICTIONARY COMPREHENSIONDictionary: A dictionary is a collection which is unordered, changeable and indexed. In Python dictionary written with curly brackets, and they have key and value pairs.DictionaryExample = {“IDE”: “JupyterNotebook”, “Language”: “Python”, “Version”: 3}Dictionary Comprehension: A dictionary comprehension is also a collection which is unordered, changeable and indexed, where Key-Value pairs generated with help of expression.Example of Dictionary Comprehension:a. Create dictionary where Key as Alphabets and Value as the number of times alphabets occurred in the sentence (Dictionary definition).b. Dictionary Comprehension — Mathematical Application3. SET COMPREHENSIONSet: In Python Set is unique, unordered, mutable collection of elements. Set where elements enclosed with curly brackets and elements separated by a comma.SetExample = {‘Python’,’Java’,’R’}Set Comprehension: Simillar to list comprehension but consist of a collection of unique, unordered, mutable elements, where dictionary elements inside it contain an expression.Example of Set Comprehension:a. Create Set consist squares of a number between 1 to 10.b. Create Set of Unique elements available in the list.Advantages of Comprehensions:Disadvantages of Comprehensions:Conclusion:As discussed above, different types of comprehensions available in Python. Above implemented are basic examples expressions used in comprehensions can be used to call the functions. Most often, list and dictionary comprehension use to simplify the code.Hope, this blog helps you understand some effective and easy-to-use Python techniques.Thank you for reading!!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
170,Working on Your Dashboard Layout?,713,2018-02-17,https://towardsdatascience.com/working-on-your-dashboard-layout-9b7c38d7b61e?source=collection_archive---------2-----------------------,2018,"These days it seems like everyone wants a Data Scientist, and for a good reason. Harvard University labeled the profession “the sexiest job of the 21st century.” And according to LinkedIn, the career has seen an exponential growth becoming the second fastest growing profession. But many companies are still facing obstacles when it comes to utilizing their data correctly, and some aren’t even sure of what to do with their data science teams.You’ve set your foundation for data analytics, implemented artificial intelligence algorithms, and stored your data. Now what? Without the correct personnel, you’ll just be sitting on this mine of data. And even if you have hired a whole team of data scientists, not given the proper tools and responsibilities, you’re wasting your investment.As Natalie Kortum, Senior Data Science Leader at Quadratic for The Richards Group mentioned, data science technology allows for a greater capture of data and an efficiency in being able to analyze it, so more can be monitored and addressed. If you want to evolve your data strategy and optimize your data, there’s a few components to consider. We recently caught up with our expert panel of data scientists beforeour next Data Science Salon event. We’ve summarized some of their insights in this article to offer you a clearer concept on how to optimize your data scientist’s role and what using AI, machine learning solutions and deep learning can do for your business.As Viswanath Puttagunta, Chief Technology Officer of Divergence.AI put it, general education and awareness of data science principles & project management from leadership still has long way to go. Data science projects can be agile but leadership must provide a clear direction (like to improve reward redemption rates or improve customer experience) or else their data science teams will spend a lot of time on the small details while missing the big picture.These sentiments are shared by Hila Lamm, Chief Strategy Officer at Firefly.AI, who pointed out that data science is still very much a mixture of research and art which often keeps it in the sidelines of the corporate world. In reality, it should be an established practice in every enterprise. Data scientists implement best practices and tools that provide measurable and predictable machine learning solutions, so that executives will feel confident incorporating machine learning as part of their regular product development or BI processes. She adds that there is a huge gap between the big-picture approach great data scientists need to apply to solving real business problems and the tedious tasks they end up spending their time on due to mismanagement by higher-ups.Lacking in dedicated data scientist resources, many industries haven’t found automated alternatives and aren’t embracing the full potential of data science. Organizations might understand their data as a strategic asset, but often fail to make the jump to using it as actionable analytics. Meltem Ballan, Data Science and Analytics Manager at Ernst and Young, concured that sector leaders and advocates who can come together to define the best practices, standards and training rules would make the best use of their data scientist. To overcome many of these barriers, companies need to stop viewing data and innovation as cost-center functions and start viewing them as potentially business transformative capabilities. Viswanath Puttagunta said that leadership and finance is still heavily CAP-ex biased but to leverage what the cloud offers today in being nimble, it needs to be OP-ex biased.To align your company’s goals, efforts must be made to drive innovation throughout the company and define internal best practices. You can also aide your team by hosting trainings to increase data literacy and promote data culture to identify innovative ideas. This will give your data scientists optimal opportunities to implement machine learning and AI to collect and dissect your data without wasting efforts or investments.If you aren’t making the most of your data, you could wind up sinking resources and invaluable time into misguided efforts. In other words, if you’re just collecting and storing data from everywhere, you could miss out on potential clients, insights and revenue. To institute a data culture that respects and utilizes data science and a model driven approach at your organization, you need a true data scientist to lead the way. They deeply understand existing tools and techniques and they can better communicate their value to the organization. What makes a successful data scientist is one who can identify pertinent questions and interpret the results of your machine learned data to craft the right solutions. Think of data scientists as the translators between data and your company.SK Reddy, the Chief Product Officer at Hexagon, mentioned that one of the most desirable characteristics in a data scientist is critical thinking although many lack this quality. Adrian Antico, Data Science Manager of Arizona Public Services, said that many companies ignore this skill when it’s time to promote individuals to leadership roles. This can lead to problems down the line when it comes to ROI expectations. It’s important to make sure that your data scientist is trained and exposed to this in order to better serve your wider company goals in relation to marketing and sales trends. Viswanath Puttagunta added: they must also have emotional intelligence as that can be a true obstacle for any data scientist when it comes to communication. Having business intelligence is the engine that will drive your company, but it’s impossible to accomplish without a trained professional available to gather the relevant data and present it in a digestible manner.Hila Lamm mentioned that there is a tremendous amount of work involved in preparing data, tuning algorithms and preprocessing parameters. Looking at it from the outside, it’s difficult to estimate how very much of their time would be spent on those tasks that need to be automated. That’s why you need to be sure that once you’ve hired your data scientist — or if you already have — that they’re given the correct tools and resources.So what are the right tools and resources? KDNuggets conducts a massive yearly data science tools survey, some popular tools among data scientists according to this survey include: Plotly, Domino Data Lab, Data Robot, RStudio, KNIME, Tensorflow, and many others. Python surpassed R as the most popular programing language among data scientists last year. Our experts also offered some suggestions for what a data scientist needs to excel in any field and implement best practices when collecting and demonstrating the right data.Adrian Antico told us he uses H20 for most modeling tasks and R for data management, visualization, and other modeling tasks not handled by H20. He uses Python for modeling tasks not covered well by R and R package vignettes to get a deeper understanding of methodology details useful for the projects he works on. Meltem Ballan said that as a data scientist she tries to stay domain agnostic. However, R is a very powerful statistics tool and Python is her go-to tool when working on ML and AI projects because it has a great capacity to analyze complex data like images. Viswanath Puttagunta shared these sentiments but added that he directly models in Spark-Scala with Spark and that R + H2O is another powerful combination.Another important area to look at is external reading and keeping up with trends. A professional data scientist should constantly be researching and learning the newest concepts in the data science field, a vast area that’s constantly changing. Meltem Ballan suggested investing into O’Reilly books, OpenCV, Hadoop, and Elasticsearch as it helps her to improve her skills and are necessary readings for any serious data scientist. Hila Lamm’s CTO, Erez Sali, recommends “Machine learning, a probabilistic perspective” by Kevin Murphy and “Outlier Analysis 2nd ed. 2017 Edition” by Charu C. Aggarwal as essential readings for understanding data science. SK Reddy and Natalie Kortum agreed that attending local conferences, like DSS, reading data science blogs like KDNuggets and technical papers are all a must to stay updated on the latest trends.They say that in life, beauty is pain. There certainly is a lot of beauty in data, but cultivating insights doesn’t have to be painful. With the right tools, resources, and best practices in the hands of the right team, organizations can turn the sexiest job of the 21st century into their most valuable asset of today.Upcoming Events | Data Science Salon NYC | DSS MIA | DSS SEA | DSS LA | DSS ATXWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
492,A guide to an efficient way to build neural network architectures- Part II: Hyper-parameter selection and tuning for…,1600,2018-05-07,https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7?source=collection_archive---------0-----------------------,2018,"Suppose you have hundreds of articles/sentences. You want to know what topics each of those articles/sentences talk about. An article describing allegations made on a pharmaceutical company may talk about topics like Government, Medicine or Business. Our goal is to assign these topics to documents.One of the methods to perform this task is Probabilistic Latent Semantic Analysis (PLSA).PLSA or Probabilistic Latent Semantic Analysis is a technique used to model information under a probabilistic framework. Latent because the topics are treated as latent or hidden variables.PLSA can be understood in two different ways.The first one helps you understand the mathematics of PLSA very well. While the second method is easy to implement in Python.Let’s formally define the variables that appear in PLSA.We have three sets of variablesHaving specified the naming convention, let’s take a look at the first method.As mentioned earlier, the topics are hidden variables. The only thing we see are the words and the set of documents. In this framework, we relate the hidden variables with the observed variables.The way we associate z with (d,w) is that we describe a generative process where we choose a document, then a topic, then a word. Formally,If it is not completely clear, don’t worry. I have mathematically put everything ahead. Before diving into the equations, let’s discuss two assumptions the model makes.Assumption 1 — Bag of Words: As we discussed earlier, words ordering in the vocabulary doesn’t matter. More precisely, the joint variable (d,w) is independently sampled.Assumption — Conditional Independence: One key assumption we make is that the words and the documents are conditionally independent. Focus on the word conditionally. This means —P(w,d|z) = P(w|z)*P(d|z) — (3)The model under the above stated discussion can be specified as follows —Now,Using conditional independence,Using Bayes Rule,The parameters in the model are —The above parameters are determined using Expectation Maximization or the EM algorithm for the Likelihood function.Likelihood function —Log Likelihood —We won’t go into how the EM algorithm works. Main purpose of this discussion was to familiarize you with how the topics are related to the observed variables, words and the documents. If you want read about it’s math, you can find a relevant paper here.An alternative way to represent PLSA is Matrix Factorization Model.Consider a document-word matrix of dimensions N*M, where N is the number of documents and M is the size of the vocabulary. The elements of the matrix are counts of the occurences of a word in a document. If a word wi occurs once in the document dj, then element (j,i) = 1.If you think of this matrix, most of the elements are 0. Say we have a document of 10 words and a vocabulary of 1000 words. Naturally, 990 elements of the row will have the value 0. Such a matrix is called a Sparse Matrix.What Matrix Factorization does is it breaks this matrix (let’s call it A) into lower dimension matrices (Singular Value Decomposition)The dimensions of L, U and R are N*K, K*K and K*M respectively.Matrix U is a diagonal matrix with values as the square root of the eigenvalues of AA*, where * denotes transpose. For any given k , you choose the first k rows of L, the first k elements of U and the first k columns of R. And remember, k is the number of topics we set.This model is not very different from the Latent Variable Model. The three matrices can be interpreted as —So if you multiply the three matrices, you actually do what the below equation says —Note that the elements of these three matrices cannot be negative as they represent probabilities. Hence, the A matrix is decomposed using Non-Negative Matrix Factorization.If you wish to apply the algorithm in Python, you can find an example here.Thanks for reading! If you liked this blog, you might want to check another piece of mine on Word Embeddings and Word2Vec.Feel free to share your views. Would love to connect with you on LinkedInWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3252,Debating the AI Safety Debate,51,2019-07-24,https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d?source=collection_archive---------22-----------------------,2019,"Machine learning could be used to bypass and dismantle cyber-security systems faster than most prevention and detection tools can keep up. AI will exacerbate existing threats and create new ones, but its speed could prove a great boon for cybercriminals, as it is much more effective at fighting them than human experts. The algorithm is attempted to model a decision mechanism that resembles real human decision mechanisms but is modeled by algorithms.In the context of cybersecurity, artificial intelligence (AI) tries to defend the system by weighing patterns of behavior that indicate a threat against predictive logic. Machine learning (ML) is the process by which AI learns patterns that lead to bad behavior. This happens when an AI system or neural network is tricked into incorrectly identifying or intentionally modifying the input.In the last 12–18 months, the number of applications of artificial intelligence (AI) in cybersecurity has increased massively and shows no signs of slowing down any time soon. This is partly due to the rising costs of developing and adapting the technology as it declines. The use of artificial intelligence in cybersecurity creates new threats to digital security. While AI technology can be used to more accurately identify and stop cyberattacks, cybercriminals also use AI systems to carry out more complex attacks.This means that artificial intelligence can be used to create more complex, adaptable, and malicious software, such as machine learning and artificial neural networks (AI).A high-tech bridge that provides web and mobile security testing services as AI is deployed in cybersecurity. The company’s AI-based product, FortiWeb, is a web application firewall that uses machine learning and artificial neural networks (AI) to accurately detect threats. It works with human intelligence to identify other vulnerabilities and risks without returning false positives.Vectra’s Cognito platform uses AI to detect real-time cyber attacks and how they are used in cybersecurity. Palo Alto Networks is a heavyweight when it comes to cybersecurity providers working with companies such as IBM, Cisco, Microsoft, Google, IBM Research, and many others.Just like a new employee, the company’s Virtual Analyst Platform takes on the task of studying corporate cybersecurity protocols to make an accurate assessment. Combined with Vectra’s proprietary Cognito platform and a range of other tools and services, it automates tasks normally performed by security analysts and significantly reduces the workload required to conduct threat investigations. PatternEx, an artificially intelligent cybersecurity tool, detects patterns of malicious behavior in real-time and how it deploys AI in cybersecurity.Spark Cognitions unveiled a new, powerful, and sophisticated anti-malware system powered by its technology called Deep Armor. The system protects the network from new and unique cyber threats by combining it with antivirus programs to find and remove malicious files. Based on the range achieved by the two components, they can be found interesting and communicate through sharing.DarkTrace, a British company, manufactures defense systems that replicate human antibodies to automatically detect and neutralize cyber threats without human intervention. By applying learning models to the network, AI systems can be trained to test various defense strategies to minimize or stop the spread of malware during a cyber-attack by controlling the infection and eliminating the root cause.Indeed, Darktrace has benefited from a growing interest in self-learning cyber-defense products that use artificial intelligence to detect and even respond to emerging attacks. Faced with a constant stream of threats from cybercriminals, hackers, and other malicious actors, it is almost impossible for anyone to keep up with any form of automation or artificial intelligence.In the future, response speed will enable a faster and more efficient response to cyber attacks, enabling human experts to undertake more complex investigations. There is a reciprocal process, as analysing AI responses can enable people to improve their understanding and readiness to deal with cyber threats. When AI can observe successful responses to a cyberattack, it becomes self-healing and dynamically replicates the best defense strategies developed by human analysts.Cited SourcesWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
5281,Let me recall you this: accuracy isn’t everything in Machine Learning.,2,2019-12-30,https://towardsdatascience.com/let-me-recall-you-this-accuracy-isnt-everything-in-machine-learning-8e3a84ce0a8?source=collection_archive---------40-----------------------,2019,"So you already know the basics of regular expressions, or regex, in Python. Things like how to use character sets, meta characters, quantifiers, and capture groups are the basic building blocks, but you are a power user, never satisfied with just the basics. Your text wrangling problems are more intricate than you could ever hope to solve with just those tools. Lucky for you there are more regex concepts in Python to learn. It’s anchors away for more tools for text wrangling goodness!Not sure about the basics? Check out my piece on the building blocks of regular expressions, or regex, in Python.Before we can set sail on the SS Regular Expressions, we need to discuss the anchor. More specifically, text anchors. A text anchor says to look for matches either at the beginning or end of a string. In Python, there are 2 types of anchors:As a reminder, to use regex in Python, you need to import the re module. The re.findall() function is particularly useful when experimenting with new regex topics such as anchors. It will return a list containing a vector of the actual values of the matches in the string. Make sure to load the re module before you get started.To set sail, we must raise the anchor at the beginning of the trip. When working with text data, you may need to match a regex pattern, but only if it appears as the first thing in the string. To do that, we also use an anchor, specifically ^.To demonstrate, our goal is to find the word “the,” but only if it appears at the beginning of a string.Starting with anchor, when we use the ^ anchor to find “the,” we have only one instance of it returned.We know this is the first instance because “The” at the beginning of the string is capitalized. Now with anchor_n, no results are returned. The regex would normally match “the” in the sentence, but with the ^ anchor it is only checking the beginning of the sentence.At the end of our regex trip, we need to drop the anchor. Sometimes you need to match a regex pattern only if it appears at the end of a string. This is accomplished with the $ anchor.Let’s take another look at the anchor string, this time looking for “ocean” at the end of the string. We will have one result, “ocean.”Again, if we look at anchor_n, this time using the $ or end anchor, we will get no matches, even though “ocean” appears in the string. If it isn’t at the end of the string, the $ anchor won’t pick it up.Now that we know how to match strings at the beginning and end of strings (raising and lowering the SS Regular Expressions’ anchor), we can move on to the next concept: telling regex what not to match.Imagine for a moment you are the captain of a large ship and it is the maiden voyage of a ship. Let’s call this ship The Titanic. As the captain of that ship, you probably are more focused about not hitting an iceberg than anything else specific.In strings, you may want to specify certain patterns to avoid. To do this, use negations. These will match anything EXCEPT what you specify. There are two main ways to handle them in Python:In my previous piece on regular expressions in Python, we covered three different meta characters: \s, \w, and \d. As a refresher, they match a certain set of characters in a string: \s matches whitespace characters (spaces, tabs, and newlines), \w matches alphanumeric characters (letters and numbers), and \d matches any digits (numbers).When you capitalize any of these meta characters, it will match everything EXCEPT what it would normally match. To see them in action, let’s create a new string that has spaces, numbers, letters, and punctuation:Now, when we take a look at our capitalized meta characters, we will see how their output is changed.Some use cases require negation with more flexibility than meta characters can provide. Let’s look at an example:Let’s break down this problem. There are 21 consonants in the alphabet, and we want all of them. They are non consecutive, so we can’t just use a range of letters in a character set to get them. We do know that vowels and consonants are mutually exclusive, and that there are only 5 vowels. If we can find all of the vowels, we should be able to negate it to get all of the consonants by negation. Let’s start with a refresher of how to find vowels with a character set.The characters inside the brackets define the character set. In this case, any of the vowels are matched. To get everything except vowels, we can use the ^. This will negate everything inside the character set.…But that isn’t the consonants. There is a little bit of work to do still. We have spaces, digits, and punctuation in our results. Since everything is negated in the capture group, we simply match everything we don’t want in the result. \s takes care of spaces, \d takes care of digits, and \. takes care of the period. \. is not a meta character, but an escaped period.After all of the characters we needed to end up with just consonants, maybe that isn’t quicker than typing 21 different consonants, but it does demonstrate how we can add everything together and use regular expressions to get exactly what we want.When out to sea, we may have something that we are keeping an eye out for. Seeing some wildlife would make for a more interesting journey. We look for the wildlife so that we can find somewhere close to it for our ship. As a seasoned captain, we don’t really care as much about the wildlife itself, just the safe space nearby to navigate the ship to.Similar to how we want to look by the wildlife for a place to park, we may want to look around some text for the information we really want. In case you haven’t figured it out by now, regular expressions can handle this with ease. To do so, we use something called a look around. Look arounds search for a specific match in the string, then return something either immediately before or after it. This leaves us with two main types of look arounds: look aheads and look behinds.Let’s use the following string as an example for how both of these work.We’ll have two scenarios for what information we want to extract from this string.At first you may wonder why we need look arounds to do this. You could simply look for a digit, period, and two more digits. Something like this:That isn’t a bad start, but the output of that line of code would give you three prices.There are only two animal prices, the last one is unrelated. Look arounds will help us remove that last one and get everything nicely into a data frame.Looking at the lookaround string, we see that each animal price is preceded by the word “costs.” We will use a look behind. This will match whatever we would normally match, but only if it has a match of something else before it. The general formula for a look ahead is ""(?<=if preceded by this)match_this"". In our case, this would translate like below (\s is added to account for the space between the word and the numbers):Now we have the prices for each of our animals, excluding the amount of money I have left in the string.If you look closely, you’ll notice that there are space in the strings. Don’t worry about those for now, we will take care of those when we build a data frame to hold our price list.Now that we have the prices for our animals from the string, we want to get the names of the animals for our price list data frame.Do to this, we now need to match the word directly ahead of the word “costs.” For this we will use look aheads. The basic formula for a look ahead is as follows: ""match this(?=if followed by this)"". To get the animal names from our string, it would look like this:Just like that, we have captured every word that comes before the word costs. In our string that is the names of all the animals.To create the data frame for our animal prices, we already have most of what we need. We just create a data frame with the vectors from animals and prices as the columns. Of course to do this in Python we need to import the pandas module as pd first.In case you forgot, when we use re.findall(), it returns a list of strings. This can easily be used to create a dictionary and then a data frame.Our next step is to take care of those extra spaces. We will use the .strip() method. It removes and spaces at the beginning or end of a string. We will use a list comprehension to do this in a single line of code for each column.The resulting data frame looks like the one below:Just like that, your cruise on the SS Regular Expressions has come to an end. You learned:Here are some other resources related to regular expression in Python that you may find helpful:As always, let me know if you enjoyed the content. Don’t like how I use regex? Tell me about it in the comments. Either way, subscribe so you get notified every time I post new content!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
1173,Topic modelling with PLSA,445,2018-10-06,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41?source=collection_archive---------3-----------------------,2018,"As a lover of delicious food and a frequent reddit surfer, the subreddit r/foodporn (a reddit forum for posting pretty pictures of food) seemed like a great place for me to grab some data for an unsupervised learning project using Natural Language Processing (NLP).Unsupervised learning is when we do not know the underlying structure to our data beforehand and are trying to uncover such a structure. The opposite is supervised learning, where we do know the structure to our data beforehand in the form of labels. Typically, unsupervised learning involves grouping/clustering data points based on their similarity to one another.My goal for this project was to surface any underlying structure to titles on r/foodporn. Going into this project, I had the hunch that if we cluster titles together, we’d see a natural structure in the form of cuisines. I.e, we’d observe a Japanese food cluster, an Italian food cluster, and so on.I used the pushshift API to grab 48,000 posts from the subreddit. Specifically, I extracted the top 1000 posts per month for the last 48 months. Here is an example of one of those posts:As I was only analyzing post titles, my data for the above example is the text “Smoked Prosciutto, Goat Cheese, Baby Arugula, Tomato & Balsamic Glaze Panini”.I started by cleaning my data. This involved making all words lowercase, removing punctuation, emojis, and numbers, and filtering advertisements and non-english posts.After cleaning and filtering, I had 42,234 titles to play with.I’m going to introduce some jargon here to put everything in the context of NLP. Whenever I say “document”, I am referencing an individual instance of an r/foodporn title. When I say “corpus”, I am referencing the entire collection of “documents” (the entire collection of my titles).In NLP, we have to convert all documents into vectors of numbers so we can run statistical models on them. There are many methods to do this. My method of choice (also one of the most popular methods) is TF-IDF (term frequency — inverse document frequency). Essentially, TF-IDF looks at all the words in a given document in the context of all unique words across the corpus. If a term appears many times in a document, it is considered important. But if a term appears many times across the corpus, this negates its importance.Take “bacon”, for example. The word “bacon” appears often across all titles, so if we are looking at a specific title and see the word “bacon”, it doesn’t really make this document unique. TF-IDF gives “bacon” a low number. TF-IDF would give a rarer word, such as “sashimi” a higher number. Since “sashimi” is a rarer word, it would differentiate a title more than “bacon” would. You can read more about TF-IDF here.The output of TF-IDF per document is a vector with a length equal to the number of unique words in the corpus. In my case, there are 20,857 unique words. Therefore, running TF-IDF on my entire corpus results in a 42,234 by 20,857 matrix (number of documents by number of unique words).With my documents vectorized in this form, I was finally ready to group them! But what should the ideal number of groups be? To find out, I clustered my data (using K-Means) repeatedly, increasing my number of clusters and plotting the silhouette score associated with each number of clusters along the way. At a high level, a silhouette score is a measurement that takes into account cohesion and separation of clusters. The denser individual clusters are and the further apart they are from one another, the better. A low silhouette score reflects strong clustering.As expected, the Silhouette score drops as I increased the number of clusters. But it looks like we start to hit a point of diminishing returns around 6 clusters, so I chose 6 to be my ideal number of groups.Finally, I used a dimensionality reduction technique known as NMF (Non-Negative Matrix Factorization) to partition my 42,234 titles into 6 groups (groups are formally referred to as “topics” in the context of dimensionality reduction with NLP projects). NMF’s inner workings are out of the scope of this blog post, but you can read more about NMF here. NMF allows us to reduce the dimensions of our matrix by going from a word-space to a topic-space. In this situation, we compress a 20,857-element long word-space into a 6-element long topic-space. We lose some information during this process, but gain increased interpretability.By modeling 6 topics, I am expecting to see each topic correspond to a cuisine.Ok enough with all this crazy data science voodoo. Let’s see some results!Here are the top 10 most important words for each of the 6 topics:After I modeled my topics, I inspected them to see if they made sense in the context of the problem I am trying to solve.It looks like topic 1 is a series of classic American lunch ingredients, with an emphasis on fat and cheese. I named this topic Burger.If you inspect topic 2, it is obviously related to pizza/Italian food. I named this one Pizza. It’s nice how specific this topic is!Topic 3 is a little all over the place. It’s tough to tell what the essence of this topic is from its top 10 words alone, so I looked at the top 30 and saw words such as masala, turmeric, wonton, and noodles. I named it Asian Fusion.Topic 4 is distinctly a Dessert topic. Like the Pizza topic, I love how obvious this one is.I named Topic 5 Breakfast, though an argument can be made that its essence is more about healthy-sandwich-type ingredients.Finally, I named Topic 6 American Entree. This topic is rather broad compared to the other ones.Let’s use T-SNE to visualize how the titles fall into different topics:Interesting. There is strong separation for the Pizza, Burger, and Asian Fusion topics (pink, red, and blue). The American Entree, Dessert, and Breakfast topics (yellow, green, and purple) all have some areas where they are distinct, but also intermingle significantly in the largest “blob”. This means that these topics aren’t well separated from one another.I observed this to be anecdotally true after creating a classifier on top of these topics. My classifier will take a string as input and convert it to a 6-length vector of probabilities, corresponding to the 6 topics here. Whichever topic has the highest probability is the output of the classifier. You can try it out yourself here!You’ll notice that the classifier makes some good predictions and some bad ones. For example, it’ll classify “Rhubarb Pie”, “Chicken chow mein”, and “Flatbread slice with anchovies and mozzarella” correctly, but it thinks that “lobster mac and cheese” is a burger and that “penne” is asian fusion.Contrary to my expectations, r/foodporn title topics didn’t fall neatly into cuisines, but they do reflect an underlying structure to the posts. Some groups are easily distinguishable, whereas others intermingle a lot.The intermingling between some topics reflects that people are posting titles that span multiple topics, corresponding to foods that aren’t easy to categorize. Given the amount of cuisine fusion and experimentation going on in the food-world today, this makes a lot of sense. Dishes today are harder to categorize than ever before because they are increasingly made up of ingredients from around the world.The topics that don’t intermingle also make sense. Dessert is so distinct because people aren’t mixing “chocolate” with “ribs”, “pie” with “barbecue”, and “cookies” with “onions”. Similarly, there are things that make Pizza and Burger distinctly themselves.Thanks for reading my post and I hope you found it interesting! Feel free to reach out with any questions/comments.Project Code: GithubProject App: Cuisine ClassifierMore about me: LinkedInWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
883,Announcing fast.ai part 1 now available as Kaggle Kernels,1000,2018-08-10,https://towardsdatascience.com/announcing-fast-ai-part-1-now-available-as-kaggle-kernels-8ef4ca3b9ce6?source=collection_archive---------9-----------------------,2018,"Decision trees are the most important elements of a Random Forest. They are capable of fitting complex data sets while allowing the user to see how a decision was taken. While searching the web I was unable to find one clear article that could easily describe them, so here I am writing about what I have learned so far. It’s important to note, a single decision tree is not a very good predictor; however, by creating an ensemble of them (a forest) and collecting their predictions, one of the most powerful machine learning tools can be obtained — the so called Random Forest.Make sure you have installed pandas and scikit-learn on your machine. If you haven't, you can learn how to do so here.Let’s start by creating decision tree using the iris flower data set. The iris data set contains four features, three classes of flowers, and 150 samples.Features: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)Classes: setosa, versicolor, virginicaNumerically, setosa flowers are identified by zero, versicolor by one, and virginica by two.For simplicity, we will train our decision tree using all features and setting the depth to two.Of course we still do not know how this tree classifies samples, so let’s visualize this tree by first creating a dot file using Scikit-Learn export_graphviz module and then processing it with graphviz.This will create a file named tree.dot that needs to be processed on graphviz. Here is a YouTube tutorial that shows you how to process such a file with graphviz. The end result should be similar to the one shown in Figure-1; however, a different tree might sprout even if the training data is the same!A single decision tree is the classic example of a type of classifier known as a white box. The predictions made by a white box classifier can easily be understood. Here is an excellent article about black and white box classifiers.In Figure-1, you can see that each box contains several characteristics. Let’s start by describing the content of the top most node, most commonly referred to as the root node. The root node is at a depth of zero, see Figure-2. A node is a point along the decision tree where a question is asked. This action divides the data into smaller subsets.petal length (cm) <=2.45: The first question the decision tree ask is if the petal length is less than 2.45. Based on the result, it either follows the true or the false path.gini = 0.667: The gini score is a metric that quantifies the purity of the node/leaf (more about leaves in a bit). A gini score greater than zero implies that samples contained within that node belong to different classes. A gini score of zero means that the node is pure, that within that node only a single class of samples exist. You can find out more about impurity measures here. Notice that we have a gini score greater than zero; therefore, we know that the samples contained within the root node belong to different classes.samples = 150: Since the iris flower data set contains 150 samples, this value is set to 150.value = [50, 50, 50]: The value list tells you how many samples at the given node fall into each category. The first element of the list shows the number of samples that belong to the setosa class, the second element of the list shows the number of samples that belong to the versicolor class, and the third element in the list shows the number of samples that belong to the virginica class. Notice how this node is not a pure one since different types of classes are contained within the same node. We knew this already from the gini score, but it’s nice to actually see it.class = setosa: The class value shows the prediction a given node will make and it can be determined from the value list. Whichever class occurs the most within the node will be selected as the class value. If the decision tree were to end at the root node, it would predict that all 150 samples belonged to the setosa class. Of course this makes no sense, since there is an equal number of samples for each class. It seems to me that the decision tree is programmed to choose the first class on the list if there is an equal number of samples for each class.To determine which feature to use to make the first split — that is, to make the root node — the algorithm chooses a feature and makes a split. It then looks at the subsets and measures their impurity using the gini score. It does this for multiple thresholds and determines that the best split for the given feature is the one that produces the purest subsets. This is repeated for all the features in the training set. Ultimately, the root node is determined by the feature that produces a split with purest subsets. Once the root node is decided, the tree is grown to a depth of one. The same process is repeated for the other nodes in the tree.Suppose we have a flower with petal_length = 1 and petal_width = 3. If we follow the logic of the decision tree shown on Figure-1, we will see that we will end up in the orange box. In Figure-1, if the question a node asks turns out to be true (false), we will move to the left (right). The orange box is at a depth of one, see Figure-2. Since there is nothing growing out of this box, we will refer to it as a leaf node. Notice the resemblance this has to an actual tree, see Figure-3. Moreover, note that the gini score is zero — which makes it a pure leaf. The total number of samples is 50. Out of the 50 samples that end up on the orange leaf node, we can see that all of them belong to the setosa class, see the value list for this leaf. Therefore, the tree will predict that the sample is a setosa flower.Let us pick a more interesting sample. For instance, petal_length = 2.60 and petal_width = 1.2 . We start at the root node which asks whether the petal length is less than 2.45. This is false; therefore we move to the internal node on the right, where the gini score is 0.5 and the total number of samples is 100. This internal node at a depth of one will ask the question “Is the petal width less than 1.75?” In our case, this is true, so we move to the left and end up in the green colored leaf node which is at a depth of 2. The decision tree will predict that this sample is a versicolor flower. You can see that this is most likely the case because 49 out of the 54 samples that end up in the green leaf node were versicolor flowers, see the value list for this leaf.Now that we know how our decision tree works, let us make predictions. The input should be in a list and ordered as [sepal length, sepal width, petal length, petal width] where the sepal length and sepal width wont affect the predictions made by the decision tree shown in Figure-1; therefore, we will can assign them an arbitrary value.The output should be:If you take a look at the parameters the DecisionTreeClassifier can take, you might be surprised so, let’s look at some of them.criterion : This parameter determines how the impurity of a split will be measured. The default value is “gini” but you can also use “entropy” as a metric for impurity.splitter: This is how the decision tree searches the features for a split. The default value is set to “best”. That is, for each node, the algorithm considers all the features and chooses the best split. If you decide to set the splitter parameter to “random,” then a random subset of features will be considered. The split will then be made by the best feature within the random subset. The size of the random subset is determined by the max_features parameter. This is partly where a Random Forest gets its name.max_depth: This determines the maximum depth of the tree. In our case, we use a depth of two to make our decision tree. The default value is set to none. This will often result in over-fitted decision trees. The depth parameter is one of the ways in which we can regularize the tree, or limit the way it grows to prevent over-fitting. In Figure-4, you can see what happens if you don’t set the depth of the tree — pure madness!min_samples_split: The minimum number of samples a node must contain in order to consider splitting. The default value is two. You can use this parameter to regularize your tree.min_samples_leaf: The minimum number of samples needed to be considered a leaf node. The default value is set to one. Use this parameter to limit the growth of the tree.max_features: The number of features to consider when looking for the best split. If this value is not set, the decision tree will consider all features available to make the best split. Depending on your application, it’s often a good idea to tune this parameter. Here is an article that recommends how to set max_features.For syntax purposes, lets set some of these parameters:Now you know how create a decision tree using Scikit-learn. More importantly, you should be able to visualize it and understand how it classifies samples. It’s important to note that one needs to limit the liberty of a decision tree. There are several parameters that can regularized a tree. By default, the max_depth is set to none. Therefore, a tree will grow fully, which often results in over-fitting. Moreover, a single decision tree is not a very powerful predictor.The real power of decision trees unfolds more so when cultivating many of them — while limiting the way they grow — and collecting their individual predictions to form a final conclusion. In other words, you grow a forest, and if your forest is random in nature, using the concept of bagging and with splitter = ""random"", we call this a Random Forest. Many of the parameters used in Scikit-Learn Random Forest are the same ones explained in this article. So it’s a good idea to understand what a single decision tree is and how it works, before moving on to using the big guns.You can find me in LinkedIn or visit my personal blog.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
4867,Let’s Start Talking About Our Intellectual Insecurities,6,2019-12-02,https://towardsdatascience.com/lets-start-talking-about-our-intellectual-insecurities-db83ca200909?source=collection_archive---------34-----------------------,2019,"Music is a powerful language to express our feelings and in many cases is used as a therapy to deal with tough moments in our lives. The different sounds, rhythms, and effects used in music are capable to modify our emotions for a moment, but there’s a component that sometimes goes unnoticed when we are listening to music; The Lyrics of the songs.Lyrics are powerful texts who share the ideas that came from the mind of the author when the song was been created. That’s why I decided to analyze the lyrics of one of my favorite bands; Metallica.Metallica has had a noticeable change of concepts and ideas on their song lyrics throughout their music career and considering they started playing music in the ’80s until now, this band is a good option to study.In this article, I will expose and explain how I could achieve this idea using Word Clouds, a Statistics Table, Frequency Comparision Plot of Words, VADER Sentiment Analysis, and a cool Dataset provided by Genius. So with no more to say, let’s start working!.Full code, scripts, notebooks, and data are on my Github Repository(click here)The first step is to obtain information on the most popular songs by the artist. to do that, I created a function called search_data() that helps to automatize the process to collect the attributes of each song. This function uses the library lyricsgenius to obtain the data and you must pass the parameters of the artist name, max number of songs to extract, and your client access token:As you may notice, the lyric column has a lot of words and symbols that are not important to study because they are used to explain the structure of the song, so I cleaned this information using the function clean_data() and also creating a new column to group songs by decade. This new column will help us to have a better understanding when analyzing the data. Finally, I filtered the information to just use songs that have lyrics because some artists have instrumental songs.Now we have a clean data frame to start creating our data frame of words. You can access the CSV file of this data clicking here.To have a complete analysis of Metallica lyrics I wanted to take a look at how they tend to use words on different decades. So I had to create a Dataframe of words based on the lyrics of each song. To do that, first I considered unique words by lyrics due to some of the songs repeat the same words on the chorus part. I defined a function called unique to do this process, the parameter corresponds to a list of wordsThen I use the following code to store the unique words of lyrics with the function defined above and another function called lyrics_to_words that you can find on the helpers script. I saved this information on a new column called words on the data frame of lyrics.As you may notice now we have a column that stores the unique words for each song used in lyrics.But this is the first step to create our data frame of words. The next step is to use this new words column, count how many times a unique word is used on songs lyrics by decade, and store all these results into a new data frame of 5 columns, one for words and the others for the frequency of occurrence by decade.It’s important to consider remove your own stopwords depending on each data in case the clean function does not remove all of them. Stopwords are natural language words that have very little meaning, such as “and”, “the”, “a”, “an”, and similar words.you can access to look the code clicking here.This Data Frame is interesting and useful because it shows us how many times Metallica used a word on the lyrics of their songs depending on the decade the song was released. For instance, the word young was used in 1 song in the 1980s, 2 songs in the 1990s, and 0 times in the 2000s, and 2010s.You can access the CSV file of this data clicking here.To start analyzing the words used by Metallica to create their song lyrics, I wanted to answer a lot of questions that I had in mind. These questions are:Cited by Google a Word Cloud is “an image composed of words used in a particular text or subject, in which the size of each word indicates its frequency or importance”. For this purpose, the Word Cloud is grouped by decade and will show us which are the most frequent words used in song lyrics of Metallica during the different decades.I used the libraries Matplotlib and Wordcloud to create this graph with a function where you must pass the data frame, the number of rows, and columns of the figure depending on the decades you want to plot. In my case, I have 4 decades (80s, 90s, 00s, 10s) and I want the graph in a 2x2 format.It’s cool to observe the differences among the words used during the different decades of Metallica musical career. During the 80s words are focused on concepts related yo death and life and in the 10s words are about more deep concepts about feelings.I also defined a function to calculate some Stats for the number of words in the different periods of decades. You must pass as parameters, the data frame of lyrics, and the data frame of words. I used the following code to create the table:With this table, we can show a lot of information about the songs and the Lyrics of Metallica. For instance, the 1980s have more number of words and songs, and that’s because the most famous songs were released during this decade. Word per song of the 2000s is less than the rest of the decades, maybe we can infer that during the 2000s songs are shorter in time than the other decades.Another cool analysis that can help us to understand this data is taking a look at the tendency among the most frequent words used in a decade compared to the frequency of the same words in other decades.Using the function below you can create a line plot to look the tendency for a set of common words in a specific decade. for instance, if I want to compare the 10 most common words of the 1980s to the other decades I must pass this information and the data frame of words as parameters to the function:As you may notice during the 1980s the first 2 most common words used in lyrics of Metallica are Life and Death with a frequency of 12 for both words. But during the 1990s just life was used only in 6 lyrics and Death in just 1 for the rest of the decades.VADER (Valence Aware Dictionary and sEntiment Reasoner) of the NLKT Python Library is a lexicon and rule-based sentiment analysis tool. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. VADER model uses 4 different Sentiment Metrics.If you want to read more information about the VADER metrics, click here.I used the following code to calculate the 4 metrics for the Songs Lyrics of the Data Frame.Now a good way to vizualize the results are plotting the songs and their respectives Sentiment Metrics on a Scatter Plot using Matplotlib Library. In this case I plotted the Negative Score and the Positive Score for each lyric grouped by decade.Analyzing this plot I can inferred that the Lyrics of Metallica’s songs tends to have more Negative Valence, so for leading to generate a little more of negative feelings.I also wanted to analyze the sentiment but using the mean of scores by decade. so I just group by decade the main Data frame having this result.I realized that the 90s Songs Lyrics of Metallica tends to have more positive valence than the other decades. That’s really insteresting considering the most famous exposition of Metallica in the mainstream music was during the 90s.To conclude this article we learnt how to use a new technique to analyze words and text sentiments applied to Music. The advantage that people have living in this modern decade rather the past decades are astonishing. I mean, using simple techniques in the comfort of your home to create amazing researchs and projects, it allows us keep growing as society and taking advantage of the technology to achieve our goals and enjoy the time doing interesting things.This article will have a second part where I will try to find which are the main Topics, Concepts and Ideas that Metallica expose on their songs lyrics.References:Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
3722,On Eliminating Rush Hours,38,2019-09-06,https://towardsdatascience.com/on-eliminating-rush-hours-cdadd3fddcc3?source=collection_archive---------24-----------------------,2019,"The process of differentiating categorical data using predictive techniques is called classification. One of the most widely used classification techniques is the logistic regression. For the theoretical foundation of the logistic regression, please see my previous article.In this article, we are going to apply the logistic regression to a binary classification problem, making use of the scikit-learn (sklearn) package available in the Python programming language.We will use the Titanic dataset (available on Kaggle), where the goal is to predict survival on the Titanic. That is, on the basis of the features (explanatory variables) included in the dataset, we want to predict whether a particular person survived the Titanic shipwreck.We start by importing the required packages and loading the titanic data set.Here, we see the different variables included in the data set. Let us briefly describe each variable:When applying any predictive algorithm, we can never use it immediately without having done any pre-processing of the data. This step is extremely important, and can never be overlooked. For this data set, we perform the following pre-processing steps:1. Drop features that do not seem to add any value to our modelHere, we drop the PassengerId, Name, Ticket and Cabin features from the data set. The reason for this, is that these do not supply any predictive power to our model.2. Create categorical dummies for the embarkment portsWe created three binary features from the categorical Embarked feature, since the model cannot handle the string names in the original categorical variable.3. Transform gender names to binariesTransform the ‘male’ and ‘female’ names to binaries (0 and 1) through a mapping.4. Replace missing valuesNote that all the missing values (NaNs) appear to be in the Age feature. To solve for these missing values, let us fill the missing values with the average age in the data.Now that we have pre-processed the data, we can extract the explanatory variables in X and the target variable in y:Next, we split the data in a training and test set. The training set is used to train the logistic regression model. The model learns from the features included in the training set. The test set is used to validate the performance of the logistic regression model. For each observation in the test set, we predict whether the person survived or not, and compare the predictions with the true values.We split the data so that the training set consists of 75% of the data, and the test set consists of 25% of the data. We make use of the train_test_split module of the scikit-learn package.By making use of the LogisticRegression module in the scikit-learn package, we can fit a logistic regression model, using the features included in X_train, to the training data.Next, now that we have trained the logistic regression model on the training data, we are able to use the model to predict whether the persons included in the test set survived the shipwreck:Here, we see that for the first five observations of the test set, the logistic regression model predicted 4 out of 5 correctly.To evaluate the entire test set, we can use the metrics module from the scikit-learn package.Output:That is, the logistic regression model results in 80.3% accuracy. Definitely not bad for such a simple model!Of course, the model performance could be further improved by e.g. conducting further pre-processing, feature selection and feature extraction. However, this model forms a solid baseline.A nice way to visualize the results of the model is by making use of a confusion matrix. Here, we use matplotlib and seaborn to create a nice confusion matrix figure.In the confusion matrix, we see that 121 persons that did not survive the shipwreck were correctly predicted as not surviving, 18 persons that did not survive the shipwreck were incorrectly predicted as surviving the shipwreck, 26 persons that survived the shipwreck were incorrectly predicted as not surviving the shipwreck and lastly, 58 persons that survived the shipwreck were correctly predicted.Pros:Cons:Hopefully, after reading this article, you are now able to utilize the logistic regression technique yourself by making use of the scikit-learn package within Python. Good luck on using it within your own projects!Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
435,NFL — Does winning in the first quarter really matter in the NFL?,41,2018-06-18,https://towardsdatascience.com/nfl-which-quarters-correlate-most-with-winning-87f23024c44a?source=collection_archive---------4-----------------------,2018,"The Data Science Salon is returning to Miami on November 6–7, and we’re excited to show you our impressive lineup of speakers in AI and Machine Learning. We reached out to the speakers to ask them about the importance of model-based decision making, how models combine with creativity, and the future of models for the industry.There’s no doubt that we live in a world that is quantified and powered by models. And according to our panel of experts, this is largely a good thing. “A model should fit the data and the problem one is trying to answer,” said Colleen Farrelly, Data Scientist at Graham Holdings. “When models fit projects, the potential for real insight into difficult problems exists, particularly within economics and finance data.” Lloyd Reshard, CEO of Cognitive Big Data Systems, agrees that models are an inextricable part of the modern economy. “Models are an important key to decision making, especially in the research, engineering and business fields, and especially when you are building models from experimental, test, or marketing campaigns or financial data.”But why have models become a go-to strategy for business answers? “Modern businesses produce an immense amount of accessible data and it only makes sense to complete the feedback loop using decisions from data driven models,” said Sreya Ghosh, Data Science Manager at Restaurant Brands International. “Users of the product invariably give you feedback in usage patterns that you can use to build models and arrive at effective decisions. This quantitative feedback loop ends up evaluating the decision itself, allowing timely edits in the decision.” Madhav Khurana, Senior Data Scientist at Careem, agrees, “Human judgement is highly biased, especially in evaluating recent events, and can lead to decisions which won’t necessarily work in all situations. Although the human brain is quite powerful, it won’t be able process as much data as a model. Even if we do manage to overcome these inefficiencies of human judgement, it is also important to make decisions in real time when there are high number of transactions — be it predicting fraud in banks, cancellations in bookings or the next move of a player in a game.”But despite the power and wide adoption of models, there are still some limitations.“It is important to remember that, in the words of George Box, ‘all models are wrong but some are useful,” said Anton Antonov, Co-founder at Accendo Data. In other words, “Models are limited by the data that is chosen, the quality of the data, and how well it represents the range of values for each input,” according to Reshard. Ghosh goes further, “GIGO “garbage in, garbage out” is a very popular acronym in computer science: the model is only as good as the data it is learning from. Biases in the data consequently bias models and decisions. Models are also built on known information, making it very effective when you look at a lot of data. But it can’t disrupt — creative decisions still need the human brain.”And models can also be extremely resource intensive as well. Khurana says, “The most obvious limit of model-based decision making is that it needs data and a lot of it. My viewing behavior on Netflix may lead a model to decide that I like comedies, but because I have never watched a Korean action thriller, neither me nor the model will ever know if I’d like it. One skill that humans have but a machine learning model doesn’t is the skill of improvisation — taking action when something cannot be predicted” In the end, Khurana says, maybe it’s best to live by this quote from Jim Barksdale:“If we have data, let’s look at data. If all we have are opinions, let’s go with mine.”Even though your data may be inflexible, there a lot of creativity when it comes to analysis and application of data. Antonov says, “Data driven journalism requires clever interpretations of the available data. For any given data and problem domain, it is usually a very good idea (i) to come up with as many applicable models as possible, and (ii) evaluate and keep in mind the alternatives of the currently adopted modeling approach.”“Many times, pure mathematics can be used to simplify problems and models, such that a very accurate model that provides that right sort of insight in the right way can be created,” says Farrelly. “I often mix and match pieces of algorithms based on the underlying mathematics to create something new that fits the problem at hand better than packaged algorithms. Feature engineering, problem conversion, and proofs are great tools to aid in model design outside of the typical approach to modeling.”But sources of creativity in data aren’t limited to the analyst. Reshard says, “Domain knowledge also enables creativity for model creation. All employees have domain knowledge related to their job and can play a key role in model creation. Best results come when both a domain expert and model building expert work together.”Ultimately, your model is still only as good as your data. “There is an idea of “no free lunch” in model creation, i.e. given the exact same amount of data eventually any two models reach the same accuracy which only goes to show that an effective model is based on good data,” says Ghosh. “Collecting unusual data, looking at non-standard statistics, and creatively using them is a hallmark of good data science.”“Bringing a model from good to great is an iterative process.” says Reshard. “Models need to be validated against the real system. The implementation of the model needs to be exercised using a simulation to verify that it is producing accurate results.” Farrelly goes further, “Great models often go beyond measures of performance. Great models involve the blend of good performance (accuracy and speed), useful output for a given problem, and an ease of explanation to lay audiences, who are often the ones requesting the analyses. If an evolved tree model gives a good performance and an easy visualization of results, it might be a better model than a highly accurate and fast XGBoost model.”But like anything else, it’s hard to create a masterpiece out of low quality ingredients. “The difference between a good and great model is dependent of how you treat the data” says Ghosh. “A great model finds the underlying parameters that can swing the output, its ideally succinct and repeatable.” Antonov agrees, “It is usually the data. Great results are achieved with greatly curated data. But using (not very) bad data we can get good results. (Provided, sufficient data quantity and modeler cleverness.)”When it comes to predicting the future, our panel is certain of one thing — we’ll definitely be seeing more models. Here are some of their predictions:Fortunately, we can expect model-driven innovation to continue to improve the quality of our lives. “As our skills improve in making better models and engineering better technologies, our abilities in helping humanity also get a boost — from diagnosing diseases to forecasting weather,” says Khurana.Upcoming Events | Data Science Salon NYC | DSS MIA | DSS SEA | DSS LA | DSS ATXWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
4490,Generating a logo with MapBox GL and Python.,10,2019-11-08,https://towardsdatascience.com/generating-a-logo-with-mapbox-gl-and-python-2c44a357f462?source=collection_archive---------15-----------------------,2019,"In a monthly/yearly (i.e., discrete-time) subscription business setting (i.e., contractual), based on our current customer’s characteristics, what is the expected value of a new customer?(Fader, Peter, & Bruce (2007a))The equation of the expected value of customer life time value is quite straightforward:Here is an example from the paper Fader, Peter, & Bruce (2007b). Assume we have 1000 customers, at Year 1 (t0) we have all 1000 customers. At Year 2 (t1), only 631 customers are active. And at Year 5, only 326 customers are still active. Assume our subscription rate is $100/year and the discount rate is 10%.(Fader, Peter, & Bruce (2007a))We assume that customer duration/lifetime follows a geometric distribution because customers can only churn once.We model the heterogeneity of 𝜃 as a beta distribution. We use beta distribution because it is bounded by the interval [0, 1], and it comes in all possible shapes.Then we can combine the geometric distribution and the beta distribution and calculate the joint distribution. And through some calculation of Beta functions and Gamma functions, we can calculate the probability of customer churn at any time t and also the retention rate at any time t.Log-likelihood function: 𝐿𝐿(𝛼,𝛽|𝑑𝑎𝑡𝑎)=𝑙𝑛(𝐿(𝛼,𝛽|𝑑𝑎𝑡𝑎))choose a reasonable large number for kHere is my Python implementation for calculate the log likelihood function, and to optimize the negative log likelihood function. Note that we need to give the parameter an initial value as a starting point for optimization. In our example here, the initial values for 𝛼 and 𝛽 are both 1.Through optimization, the optimized negative log likelihood is 1409.5, and the MLE for 𝛼 and 𝛽 are 0.78 and 1.35.Here is my function to access model fit. Here I am only comparing the observed and calculated survival rate. You can change this function to compare likelihood function and retention rate as well.The first plot here used the initial parameters (1,1) as the model parameter, we can see that the model calculated survival rate is not very ideal. The second plot used the MLE values for the parameters and the model calculated survival rate is very close to the observed survival rate.Now we can calculate the expected value of the customer lifetime value using the MLE parameters. We can see that the expected value of a new customer is $362.Reference:Fader, Peter S. and Bruce G.S. Hardie (2014), “A Spreadsheet-Literate Non-Statistician’s Guide to the Beta-Geometric Model.” http://brucehardie.com/notes/032/Fader, Peter S. and Bruce G.S. Hardie (2007), “How to Project Customer Retention,” Journal of Interactive Marketing, 21 (Winter), 76–90.Fader, Peter S. and Bruce G.S. Hardie (2017), “Exploring the Distribution of Customer Lifetime Value (in Contractual Settings).” http://brucehardie.com/notes/035/Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
3621,Left Join with Pandas Data Frames in Python,74,2019-08-27,https://towardsdatascience.com/left-join-with-pandas-data-frames-in-python-c29c85089ba4?source=collection_archive---------2-----------------------,2019,"In this part, we will go through the applications of LIME. We have already seen the data preprocessing of the Titanic dataset and the theory and working of LIME in part 1 of this article. We have the preprocessed data. Next, we move to the LIME application.First, we will use the LIME application for a designed neural network-based model.This is our neural network class. I have used the TensorFlow Keras layers. Our input layer that takes in a dataset of 31 columns. It has 4 dense layers and the output layer has 1 neuron to decide “survived” or “not survived”. The output function is sigmoid. We will use Adam optimizer and binary cross-entropy.Now we will call and train the model.On our training set, the “predict_classes” function produces a 2-D list.If we observe, each element has 2 values the first value is the probability of the occurrence of the event and the second value is the probability of non-occurrence.Now, we go for the LIME.Next, we call the explainer.This gives a result as shown in the image. Now, here the leftmost box is the prediction probability of the two classes “Survived” class and “Not-survived” class. The middle chart shows the important features with their bounding values and the right table is the actual corresponding feature value in the observation row passed.This is a second instance for reference.Now, let’s check the LIME explainer for XGBoost. This following code snippet used to initiate the explainer for XGBoost.We call the explainer.Now, the small fonts 0.54,0.15 and so on are the corresponding contributions of the features, and the values 1.0 and 0.0 are bounding values.Second instance:We can obtain these results in other forms also.Result:This provides the map in this form. 1 is the prediction, and the list has feature number and importance.We will see an instance using Random Forest also.Now, in our theories, we have seen several interpretable models are used to get the intuitions. So, let’s backtrack and check if we can produce similar results.Now we can’t produce the dataset by tweaking the features because it is randomly done by the LIME module for a particular observation. So, let's take our entire dataset and try to interpret the results. We will do so using our Neural Network model.Results:This is our neural network prediction of each class on our entire dataset. So, now, from the results lets chalk out the predictionsThese are our predictions obtained. Now, this list will serve as our target set and our entire dataset without the survived columns as our feature set. So, actually we are trying to create a model that behaves like our ANN model.Let’s go for interpretation with the decision tree model first.We, fit the model with X as the training and pred as the prediction set.This gives accuracy as 87% which is quite good for our purpose. So, our decision tree model is behaving 87% as our black-box model. Now, let’s try to interpret it.This will give us a visualization of the nodes of the formed tree.This is our tree formed. Each node has a decision on a node that decides the results obtained. From here we can obtain the bounding values. Now, let's talk about the feature importance.ResultThis provides a list of 31 feature importances, a value for each feature.The above image shows the feature importance plot. Thus, we can derive a very clear intuition using Decision Tree about our neural network model overall.Now, Let’s see if we can derive something from the regression model.Similar to the Decision tree here we fit X and Pred to copy our black-box model.Results:So, it returns the weights of each feature and the B0 bias. So its a list of 32 elements. So, these are our feature importances.The above list gives us this plot. So we have seen according to the theory we can get the intuitions from the interpretable model.LIME can also be applied to images. But there it is done a bit differently. There changing a pixel won’t matter. So, a group of pixels is formed called superpixels. These superpixels are turned grey and then the images are again predicted. This way it is judged actually which portions of the image impact the decisions.SHAP stands for “SHapley Additive exPlanations”. It is based on the Shapley values, which are based on a method in the game coalition theory. It checks how a particular feature value impacts the model predictions. It first takes 2 features and tries their combinations to check how they affect the predictions and assigns them some importances. It then adds the third feature and checks how much it affects the predictions assigns it a weight again and so on. The SHAP tree explainer is used to get feature importances.This is the feature importance plot produced by applying Xgb.This is the feature importance plot produced by applying the Random forest.The red portion shows the impact of the features on the 0 or “Not_Survived” class and blue portion shows for the 1 or “Survived” Class.This is a basic insight into how human-machine learning interpretation libraries operate. I hope these articles help.This is the Github link.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
574,Citi Bike 2017 Analysis,789,2018-05-21,https://towardsdatascience.com/citi-bike-2017-analysis-efd298e6c22c?source=collection_archive---------5-----------------------,2018,"Congratulations! You’re the proud new owner of the coolest store in town. To keep the operation running, you need to ensure that you have the correct number of workers scheduled for each shift. In this tutorial, we’ll design the lowest cost schedule for the upcoming week.For the coming week, each day has two shifts of 8 hours. You currently have ten employees, four of which are considered managers. For any shifts beyond 40 hours in a given week (5 total shifts), you pay your employees overtime. To be fair to your employees, you decide that everyone has to work at least 3 shifts, but no more than 7 shifts. And to ensure that the shop runs smoothly, each shift requires at least one manager.Before diving into the code, let’s add structure to our task by defining our objective, variables, and constraints.In simple words, we want to design the lowest cost schedule, accounting for both regular time and overtime. We can define this mathematically as:Where w is our list of workers, RegCost and OTCost are the dollar costs of a regular and overtime shift for each worker, respectively, and RegShifts and OTShifts are the total number of regular and overtime shifts for each worker, respectively.We will create a list of variables for every worker/shift combination (e.g. [‘Employee1’,‘Monday1'], [‘Employee2’,‘Monday1’], etc.). Each of these variables will be a binary value to signify if a worker is scheduled (1) or not (0). We’ll also need to deal with the split of regular time and overtime, which we’ll handle as a hybrid of variable and constraint.From the problem statement above, we know that there are a number of special considerations that we need to follow. To make sure that our optimized schedule is acceptable, we’ll create specific constraints:Before diving into the the optimization model, we need some (illustrative) data to work with. Since loading data into Python is out of the scope of this tutorial, we’ll move through this part quickly.Here’s a recap of what we now have:Note: As can be seen in the above code, we are using a package called Gurobi. Gurobi is an optimization solver that is available for a number of programming languages. While the full version of Gurobi requires a commercial license, you can get an academic or online course license to run a limited version for free.We first need to create the shell of our model. We do this with the following code:Let’s turn our structured variables into code:First, we need to create binary variables for each worker/shift combination. We can do this with Gurobi’s addVars function (Note: if only adding one variable, use addVar instead). We specify that the variable is binary and we also read in the avail dictionary that we created before as a ub (“upper bound”). Each Gurobi variable has an upper and lower bound. Since we are using binary variables, naturally our variables must equal 0 or 1. By setting the upper bound equal to the values in avail , we are able to embed the constraint that certain worker/shift combinations must equal 0 (i.e. when that worker is unavailable).Next, we have to create variables to handle the regular and overtime hours. As was mentioned before, we’ll handle this split as a combination of variable and constraint. For now, we simply create the variables for each worker without further specification. The one exception is that we set overtimeTrigger to be a binary variable (0 when there is no overtime for a given worker this week and 1 when there is overtime).Similarly, let’s turn each constraint outlined above into code, using the addConstrs (adding multiple constraints at a time) and addConstr (adding one constraint at a time) functions.First, we specify that the sum of assigned workers (1 for each scheduled worker, 0 for each non-scheduled worker) for each shift equals the total shift requirement:Next, we deal with the split between regular time and overtime. To capture this correctly, we take a conservative approach. First, we specify that the number of regular shifts plus the number of overtime shifts is equal to the total number of shifts for each worker. Then, we ensure that the number of regular shifts is less than or equal to the number of shifts specified as our overtime trigger. We do this to ensure that regular shifts are accounted for before overtime shifts. To double down on this, we add the final constraint that says that if the number of regular shifts for a worker is less than 5 (OTTrigger), then the binary trigger for overtime is set to 0.With this in place, we can finish our final constraints. Similar to above, we calculate the total number of assigned shifts for each worker. We specify that this must be greater than or equal to the global input for minimum number of shifts and less than or equal to the global maximum number of shifts. Finally, we handle the requirement for each shift needs at least one manager to be staffed.Our objective is to minimize the total cost of the workers scheduled. We can handle this quite simply by defining a cost function that sums the total number of regular shifts times the cost of a regular shift for each worker and the total number of overtime shifts times the cost of an overtime shift for each worker. We tell Gurobi that the goal is to minimize this using ModelSense. Finally, we use setObjective to specify that Cost is the objective function.Before running the optimization, it can be helpful to inspect the model. A great way to do this is:With this code, you’ll be able to see the objective function, variables, constraints, etc. listed as formulas, which can be particularly helpful to ensure that the code is producing the functions that you intended.After you are happy with the model, we can solve the optimization with a simple line:The optimize function produces an output that is fairly helpful, but doesn’t give us a ton to work with. In the following steps, we’ll extract more meaningful information from the model.First, we want to know the total cost of the proposed schedule. We find out that the cost is $7535 by running the following:Now, let’s see a dashboard of the schedule using the following:Finally, let’s create an alternative view of the dashboard by simply printing out the names of each employee assigned to each shift:Through this tutorial, we produced an end-to-end solution to an optimization problem using Python. If this piqued your interest, play around with an example of your own. Try handling continuous decision variables, multi-objective problems, quadratic optimization, infeasible models- the possibilities are endless. And if you’re curious about creating your own optimization algorithms, check out my tutorial on building a genetic algorithm using Python!You can find a consolidated notebook here.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
3578,Confounders made simple,44,2019-08-23,https://towardsdatascience.com/confounders-made-simple-19131ff63ed1?source=collection_archive---------16-----------------------,2019,"Now that we have seen how difficult it is for an untrained professional to interpret X-ray images, lets’ look at a few techniques to view and analyze the images, their histograms, and a technique to add images and labels together, using Python programming.The image dataset (Chest X-Rays) was obtained from Kaggle. Dataset is available on the following link — https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/dataAbout the dataset — direct quote from the Kaggle challenge — The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal). Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.As the content clearly states, there are a total of 5863 images available in the challenge, which have been split into 2 classes, Pneumonia and Normal, and further split into train/test and validation sets. To make the challenge even harder, we have split the data into three classes, Normal, Bacterial Pneumonia, and Viral Pneumonia. In this part, we will focus only on the images — loading them with python, analyzing various important aspects of the image from a medical imaging perspective, and loading the images and labels together. Let's dive straight into it.Numpy — Numpy is one of the most commonly used libraries in Python. It is used for operations on multi-dimensional arrays and matrices and doing high-level mathematical functions to operate on these arrays.Matplotlib — A library for creating static and animated visualizations in python.os —A module that comes built-in with python. It provides functions for interacting with the operating system.cv2 — OpenCV (Open Source Computer Vision Library) — A very important library mainly used for computer vision. Other similar libraries are SimpleITK and Pillow (Python Imaging Library).random — A module that generates pseudo-random numbers.The images from the dataset have been split into three classes as mentioned previously. 1-Normal, 2-Bacteria (Bacterial Pneumonia), 3- Virus (Viral Pneumonia).The above code snippet is creating a function load_image, which will be used to load a single image from the training sets, Bacteria folder. os.listdir is used to list all the files present inside that directory. In this case, it can be used to access all the images present inside the folder Bacteria. Next, it will print the name of the image. Finally, the OpenCV library is used to read the image.Break- is necessary here, so that only the first image is accessed, otherwise the function will loop through all the images present inside the Bacteria folder.output-In this code snippet, first, the path of the images is defined. Then the first image from the folder is loaded into variable ‘image’ by calling the function load_image. The image is then viewed by using matplotlib.imshow. After this, the dimensions of the image, the maximum pixel value, and the minimum pixel value in the grayscale bar is printed. Also the mean and standard deviation of the image pixels are calculated.Next, we plot the histogram of all the pixels of the image. A histogram is a graphical display of data using bars of different heights.Output-Matplotlib.hist is used to plot the histogram. As the image is mostly dark, we see a huge cluster of pixels on position zero of the grayscale bar.These are some basic functions that can be carried out on images using OpenCV and matplotlib. We will in later parts see more uses of OpenCV.The path of the training set is defined, and the directories under the path are saved in ‘train’. In this case, there are three folders, 1_Normal, 2_Bacteria, and 3_Virus.output-[‘1_NORMAL’, ‘2_BACTERIA’, ‘3_VIRUS’]We create an empty list — folders. Then, iterate over the path, using os.listdir, and sort and store the folder names in the list — ‘folders’.output-The folder names are set as labels for the images, and the image size is selected to be 256*256. That is, all the images will be resized into 256*256. If we go through the dataset, we see all the images are of varying dimensions, and to feed images into a Convolutional Neural Network (CNN) it is necessary to resize the images into the same dimensions.Here we define a function to load in all the images according to the label names, resize them into 256*256 pixels, and return the image arrays.An empty list is created to save all the images. Then a ‘for’ loop is run to extract all the images from all the three folders. os.path.join is used to combine paths from directories. cv.IMREAD_GRAYSCALE converts all images to grayscale format. cv.resize is used to resize images to 256*256 pixels. ‘.append’ is used to append all the images into a list, which is finally converted to an array and returned using the return statement.output- Shape of the training images = (5208, 2)The function ‘load_train’ is then called, and all the training images are saved as an array in train_images. The shape of training images is (5208,2)output -The images and labels need to be separated for training a neural network, and they are done so, by looping over the train_images, and by extracting the images and their corresponding labels.output-To check the number of images in each class, a for loop was run. The results are then plotted using matplotlib.bar which is used to create bar charts.From the data, it is clear, that there is a big difference in the number of images belonging to each label. If the network is trained with exactly these numbers of images, it might be biased towards the class with most labels. This is known as the Class Imbalance Problem. Hence it is necessary for each class to have a similar number of images, which we will talk about in the next part.Finally, we use the random module to generate nine random images from the training set and then used matplotlib to plot these images.This is the end of this part. As we see, for medical imaging analysis it is first very important to understand the dataset properly, in this case, X-ray images. In the next part, we will deal with the class imbalance problem and more operations using matplotlib and OpenCV.References:Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
636,Python Sets and Set Theory,527,2018-06-06,https://towardsdatascience.com/python-sets-and-set-theory-2ace093d1607?source=collection_archive---------5-----------------------,2018,"I decided to read all the abstracts from NIPS/NeurIPS 2018. But it turned out to be implausible, both physically and mentally, in the time frame I wanted. There are 1011 accepted papers for this year conference including 30 orals, 168 spotlights and 813 posters out of 4856 papers with an acceptance rate of 20.8%.(source)I wanted to read all the abstracts in 24 waking hours I could get before the conference starts. I got 1440 mins to read 1011 abstracts, having an average time of 1.42 mins. Totally being stupid, I wanted to summarize the abstract to make a mini-abstract so that it would be easy to follow a concise abstract when I come back to it later or to share it.I started reading abstracts, taking a set of 20 (first 20) from the first poster session of the conference ‘Tue Poster Session A’ (it has 168 papers). It took me a little over 210 mins to read and summarize (extractive manner, taking some pieces of the abstract), on an average of 10.5mins/paper. I paced it up a little not worrying too much about summarizing, I finished next 20 in about 150 mins on an average 7.5 mins. The next 20 in about 90 mins. The next 20 in about 70–80 mins. And next 20 in 60–70 mins. After 140 papers I gave up the time limit and took a break.Nonetheless, wonderful thing happened when I am finishing a group of 20 and going to an other. Its really intimidating and overwhelming to read a concentrated abstract of solid research investigation, even one, and I have to read 20 such papers and keep reading. Reading first 20 papers, any theories that I don’t know or topic that I am not well versed with would stop me from understanding what they are solving or the value of their solution.But, eventually I got less intimidated by the theories they used or their peculiar novelty to get to the solution, and saw them as the sort of inspirations or insights to solve a particular limitation or to extend the versatility of existing work that one find. And I felt the ease, when reading the abstract, to attend to the problem they are solving and the novelty, validity, and the impact of their solution to the field.Overall, I am really happy that I made myself read not-a-regular number of abstracts, even though it seemed fatal in many ways!! I still want to read all the abstracts from the conference but it could take, may be, a week. I will get you posted.These are the must read from the papers I have gone through (18 papers short of the entire ‘Tue Poster Session A’) and theirs abstracts. Sort-of-tags are not so efficient in representing these papers, they are just a mere human latent perceptive overhead, sometimes seen as feelings.FUNDAMENTALSA novel framework for embeddings which are are numerical flexible and which extend the point embeddings, elliptical embeddings in wessserstein space. Wasserstein elliptical embeddings are more intuitive and yield tools that are better behaved numerically than the alternative choice of Gaussian embeddings with the Kullback-Leibler divergence. The paper demonstrates the advantages of elliptical embeddings by using them for visualization, to compute embeddings of words, and to reflect entailment or hypernymy.SYSTEMATIC EVALUATION, REALLY KNOWINGDespite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyper-parameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.FUNDAMENTALS, AT THE COREThe basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixel-level, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks, which may require very deep features with high resolution. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot \emph{directly} propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular, on ImageNet-1k, the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. The code is available at https://github.com/kevin-ssy/FishNet.PRACTICAL MAGIC, ELEGANTFlow based generative model with 1x1 invertible convolutions, which demonstrate significant improvement in log-likelihood and quantitative sample quality. Perhaps most strikingly, it demonstrates that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.INTERESTING, ABOUT TIMEWe have shown the curious inability of CNNs to model the coordinate transform task, shown a simple fix in the form of the CoordConv layer, and given results that suggest including these layers can boost performance in a wide range of applications. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.FUNDAMENTALS, UNDERSTANDINGWe give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant beta, given by the sum of the reciprocals of the hidden layer widths. When beta is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.PRACTICALThe large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks. Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost. However, there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. We show that O(1/√MK) convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly. We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost while preserving the O(1/√MK) convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-communication SGD with only 3%−5% communication data size.FUNDAMENTALS, NORMALIZATIONNormalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks.SYNAPTIC PRUNING, NEUROSCIENCEConvolutional Neural Networks(CNNs) are both computation and memory inten-sive which hindered their deployment in mobile devices. Inspired by the relevantconcept in neural science literature, we propose Synaptic Pruning: a data-drivenmethod to prune connections between input and output feature maps with a newlyproposed class of parameters called Synaptic Strength. Synaptic Strength is de-signed to capture the importance of a connection based on the amount of informa-tion it transports. Experiment results show the effectiveness of our approach. OnCIFAR-10, we prune connections for various CNN models with up to96%, whichresults in significant size reduction and computation saving.CLEANWe propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.REVOLUTIONARYMemory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected — i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module — a Relational Memory Core (RMC) — which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (BoxWorld & Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.MINORITY APPROACHLearning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict “em what drugs are likely to target proteins involved with both diseases X and Y?” — a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries — a flexible but tractable subset of first-order logic — on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.BIG PROBLEMSIn multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.THE SOLUTIONBatch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the “batch” dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing SOTA results on WMT’14 English-to-French translation task and the one-billion-word Language modeling benchmark. Mesh-Tensorflow is available at https://github.com/tensorflow/meshI couldn’t stop thinking about NeurIPS!! Writing about it either.Edit: I published a selection of papers from the first two poster sessions (330+ papers)Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",1
2320,A Chance to Get it Right: Embracing Automated Decision Making,19,2019-04-15,https://towardsdatascience.com/a-chance-to-get-it-right-embracing-automated-decision-making-48229904b443?source=collection_archive---------40-----------------------,2019,"Let’s say you have a processor, an image processor, and this image processor has a few processing stages. And each stage altered or processed an image in some different manner. And this image processing was awesome and you want to share it with the world. So you code up a server that takes a file upload and runs this image processing. Perhaps each processing stage was a Flow[BufferedImage]. Let’s assume for arguments sake that took about a second per stage and looked something like this:You’d then chain all of the process stages together, which would probably look something like a Flow[BufferedImage, ImageProcessed]:You then built a method that runs this flow as a Stream:In the real world, this wouldn’t go to a Sink.ignore , as we’d probably save the processed image to an S3 bucket or other storage.With Akka Http, you’d easily define a Route to handle the file upload:Now our friends can use our image service! But, what if we wanted to notify the user of our processing progress?We know from the Akka Http docs that we handle WebSockets with Flow[Message]’s, as their example shows:But how do we talk to this WebSocket Flow from the outside? 🤔The answer is to preMaterialize a Source and then build a Flow from a Sink and this preMaterialized Source … So let’s do that:Now, anything we send to wsActor will be sent back to our JavaScript web client! Our new Route now contains the WebSocket handler:And we update the processing stages to notify our wsActor:Now when we upload an image, we get a nice list telling us what stages of processing our image is going through 😎All code (including the front-end!) is open sourced here. 🐾Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
3546,Data Preprocessing in Data Mining & Machine Learning,75,2019-08-20,https://towardsdatascience.com/data-preprocessing-in-data-mining-machine-learning-79a9662e2eb?source=collection_archive---------5-----------------------,2019,"I have recently spent a lot of time thinking about the way we hire in technology. I focus on Data Science. I often ask myself what a good data scientist looks like in an interview process. Data Science skillset:Where Software Engineering is the foundation, and all others are pillars. As an industry, we should be splitting these responsibilities, but we aren’t there yet. As we need such a broad skillset it’s difficult to cover all pillars in depth in an interview process.The purpose of an interview question is to reveal a candidates aptitude for a job. There is little use in the ability to regurgitate misunderstood facts. The interviewer needs to dissect a candidates personality and approach to work. A good question should reveal skill in the above pillars. Any skill level candidate should have the possibility of giving a good answer.The industry seems divided on the necessity of Software Engineering and DataOps in their Data Scientists. A Data Scientist can’t deliver value in a silo without a software engineer to bring their ideas to fruition.As such my favourite Machine Learning and Data Science interview question of all time is:“What is your opinion on linting?”This question will showcase an individual's maturity as a software engineer. A candidate who doesn’t know what linting is, is given an opportunity to show a willingness to learn.There is nothing to do with data in this question. It’s still a great question and satisfies the above requirements.“I don’t have an opinion on linting” — This is not a good answer and would leave me very worried.“I’ve never heard of linting before, but can you tell me about it?” — The interviewer gets the chance to share their opinion on linting. If the interviewer defines linting well the candidate can proceed with the question. You can follow up with more questions:“I know what linting is, I think X, Y, Z” — This candidate is a mature software engineer. Following good practices in one facet of their knowledge suggests they follow others.This question reveals more about personality and programming skill than most technical questions. Anybody who shows curiosity here will be capable of learning anything needed for this job. And that is the purpose of an interview.To conclude, when hiring we are looking for a culture fit with the team. It is a bonus if they already know the skills we need. If they don’t already know these skills, then they need to have the right mindset to be open and improve.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
5468,QlikView Visualization of Formula 1 (F1) Relational Data Model,2,2020-01-20,https://towardsdatascience.com/qlikview-visualization-of-formula-1-f1-relational-data-model-82e8b46c9f71?source=collection_archive---------40-----------------------,2020,"Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.Most people use Excel to make financial calculations. But both SAS and Python are very capable of helping in finance too. One of the benefits in using SAS and Python is the structured automation.In this article, I will show you some of the most common formulas used in finance:Btw. There is a big error in SAS and Excel regarding the description on how to calculate net present values. I will show you the correct way to do it.You can find financial functions in Python Numpy Library until version 1.17Check your Numpy versionIf installed NumPy version is > 1.17, the financial function is moved from NumPy to NumPy Financial. So, you need to import the financial functions as npf instead of np in your Python program.Import as follow:In this example, we want to calculate how much you have to pay monthly to pay back a loan of 10.000 in 5 years. The yearly interest rate is 6%, and is calculated monthly.You can use a function called finance(). Its is a replica of the functionalities in Excel.You use the np.pmt() function to compute the monthly payment. I first create a data frame with a dictionary approach with only one record. I call it Obs and give it a value of 1. I need to create a non empty data frame, so I can assign scalars values to it.In this example, I refer to the columns, using dot notation instead of hard brackets. As I have mentioned before, you should be careful when using this notation, since it is the same way you refer to an attribute. Therefore, I mostly use this notatin when I use a column inside a function. I still use the df['var'] when I create the variable.You have to pay 193.33 per month to pay back a loan of 10.000 over five years.The next example is to find out what is left of the loan of 10.000 if you pay 200 per month over the next 60 month.The yearly interest rate is 6% per year, and is calculated monthly.We use the same finance function, FINANCE() as before, buut this time the missing argument is fv.The result is that you have paid 465.51 more than you should.You use the function np.fv(rate, nper, pmt, pv). Same syntax as before.Alternative ways in PythonI want to show you a way to do the same calculation without using a DataFrame. You can do this directly in Python. You assign the values to variables and use the variables as argument in the np.fv() function.Or you just type the values as an argument directly in the NumPy function.If you just have to make one calculation, that is for just one observation, the last two examples are sufficient.In this example, we still look the loan, but this time we want to know how much we can borrow if we repay the loan with 200 per month for the next five years. The annual interest rate is 6% and is charged monthly.We use PV to tell the finance function that we are looking for the present value.The result of the calculation is that we can borrow 10.345,11.You use the present value function np.pv() from NumPy:What about the number of payments you need to make to pay back 10.000 with 200 per month?We use the same setup as before, but this time we set the FINANCE() function to calculate nper.Again, the syntax is the same. This time we use the function np.fv():So we have to make 58 payments to pay back 10.000 over 5 years.Now we want to look at the yearly interest rate when you pay back a loan of 10,000 with 60 monthly payments of 200.The result from the FINANCE() function is the monthly interest. Therefore, you have to multiply the result with 12 to get the annual interest rate.So to pay back a loan of 10.000 with 200 per month, the anual interest would be 7.42%We use the np.rate() function. But, we still need to multiply it with 12 to get the yearly result.If you want to know how much principal you would pay each month, you can use the ppmt method.You calculate ppmt for each period using the FINANCE() function in a do loop to get the 60 principal payments.To create one records for the 60 months, we use range(1,61) to make a list of 60 values from 1 to 60 as the period column. For each period the np.ppmt() function is then used to calculates the principal payment.Remember that SAS count the possition of the records starting with the first record having the number 1. In Python the number of the first record is 0.For SAS users, think about it as your age, when you were born. In you first year you were zero years old.There are other financial calculations we can do.The first is how to calculate the effective interest rate if you are charged monthly interest.In SAS, the finance function can do the calculation for us.In Python, you have to calculate it yourself and that is very simple.I’ll show you how to use the FINANCE() function and how you can calculate it yourself.You have to calculate the effective interest rate ourselves. I will show you how to do this in Python and in a DataFrame.Notice how you can add a text in your print output!So the effective interest rate is 6.16%There are other useful calculations you can use in finance. Internal rate of return is one of them.You can still use the FINANCE() function. But this time the arguments are different as they are the cash flow for the investment.In this example, you invest 40.000 and receive four payment in return. The function returns the IRR of this investment.The internal rate of return is 10.58%We also have a np.irr() function and the syntax is very similar to SAS. Only that you don't need to have the cash flow in separate variables.You can also do the calculation outside the DataFrame with Series as an input or directly with the cash flow numbers as input.This is a very interesting formula. For many years the explanations of how to use this formula have been wrong in both SAS and Excel.When you calculate a net, you have to deduct something. But that is not explained in the documentations of SAS and Microsoft. So the function is not calculating npv.You have to add a deduction to get the right result. In Python, every thing is calculated correctly from the beginning.You can use the FINANCE() function in calculating the Net Present Value. You use the cash flow and interest rate as input and then deduct the investment from the output.I will also show you the formulas for calculating npv without a function.On the other hand, the np.npv() function takes the full list of values, both the investment and the cashflow as arguments.You can also use your own formula for calculating NPV.This last example shows how to separate the two values, investment and cashflow. If you have the values in each list, you can add the two list's in the function as investment+cashflowThat all I had about financial functions. In SAS you can use FINANCE() for all the calculations. In Python you use separate functions, but with very similar syntax.I hope you are aware of the challenge in calculating net present values in SAS and Excel and that you are not including the invesment in you function. If you are, I wish you good luck telling you boss.We all make mistake, we are humans after all.I hope you have enjoyed the article and I look forward writing more tips about SAS and Python.If you want to know more about how you can learn Python the way you work in SAS, go to the following link to get more details:https://www.nordstroem.uk/learn-python-the-way-your-work-in-sasWritten byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0
2086,Build a Pokemon bot with SAP Conversational AI and NodeJS,29,2019-03-14,https://towardsdatascience.com/build-a-pokemon-bot-with-sap-conversational-ai-and-nodejs-cb0f693410e0?source=collection_archive---------17-----------------------,2019,"Posted by Jiayi (Jason) Liu, Unmesh Kurup, and Mohak ShahAuptimizer is a general-purpose open-source Hyperparameter Optimization (HPO) framework that also allows you to scale your HPO training from CPUs and GPUs to on-prem and EC2 instances. To get started, use “pip install auptimizer”. You can find our documentation here and our repo here.Over the past decade, we have made significant advances in the building and training of machine learning models. We can now optimize very large models by utilizing improvements in algorithms, learning strategies, and the availability of distributed compute and memory. However, as the size and complexity of these models have grown, so too have the number of underlying hyperparameters. But the strategies for dealing with hyperparameter optimization (HPO) have mostly remained limited to the most commonly used grid- and random-search approaches in practice. While there are a few commercial and open-source solutions that target HPO, none has broad applicability across problems and platforms. HPO remains as much art as science and a key bottleneck in training effective machine learning models.One roadblock is the lack of consensus on the best available HPO algorithm. Developers have to experiment with multiple algorithms to find the one best suited to their problem. However, a lack of portability between the implementations means that the users are often stuck using one particular algorithm having built their tooling around them. Auptimizer makes it easy for researchers and practitioners to switch between different HPO algorithms. In addition, Auptimizer also supports cross-platform scaling enabling users to easily scale their experiments from desktop to on-premise clusters and even the cloud.For ML researchers, the use case can be different. Researchers focus on developing the algorithms to find the best hyperparameters. Thus, an easy framework to facilitate their algorithm implementation and to benchmark their results against state-of-the-art algorithms is also important.Briefly, Auptimizer is a scalable, extensible toolkit for conducting HPO. Auptimizer has three advantages:Since Auptimizer is platform independent, it can work with your framework of choice including TensorFlow, PyTorch, MxNet and Caffe.Auptimizer’s common interface makes the process of switching between HPO algorithms much easier by abstracting away the differences between their implementations. In this release, we support the following HPO techniques — Random Search, Grid Search, Hyperband, Hyperopt, Spearmint, and EAS (experimental).Auptimizer can easily support integration and benchmarking of newer HPO techniques including your own custom algorithms. As an example of Auptimizer’s ease of extensibility, we integrated Bayesian Optimization and Hyperband (BOHB) in just a couple of days (requiring to write only 138 lines of code while reusing the original 4305 lines of codes).Finally, Auptimizer also includes functionality to automate your HPO process. Auptimizer supports different computing resources, such as CPUs, GPUs, multiple nodes, and AWS EC2 instances. It is also flexible enough to extend to other cloud platforms or your on-premise solution.Acknowledgements: This work is an LG Electronics (America Research Center) team effort. Special thanks to Samarth Tripathi, Vera Serdiukova, Junyao Guo, Homa Fashandi, Guohua Ren, and Olimpiya Saha for their contributions.Written byHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look",0

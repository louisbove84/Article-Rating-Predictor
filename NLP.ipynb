{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/louisbove84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/louisbove84/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "SENT_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Twitter and Reddit DataFrames\n",
    "df_twitter = pd.read_csv('twitter.csv')\n",
    "\n",
    "#Normalize the likes/retweets/follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(text_lst, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(' '.join(text_lst)), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "#Tokenize, Lowercase, and Filter Stopwords for Column in DataFrame\n",
    "def tokenize(df, col_name):\n",
    "    #Pull down stopwords and punctuation\n",
    "    punctuation_ = set(string.punctuation)\n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    \n",
    "    #Use Snowball Stemmer for stemming\n",
    "    stemmer_snowball = SnowballStemmer('english') \n",
    "    \n",
    "    #Modify text\n",
    "    df[col_name] = df[col_name].apply(lambda row: [w.split('://')[0] for w in row]) #Remove links \n",
    "    df[col_name] = df[col_name].apply(lambda row: word_tokenize(row)) #Tokenize (separate words)\n",
    "    df[col_name] = df[col_name].apply(lambda row: [w.lower() for w in row]) #Lowercase\n",
    "    df[col_name] = df[col_name].apply(lambda row: [w for w in row if not w in punctuation_]) #Remove Stopwords\n",
    "    df[col_name] = df[col_name].apply(lambda row: [w for w in row if not w in stopwords_]) #Remove Punctuation\n",
    "    df[col_name] = df[col_name].apply(lambda row: [stemmer_snowball.stem(w) for w in row]) #Stemming (taking words back to the basics)\n",
    "    df[col_name] = df[col_name].apply(lambda row: extract_ngrams(row,1) + extract_ngrams(row,2)) #Add 2-Grams \n",
    "    df[col_name] = df[col_name].apply(lambda row: pos_tag(row))#Tag speech with noun/adj/verb/etc.\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make all words lowercase\n",
    "df_twitter['tokenized'] = df_twitter['text'].apply(lambda row: row.split('://')[0]) \n",
    "\n",
    "#Tokenize tweets\n",
    "df_twitter['tokenized'] = df_twitter['tokenized'].apply(lambda row: word_tokenize(row)) \n",
    "\n",
    "#Make all words lowercase\n",
    "df_twitter['tokenized'] = df_twitter['tokenized'].apply(lambda row: [word.lower() for word in row]) \n",
    "\n",
    "#Filter stopwords\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "punctuation_ = set(string.punctuation)\n",
    "df_twitter['tokenized'] = df_twitter['tokenized'].apply(lambda row: [w for w in row if not w in punctuation_]) \n",
    "\n",
    "#Stemming\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "df_twitter['tokenized'] = df_twitter['tokenized'].apply(lambda row: [stemmer_snowball.stem(word) for word in row]) \n",
    "\n",
    "#Add N-Grams\n",
    "#df_twitter['tokenized'] = df_twitter['tokenized'].apply(lambda row: extract_ngrams(row, 1) + extract_ngrams(row, 2)) \n",
    "\n",
    "#Part-of-Speech tagging (create tuple of word and noun/verb/adj/etc.)\n",
    "#df_twitter['tokenized'] = df_twitter['tokenized'].apply(lambda row: pos_tag(row)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the bag-of-words using TFIDF\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "vectors = vectorizer.fit_transform(df_twitter['tokenized'])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df_twitter_tfidf = pd.DataFrame(denselist, columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640, 6211)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>''</th>\n",
       "      <th>'d</th>\n",
       "      <th>'m</th>\n",
       "      <th>'s</th>\n",
       "      <th>+100</th>\n",
       "      <th>--</th>\n",
       "      <th>-/</th>\n",
       "      <th>-100</th>\n",
       "      <th>-3</th>\n",
       "      <th>-doctor</th>\n",
       "      <th>...</th>\n",
       "      <th>ü§∑‚Äç‚ôÇÔ∏è</th>\n",
       "      <th>ü•ß</th>\n",
       "      <th>ü•ßwe</th>\n",
       "      <th>ü•∞</th>\n",
       "      <th>ü•∞üêÄ</th>\n",
       "      <th>ü•∞üöÄ</th>\n",
       "      <th>ü•≥</th>\n",
       "      <th>ü•≥üéÜ</th>\n",
       "      <th>ü•∫</th>\n",
       "      <th>ü¶∏üèª</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137353</td>\n",
       "      <td>0.091855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 6211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ''   'd        'm        's  +100   --   -/  -100   -3  -doctor  ...  \\\n",
       "4  0.0  0.0  0.000000  0.000000   0.0  0.0  0.0   0.0  0.0      0.0  ...   \n",
       "5  0.0  0.0  0.000000  0.000000   0.0  0.0  0.0   0.0  0.0      0.0  ...   \n",
       "6  0.0  0.0  0.000000  0.149820   0.0  0.0  0.0   0.0  0.0      0.0  ...   \n",
       "7  0.0  0.0  0.137353  0.091855   0.0  0.0  0.0   0.0  0.0      0.0  ...   \n",
       "\n",
       "   ü§∑‚Äç‚ôÇÔ∏è    ü•ß  ü•ßwe    ü•∞   ü•∞üêÄ   ü•∞üöÄ    ü•≥   ü•≥üéÜ    ü•∫   ü¶∏üèª  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[4 rows x 6211 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_tfidf.iloc[4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
